{
  "hash": "40f9319a6a087a7821dfe3170a9215fd",
  "result": {
    "markdown": "---\ntitle: \"Métodos supervisados: continuación\"\nsubtitle: \"IND 163 - 2022/02\"\nauthor: \"Eloy Alvarado Narváez\"\ninstitute: \"Universidad Técnica Federico Santa María\"\ndate: 28/10/22\nformat: \n  revealjs:\n    theme: slides.scss\n    touch: true\n    slide-level: 2\n    code-copy: true\nincremental: true\nslide-number: true\nlang: es\nhighlight-style: github\nwidth: 1600\nheight: 900\nlogo: images/logo_usm.png\ntransition: fade\nfooter: \"IND 163 - Semana 9\"\nexecute:\n  freeze: auto\neditor: \n  markdown: \n    wrap: 72\n---\n\n\n# Máquina de vectores de soporte (SVM)\n\nEsta metodología fue desarrollada en lo '90 por la comunidad de ciencias computacionales. Consiste en la generalización de un clasificador llamado **clasificador de máximo margen**, que destaca por su simpleza, pero que en la práctica es difícil de utilizar debido a que requiere que las clases sean separables por un límite lineal.\n\n## Clasificador de máximo margen\n\nAntes de definidir el clasificador de máximo margen, debemos introducir dos conceptos primordiales:\n\n-   Hiperplano\n\n-   Hiperplano de separación óptimo\n\n## Hiperplano\n\nEn un espacio $p-$dimensional, un hiperplano es un subespacio afín plano de dimensión $p-1$. Por ejemplo, en dos dimensiones, un hiperplano es una linea. En tres dimensiones, un hiperplano es un subespacio 2-dimensional plano.\n\nMatemáticamente, para el caso bidimensional, un hiperplano está definido por la ecuación:\n\n\n$$\n\\beta_0+\\beta_1 X_1 + \\beta_2 X_2 = 0\n$$\n\n\npara parámetros $\\beta_0,\\beta_1$ y $\\beta_2$. Naturalmente, la extensión a $p$ dimensiones es:\n\n\n$$\n\\beta_0+\\beta_1 X_1 +\\beta_2 X_2 + \\dots + \\beta_p X_p =0\n$$\n\n\nque define un hiperplano $p-$dimensional, en el sentido de que si un punto $X=(X_1,X_2,\\dots,X_p)^T$ en un espacio $p-$dimensional que satisface la ecuación anterior.\n\n## Figura hiperplano\n\n![](images/week9/hyperplane.png){fig-align=\"center\"}\n\n## Clasificando usando un hiperplano separable\n\nSupongamos que tenemos una matriz de datos $\\mathbf{X}$ de tamaño $n\\times p$ que consiste en $n$ observaciones de entrenamiento en un espacio $p-$dimensional,\n\n\n$$\nx_1=\\begin{pmatrix}x_{11} \\\\ \\vdots \\\\ x_{1p}\\end{pmatrix} , \\dots,x_n=\\begin{pmatrix} x_{n1} \\\\ \\vdots \\\\ x_{np} \\end{pmatrix}\n$$\n\n\ny que estas observaciones caen dentro de dos clases, esto es, $y_1,\\dots,y_n \\in \\{-1,1\\}$ donde $-1$ representa una clase y $1$ la otra clase. También tenemos una observación de prueba, un $p-$vector de *features* observadas $x^*=(x_{1}^{*}\\, \\dots \\,x_{p}^{*})^T$\n\n## Clasificando usando un hiperplano separable: continuación\n\nNuestro objetivo es desarrollar un clasificador basado en este conjunto de entrenamiento que clasifique correctamente la observación de prueba usando las variables medidas, para esto nosotros ya hemos visto varias metodologías que podríamos usar: LDA, QDA, árboles de decisión y regresión logística.\n\nEn lo que sigue veremos una metodología que se basa en el concepto de hiperplano separable.\n\n## Clasificando usando un hiperplano separable: continuación\n\nSupongamos que es posible construir un hiperplano que separe las observaciones de entrenamiento perfectamente de acuerdo a sus clases. Por lo que si utilizamos las clase como antes ($\\{-1,1\\}$) se tendrá la propiedad que\n\n\n$$\n\\beta_0+\\beta_1 x_{i1} +\\beta_2 x_{i2}+\\dots+\\beta_p x_{ip} > 0 \\quad \\text{si} \\quad y_i=1\n$$\n\n\ny,\n\n\n$$\n\\beta_0+\\beta_1 x_{i1} +\\beta_2 x_{i2}+\\dots+\\beta_p x_{ip} < 0 \\quad \\text{si} \\quad y_i=-1\n$$\n\n\nequivalentemente, un hiperplano separable tiene la propiedad que:\n\n\n$$\ny_i(\\beta_0 +\\beta_0+\\beta_1 x_{i1} +\\beta_2 x_{i2}+\\dots+\\beta_p x_{ip})>0\n$$\n\n\npara todo $i=1,\\dots,n$.\n\n## Clasificando usando un hiperplano separable: continuación\n\n![](images/week9/hyperplane_sep.png){fig-align=\"center\"}\n\n## Clasificando usando un hiperplano separable: continuación\n\nSi un hiperplano separable existe, podemos usarlo para construir un clasificador bastante natural: una observación de prueba es asignada una clase dependiendo de que lado del hiperplano está ubicada.\n\nIntuitivamente, podremos estar seguro de nuestra clasificación conforme la magnitud obtenida tras clasificar la observación de prueba.\n\n## Clasificador de margen máximo\n\nEn general, si nuestros datos pueden ser perfectamente separados un hiperplano, entonces existiran un infinito número de aquellos hiperplanos. Para poder construir un clasificador basado en un hiperplano separable, debemos encontrar una forma razonable de decidir cual de estos infinitos hiperplanos separables usar.\n\nUna elección natural es el **hiperplano de máximo margen** (también conocido como *hiperplano separable máximo*), que es el hiperplano que está más lejos de las observaciones de entrenamiento. Esto es, podemos calcular la distancia (perpendicular) desde cada punto a un hiperplano separable dado; la menor de aquellas distancias es la mínima distancia entre las observaciones y el hiperplano, esta distancia es conocida como **margen**.\n\nEl hiperplano de margen máximo es el hiperplano separable en donde el margen es el más grande, esto es, es el hiperplano que tiene la distancia mínima más lejana a las observaciones de entrenamiento.\n\nLuego, podemos clasificar una observación de prueba basado en que lado del hiperplano de margen máximo recae.\n\n## Vector de soportes\n\n![](images/week9/support_vectors.png){fig-align=\"center\"}\n\n## Construcción de un clasificador de margen máximo\n\nAhora consideramos la tarea de construir el hiperplano de margen máximo basado en un conjunto de $n$ observaciones de entrenamiento $x_1,\\dots, x_n \\in \\mathbb{R}^{p}$ y clases asociadas $y_1,\\dots,y_n \\in \\{-1,1\\}$. En síntesis, este hiperplano es la solución de un problema de optimización dado por:\n\n\n```{=tex}\n\\begin{align*}\n&\\max_{\\beta_0,\\beta_1,\\dots,\\beta_p} \\quad M\\\\\n&\\text{ Sujeto a } \\sum_{j=1}^{p} \\beta_{j}^{2}=1 \\\\\n&y_i(\\beta_0 +\\beta_0+\\beta_1 x_{i1} +\\beta_2 x_{i2}+\\dots+\\beta_p x_{ip})\\geq M \\quad \\forall i=1,\\dots,n\n\\end{align*}\n```\n\n\n## Construcción de un clasificador de margen máximo: continuación\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(e1071)\nlibrary(ggplot2)\nset.seed(411)\ncoord <- matrix(rnorm(40), 20, 2)\ncolnames(coord) <- c(\"X1\",\"X2\")\ny <- c(rep(-1,10), rep(1,10))\ncoord[y == 1, ] <- coord[y == 1, ] + 1\ndata <- data.frame(coord, y)\nplot_svm<-ggplot(data = data, aes(x = X1, y = X2, color = as.factor(y))) +\n  geom_point(size = 6) +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\ndata$y <- as.factor(data$y)\n```\n:::\n\n\n## Construcción de un clasificador de margen máximo: continuación\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_svm <- svm(formula = y ~ X1 + X2, data = data, kernel = \"linear\",\n                  cost = 10, scale = FALSE)\nsummary(mod_svm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nsvm(formula = y ~ X1 + X2, data = data, kernel = \"linear\", cost = 10, \n    scale = FALSE)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  linear \n       cost:  10 \n\nNumber of Support Vectors:  8\n\n ( 4 4 )\n\n\nNumber of Classes:  2 \n\nLevels: \n -1 1\n```\n:::\n\n```{.r .cell-code}\nmod_svm$index\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  1  5  6  8 12 13 15 18\n```\n:::\n:::\n\n\n## Construcción de un clasificador de margen máximo: continuación\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(mod_svm, data)\n```\n\n::: {.cell-output-display}\n![](lec_week9_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Clasificador de vectores de soporte\n\n![](images/week9/hyperplane_non_sep.png){fig-align=\"center\"}\n\n## Clasificador de vectores de soporte: continuación\n\nEn el caso anterior, un hiperplano separable no existe. En la figura siguiente, la adición de un solo dato provoca un cambio drástico en el margen máximo del hiperplano.\n\n![](images/week9/hyperplane_non_sep_2.png){fig-align=\"center\"}\n\n## Clasificador de vectores de soporte: continuación\n\nEste cambio, reduce la confianza de la asignación de clases. Así, podemos concluir que nuestra metodología es extremadamente sensible a cambio, incluso de sólo una observación.\n\nEn estos casos, quizás deberíamos considerar un clasificador basado en un hiperplano que no separe perfectamente las dos clases, con el fin de:\n\n-   Tener mayor robustez a las observaciones individuales\n\n-   Tener mayor clasificación para la mayoría de las observaciones de entrenamiento\n\n## Clasificador de vectores de soporte: continuación\n\nAsí, podría ser beneficioso clasificar erróneamente un par de observaciones de entrenamiento para realizar un mejor trabajo clasificando el resto de las observaciones.\n\nEl clasificador de vectores de soporte (*support vector classifier*) o aveces llamado *soft margin classifier*, hace exactamente lo anterior; en vez de buscar el marger más grande tal que cada observación que clasifique perfectamente, permite que ciertas observaciones estén en la lado incorrecto del margen, o del lado incorrecto del hiperplano.\n\n## Clasificador de vectores de soporte: continuación\n\nMatemáticamente, corresponde a la solución del siguiente problema de optimización:\n\n\n```{=tex}\n\\begin{align*}\n&\\max_{\\beta_0,\\beta_1,\\dots,\\beta_p; \\epsilon_1,\\dots,\\epsilon_n} \\quad M\\\\\n&\\text{ Sujeto a } \\sum_{j=1}^{p} \\beta_{j}^{2}=1 \\\\\n&y_i(\\beta_0 +\\beta_0+\\beta_1 x_{i1} +\\beta_2 x_{i2}+\\dots+\\beta_p x_{ip})\\geq M (1-\\epsilon_i) \\quad \\forall i=1,\\dots,n \\\\\n&\\epsilon_i \\geq 0, \\sum_{i=1}^{n} \\epsilon_i \\leq C\n\\end{align*}\n```\n\ndonde $C$ es un parámetro de *tunning* no negativo. Cuando este parámetro es grande, habrá una gran tolerancia a que las observaciones están al lado incorrecto del margen (y por ende el margen será grande)\n\n## SVM\n\nSupport Vector Machine (SVM) o Maquina de vectores de soporte es una extensión del clasificador de vectores de soporte que se obtiene tras aumentar el espacio de variables de una manera específica: usando **kernels**.\n\nEn el caso del problema de optimización del clasificador de vectores de soporte, la solución involucra sólo **productos internos** de las observaciones (en contraste con las observaciones mismas).\n\n## SVM: continuación\n\nEl producto interno de dos $r-$vectores $a$ y $b$ se define como $\\langle a,b\\rangle=\\sum_{i=1}^{r} a_i b_i$. Por lo que el producto interno de dos observaciones $x_i,x_{i'}$ está dado por:\n\n\n$$\n\\langle x_i, x_{i'}\\rangle=\\sum_{j=1}^{p}x_{ij}x_{i'j}\n$$\n\n\nMás precisamente, se puede mostrar que:\n\n-   El clasificador de vectores de soportes lineal se puede representar como\n\n    $$\n    f(x)=\\beta_0+\\sum_{i=1}^{n}\\alpha_i\\langle x,x_i\\rangle\n    $$\n\n. . .\n\ndonde hay $n$ parámetros $\\alpha_i, i=1,\\dots,n$, uno para cada observación de entrenamiento.\n\n## SVM: continuación\n\n-   Para estimar los parámetros $\\alpha_1,\\dots,\\alpha_n$ y $\\beta_0$, sólo necesitamos los $\\begin{pmatrix} n \\\\ 2 \\end{pmatrix}$ productos internos $\\langle x_i,x_{i'}\\rangle$ entre todos los pares de observaciones de entrenamiento. (esto es $n(n-1)/2$ pares).\n\n. . .\n\nSupongamos que cada producto interno que hemos definido aparece en la representación del clasificador de vectores de soporte lineal, o en el cálculo del problema de optimización. Reemplazaremos este producto interno con una **generalización** de este, de la forma:\n\n\n$$\nK(x_i,x_{i'})\n$$\n\n\ndonde $K$ es una función que le llamaremos **kernel**.\n\n## SVM: continuación\n\nUn **kernel** es una función que cuantifica la similitud entre dos observaciones. Por ejemplo, podemos tomar:\n\n\n$$\nK(x_i,x_{i'})=\\sum_{j=1}^{p} x_{ij}x_{i'j}\n$$\n\n\nque nos entregaría el clasificar de vectores de soporte. Lo anterior se dice que es un kernel lineal porque el lineal para las *features*. El kernel lineal esencialmente cuantifica la similitud de un par de observaciones usando la correlación de Pearson.\n\n## SVM: continuación\n\nAlternativamente, podemos considerar otra forma de kernel, por ejemplo:\n\n\n$$\nK(x_i,x_{i'})=(1+\\sum_{j=1}^{p} x_{ij}x_{i'j})^{d}\n$$\n\n\nEste kernel es conocido como el **kernel polinomial** de grado $d$, donde $d$ es un entero positivo. Cuando este parámetro es mayor a 1, el clasificador de vectores de soportes tiende a tener un límite de decisión bastante más flexible.\n\n## SVM: continuación\n\nCuando el clasificador de vectores de soporte es combinado con un kernel no lineal (como el anterior), el clasificador resultante se conoce como **support vector machine**.\n\nOtra opción popular de kernel es el **kernel radial**, que tiene la forma:\n\n\n$$\nK(x_i,x_{i'})=\\exp(-\\gamma \\sum_{j=1}^{p}(x_{ij}-x_{i'j})^2)\n$$\n\n\ndonde $\\gamma$ es una constante positiva.\n\n## Figura SVM\n\n![](images/week9/radial_kernel.png){fig-align=\"center\"}\n\n## Ejemplo\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed (411)\nx=matrix(rnorm(200*2), ncol=2)\nx[1:100,]=x[1:100,]+2\nx[101:150,]=x[101:150,]-2\ny=c(rep(1,150),rep(2,50))\ndat=data.frame(x=x,y=as.factor(y))\nplot_radial<-ggplot(dat) +\n  aes(x = x.1, y = x.2, colour = y) +\n  geom_point(shape = \"circle\", size = 1.5) +\n  scale_color_hue(direction = 1) +\n  theme_bw()\n```\n:::\n\n\n## Ejemplo: continuación\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot_radial\n```\n\n::: {.cell-output-display}\n![](lec_week9_files/figure-revealjs/unnamed-chunk-10-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Ejemplo: continuación\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntrain=sample(200,100)\nsvmfit=svm(y~., data=dat[train,], kernel=\"radial\", gamma=1,cost =1)\nplot(svmfit , dat[train ,])\n```\n\n::: {.cell-output-display}\n![](lec_week9_files/figure-revealjs/unnamed-chunk-12-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Ejemplo: continuación\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(svmfit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nsvm(formula = y ~ ., data = dat[train, ], kernel = \"radial\", gamma = 1, \n    cost = 1)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  radial \n       cost:  1 \n\nNumber of Support Vectors:  35\n\n ( 16 19 )\n\n\nNumber of Classes:  2 \n\nLevels: \n 1 2\n```\n:::\n:::\n\n\n## Ejemplo: continuación\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(caret)\nconfusionMatrix(table(true=dat[-train,\"y\"],pred=predict(svmfit, newdata=dat[-train,])))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n    pred\ntrue  1  2\n   1 68  7\n   2  9 16\n                                          \n               Accuracy : 0.84            \n                 95% CI : (0.7532, 0.9057)\n    No Information Rate : 0.77            \n    P-Value [Acc > NIR] : 0.05701         \n                                          \n                  Kappa : 0.5616          \n                                          \n Mcnemar's Test P-Value : 0.80259         \n                                          \n            Sensitivity : 0.8831          \n            Specificity : 0.6957          \n         Pos Pred Value : 0.9067          \n         Neg Pred Value : 0.6400          \n             Prevalence : 0.7700          \n         Detection Rate : 0.6800          \n   Detection Prevalence : 0.7500          \n      Balanced Accuracy : 0.7894          \n                                          \n       'Positive' Class : 1               \n                                          \n```\n:::\n:::\n\n\n\n# Métodos no supervisados\n\nComo hemos mencionado antes, en los métodos no supervisados **sólo** tenemos las variables $X_1,X_2,\\dots,X_p$ medidas en $n$ observaciones. No estaremos interesados en predecir, porque no tenemos una variable respuesta $Y$ que esté asociada a nuestros datos. En cambio, nuestro objetivo será *descubrir* características interesantes de las variables medidas.\n\n-   ¿Hay alguna forma que nos entregue información de visualizar los datos?\n\n-   ¿Podemos encontrar subgrupos entre las variables u observaciones?\n\n. . .\n\nEn lo que sigue nos concentraremos en 2 técnicas particulares: *clustering* y análisis de componentes principales.\n\n## Introducción\n\nEn general, realizar técnicas no supervisadas tiende a ser más difícil que realizar un método supervisado, pues no se tiene un objetivo claro para el análisis (como lo es predecir en el caso supervisado).\n\nUsualmente, las metodologías no supervisadas se realizar como parte del análisis exploratorio de dato. Además, no existe un consenso en la mejor forma de evaluar las técnicas implementadas en datos de prueba. En el caso supervisados, podemos probar nuestro modelo creado con un conjunto de prueba, pero en el caso no supervisado no es posible debido a que no sabemos el respuesta **verdadera**.\n\nEjemplos de aplicación de estas metodologías son vastas:\n\n-   Identificación de cáncer\n\n-   Historiales de compras\n\n-   Etc.\n\n\n## Análisis de componentes principales\n\nEl análisis de componentes principales nos permite resumir, ante un conjunto grande de variables correlacionadas, un subconjunto con menor número de variables representativas que colectivamente explican la mayoría de la variabilidad del conjunto original.\n\nAdemás de producir variables que pueden ser usadas para métodos supervisados, **PCA** (por sus siglas en inglés, *principal component analysis*) sirve como herramienta para visualizar datos.\n\n## ¿Qué son las componentes principales?\n\nSupongamos que deseamos visualizar $n$ observaciones con mediciones en un conjunto de $a$ variables/características/*features*, $X_1,X_2,\\dots,X_p$, como parte de un análisis exploratorio de datos. Podemos examinar los gráficos bidimensionales de dispersión de los datos, que cada una contiene $n$ mediciones de observaciones de 2 variables. Sin embargo, hay $\\begin{pmatrix} p \\\\ 2 \\end{pmatrix}=p(p-1)/2$ de tales gráficos.\n\nPor ejemplo, para $p=10$ habrán 45 gráficos. Por lo que si, $p$ es grande no nos será posible mostrarlos todos, y además ninguno de ellos será informativo debido a que sólo tienen un pequeña fracción del total de información disponible en los datos.\n\nAsí, necesitamos una mejor metodología para poder visualizar las $n$ observaciones cuando $p$ es grande.\n\n## ¿Qué son las componentes principales?: continuación\n\nEn particular, quisieramos encontrar una presentación de baja dimensionalidad de los datos que capture la mayor cantidad de información posible. Por ejemplo, si podemos obtener un diagrama bidimensional de los datos que capture la mayoría de la información, entonces podemos graficar las observaciones en aquel espacio.\n\nPCA nos entrega una herramienta para hacer justamento esto. Encuentra una representación de baja dimensionalidad del conjunto de datos que contiene la mayor variabilidad posible. La idea es que cada una de las $n$ observaciones vive en un espacio $p-$dimensional, pero no todas estas dimensiones son igualmente interesantes.\n\n## ¿Qué son las componentes principales?: continuación\n\nPCA busca un pequeño número de dimensiones que sean lo más interesantes posibles, donde el concepto de *interesante* es medido por la cantidad que varían las observaciones en cada uno de las dimensiones.\n\nCada una de las dimensiones encontradas por PCA es una combinación lineal de $p$ variables.\n\nAhora nos enfocamos en la manera en que PCA encuentra estas dimensiones o componentes principales.\n\n## ¿Qué son las componentes principales?: continuación\n\nEl **primer componente principal** de un conjunto de variables $X_1,X_2,\\dots,X_p$ es la combinación lineal normalizada de las variables\n\n\n$$\nZ_1=\\phi_{11}X_1+\\phi_{21}X_2+\\dots+\\phi_{p1}X_p\n$$\n\n\nque tenga la **mayor varianza**. Por *normalizada*, se refiere a que\n\n\n$$\n\\sum_{j=1}^{p} \\phi_{j1}^{2}=1.\n$$\n\n\nLlamamos a los elementos $\\phi_{11},\\phi_{2 1},\\dots,\\phi_{p1}$reciben en el nombre de *loadings* y son los que definen a la componente. Así, estos elementos conforman el vector de *loadings* de los componentes principales $\\phi_1=(\\phi_{11}\\,\\phi_{21}\\dots\\phi_{p1})'$\n\n## ¿Qué son las componentes principales?: continuación\n\nDado un conjunto de datos $\\mathbf{X}$ de tamaño $n\\times p$. ¿Cómo calculamos la primera componente principal?\n\nDebido a que sólo estamos interesado en la varianza, asumiremos que cada variable de $\\mathbf{X}$ ha sido centrada en cero ( esto es, que las medias de las columnas sean cero). Luego, buscamos la combinación lineal de las variables medidas con forma:\n\n\n$$\nz_{i1}=\\phi_{11}x_{i1}+\\phi_{21}x_{i2}+\\dots+\\phi_{p1}x_{ip}\n$$\n\n\nque tenga la mayor varianza muestral, sujeto a la restricción que $\\sum_{j=1}^{p} \\phi_{j1}^{2}=1$. En otras palabras, el vector de *loadings* de la primera componente principal resuelve el siguiente problema de optimización:\n\n\n$$\n\\max_{\\phi_{11},\\dots,\\phi_{p1}}\\Bigg\\{ \\dfrac{1}{n}\\sum_{i=1}^{n} \\left( \\sum_{j=1}^{p} \\phi_{j1}x_{ij}\\right)\\Bigg\\} \\text{ sujeto a }\\sum_{j=1}^{p}\\phi_{j1}^2=1\n$$\n\n\n\n\n## Reproducibilidad de las componentes\n\nEl proceso de PCA genera siempre las mismas componentes principales independientemente del software utilizado, es decir, el valor de los *loadings* resultantes es el mismo.\n\nLa única discrepancia que podría suceder es que los signos estén invertidos, pues los *loadings* determinan la dirección de la componente.\n\n### Influencia de outliers\n\nAl trabajar con varianzas, el método PCA es altamente sensible a *outliers*, por lo que es altamente recomendable estudiar si los hay. La detección de valores atípicos con respecto a una determinada dimensión es algo relativamente sencillo de hacer mediante comprobaciones gráficas.\n\nLas técnicas diagnóstico de datos anómalos escapa de los objetivos del curso, pero son estudiados en análisis multivariado o modelos lineales (dentro del contexto de regresión)\n\n## Proporción de varianza explicada\n\nUna de las preguntas más frecuentes que surge tras realizar un PCA es: ¿Cuánta información presente en el set de datos original se pierde al proyectar las observaciones en un espacio de menor dimensión? o lo que es lo mismo ¿Cuanta información es capaz de capturar cada una de las componentes principales obtenidas? Para contestar a estas preguntas se recurre a la proporción de varianza explicada por cada componente principal.\n\nAsumiendo que las variables se han estandarizado para tener media cero, la varianza total presente en el set de datos se define como:\n\n\n$$\n\\sum_{j=1}^p Var(X_j) = \\sum_{j=1}^p \\dfrac{1}{n} \\sum_{i=1}^n x^{2}_{ij}\n$$\n\n\n## Proporción de varianza explicada: continuación\n\ny la varianza explicada por la componente $m$ es:\n\n\n$$\n\\dfrac{1}{n} \\sum_{i=1}^n z^{2}_{im} = \\dfrac{1}{n} \\sum_{i=1}^n  \\left( \\sum_{j=1}^p \\phi_{jm}x_{ij} \\right)^2\n$$\n\n\nAsí, la proporción de varianza explicada por la componente $m$ viene dada por\n\n\n$$\n\\dfrac{\\sum_{i=1}^n  \\left( \\sum_{j=1}^p \\phi_{jm}x_{ij} \\right)^2} {\\sum_{j=1}^p \\sum_{i=1}^n x^{2}_{ij}}\n$$\n\n\nEsta proporción y su forma acumulada (a lo largo de las componentes) nos entrega información crucial a la hora de elegir cuantas componentes principales utilizar en nuestro análisis.\n\n## Número óptimo de componentes principales\n\nPor lo general, dada una matriz de datos de dimensiones $n \\times p$, el número de componentes principales que se pueden calcular es como máximo de $\\min\\{n-1,p\\}$. Sin embargo, siendo el objetivo del PCA reducir la **dimensionalidad**, suelen ser de interés utilizar el número mínimo de componentes que resultan suficientes para explicar los datos.\n\nNo existe una respuesta o método único que permita identificar cual es el número óptimo de componentes principales a utilizar. Una forma de proceder muy extendida consiste en evaluar la proporción de varianza explicada acumulada y seleccionar el número de componentes mínimo a partir del cual el incremento deja de ser sustancial.\n\n## Número óptimo de componentes principales: continuación\n\n![](images/week9/optimal_pca.png){fig-align=\"center\"}\n\n## Ejemplo\n\nDatos del porcentaje de asaltos, asesinatos y secuestros por cada 100 mil habitantes para cada uno de los estos de US, en el año 1973. Adicionalmente, se incluye el porcentaje de la población de cada estado que vive en zonas rurales.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"USArrests\")\nhead(USArrests)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           Murder Assault UrbanPop Rape\nAlabama      13.2     236       58 21.2\nAlaska       10.0     263       48 44.5\nArizona       8.1     294       80 31.0\nArkansas      8.8     190       50 19.5\nCalifornia    9.0     276       91 40.6\nColorado      7.9     204       78 38.7\n```\n:::\n\n```{.r .cell-code}\napply(X = USArrests, MARGIN = 2, FUN = mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Murder  Assault UrbanPop     Rape \n   7.788  170.760   65.540   21.232 \n```\n:::\n\n```{.r .cell-code}\napply(X = USArrests, MARGIN = 2, FUN = var)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Murder    Assault   UrbanPop       Rape \n  18.97047 6945.16571  209.51878   87.72916 \n```\n:::\n:::\n\n\nSi no estandarizamos, la variable *Assault* será la que dominará nuestro PCA.\n\n## Ejemplo: continuación\n\n\n::: {.cell}\n\n```{.r .cell-code}\npca <- prcomp(USArrests, scale = TRUE)\nnames(pca)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"       \n```\n:::\n\n```{.r .cell-code}\npca$center\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Murder  Assault UrbanPop     Rape \n   7.788  170.760   65.540   21.232 \n```\n:::\n\n```{.r .cell-code}\npca$scale\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Murder   Assault  UrbanPop      Rape \n 4.355510 83.337661 14.474763  9.366385 \n```\n:::\n\n```{.r .cell-code}\npca$rotation\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                PC1        PC2        PC3         PC4\nMurder   -0.5358995  0.4181809 -0.3412327  0.64922780\nAssault  -0.5831836  0.1879856 -0.2681484 -0.74340748\nUrbanPop -0.2781909 -0.8728062 -0.3780158  0.13387773\nRape     -0.5434321 -0.1673186  0.8177779  0.08902432\n```\n:::\n:::\n\n\n- Elementos *center* y *scale* contienen la media y desviación en escala original.\n\n- Elemento *rotation* contiene el valor de los loadings para cada componente\n\n## Ejemplo: continuación\n\nValor de las componentes principales para cada observación (*principal component scores*)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(pca$x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                  PC1        PC2         PC3          PC4\nAlabama    -0.9756604  1.1220012 -0.43980366  0.154696581\nAlaska     -1.9305379  1.0624269  2.01950027 -0.434175454\nArizona    -1.7454429 -0.7384595  0.05423025 -0.826264240\nArkansas    0.1399989  1.1085423  0.11342217 -0.180973554\nCalifornia -2.4986128 -1.5274267  0.59254100 -0.338559240\nColorado   -1.4993407 -0.9776297  1.08400162  0.001450164\n```\n:::\n\n```{.r .cell-code}\ndim(pca$x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 50  4\n```\n:::\n:::\n\n\n## Ejemplo: continuación\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbiplot(x = pca, scale = 0, cex = 0.6, col = c(\"blue4\", \"brown3\"))\n```\n\n::: {.cell-output-display}\n![](lec_week9_files/figure-revealjs/unnamed-chunk-24-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Ejemplo: continuación\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npca$rotation <- -pca$rotation\npca$x        <- -pca$x\nbiplot(x = pca, scale = 0, cex = 0.6, col = c(\"blue4\", \"brown3\"))\n```\n\n::: {.cell-output-display}\n![](lec_week9_files/figure-revealjs/unnamed-chunk-26-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Ejemplo: continuación\n\n\n::: {.cell}\n\n```{.r .cell-code}\npca$sdev^2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.4802416 0.9897652 0.3565632 0.1734301\n```\n:::\n\n```{.r .cell-code}\nprop_varianza <- pca$sdev^2 / sum(pca$sdev^2)\nprop_varianza\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.62006039 0.24744129 0.08914080 0.04335752\n```\n:::\n\n```{.r .cell-code}\nej_pca<-ggplot(data = data.frame(prop_varianza, pc = 1:4),\n       aes(x = pc, y = prop_varianza)) +\n  geom_col(width = 0.3) +\n  scale_y_continuous(limits = c(0,1)) +\n  theme_bw() +\n  labs(x = \"Componente principal\",\n       y = \"Prop. de varianza explicada\")\n```\n:::\n\n\n## Ejemplo: continuación\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nej_pca\n```\n\n::: {.cell-output-display}\n![](lec_week9_files/figure-revealjs/unnamed-chunk-30-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Ejemplo: continuación\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprop_varianza_acum <- cumsum(prop_varianza)\nprop_varianza_acum\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.6200604 0.8675017 0.9566425 1.0000000\n```\n:::\n\n```{.r .cell-code}\nej_pca2<-ggplot(data = data.frame(prop_varianza_acum, pc = 1:4),\n       aes(x = pc, y = prop_varianza_acum, group = 1)) +\n  geom_point() +\n  geom_line() +\n  theme_bw() +\n  labs(x = \"Componente principal\",\n       y = \"Prop. varianza explicada acumulada\")\n```\n:::\n\n\n## Ejemplo: continuación\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nej_pca2\n```\n\n::: {.cell-output-display}\n![](lec_week9_files/figure-revealjs/unnamed-chunk-34-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Métodos de agrupamiento\n\nLos métodos de agrupamiento o *clustering* hacen referencia a un conjunto de técnicas que tienen por finalidad encontrar subgrupos o *clusters* en un conjuntos de datos dado. Al separar en grupos las observaciones que componen un conjunto de datos, interpretamos que los elementos en un mismo grupo son más *similares* entre sí que con los de otros grupos.\n\nLo anterior, naturalmente provoca la interrogante ¿Qué se considera que dos datos sean *similares* o *distintos*?\n\nGeneralmente, responder a esta pregunta frecuentemente requiere tener consideraciones sobre el dominio o naturaleza de los datos en estudio.\n\n## Métodos de agrupamiento: continuación\n\nPor ejemplo, supongamos que tenemos un conjunto de $n$ observaciones, cada una con $p$ *features*. Las $n$ observaciones podrían corresponder a muestras de tejido de pacientes con cáncer de mamas, y las $p$ características podrían corresponder a las mediciones recogidas desde cada tejido; mediciones clínicas, mediciones sobre la expresión de genes, etc.\n\nPodríamos tener alguna razón para creer que existe alguna tipo de heterogeneidad entre las $n$ muestras de tejidos; por ejemplo, quizás existen algunos sub-tipos diferentes de cáncer de mama. En este caso, las técnicas de agrupamiento podrían ser utilizadas para encontrar estos subgrupos.\n\n## Métodos de agrupamiento: continuación\n\nEste es un problema no-supervisado debido a que estamos tratando de descubrir la estructure en base a un conjunto de datos. Las técnicas de PCA y *clustering* buscan simplificar los datos a través de un pequeño número de abstracciones, pero sus mecanismos son diferentes:\n\n-   PCA busca encontrar una representación de dimensión baja de las observaciones que expliquen una gran parte de la varianza.\n\n-   Los métodos de agrupamiento buscan encontrar subgrupos homogéneos entre las observaciones.\n\n. . .\n\nDebido a que la popularidad y uso de agrupamientos, existen un gran número metodologías de *clustering*. Nos concentraremos un particularmente dos, **K-medias** y **agrupamiento jerárquico**\n\n## Medidas de distancia\n\nTodos los métodos de *clustering* tienen una cosa en común, para poder llevar a cabo las agrupaciones necesitan definir y cuantificar la similitud entre las observaciones. Dentro de este contexto, cuantificaremos aquella similitud usando distintos tipos de distancia entre las observaciones, debido a que en principio podemos escoger cualquier tipo de distancia, hace a esta metodología bastante flexible.\n\n## Distancia euclidiana\n\nPara un espacio euclidiano $n-$dimensional definimos la distancia euclidiana entre dos puntos $p=(p_1,p_2,\\dots,p_n)$ y $q=(q_1,q_2,\\dots,q_n)$ como la cantidad:\n\n\n$$d_{euc}(p,q)\\sqrt{(p_1-q_1)^2+(p_2-q_2)^2+\\dots(p_n-q_n)^2}=\\sqrt{\\sum_{i=1}^n (p_i-q_i)^2}$$\n\n\nElevar esta distancia al cuadrado permite dar más peso a aquellas observaciones que están más alejadas.\n\n## Distancia de Manhattan\n\nLa distancia de Manhattan, también conocida como *taxicab metric* o distancia $L^1$, define la distancia entre dos puntos $p$ y $q$ como la sumatoria de las diferencias absolutas entre cada dimensión. Esta medida es menos sensible a datos anómalos que la distancia euclidiana.\n\nEn este tipo de distancia existen múltiples caminos para unir dos puntos con el mismo valor de distancia de Manhattan, ya que su valor es igual al desplazamiento total en cada una de las dimensiones. Esta distancia está definida como:\n\n\n$$d_{man}(p,q)=\\sum_{i=1}^{n}|(p_i-q_i)|$$\n\n\n## Distancia de Manhattan: continuación\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lec_week9_files/figure-revealjs/unnamed-chunk-36-1.png){width=960}\n:::\n:::\n\n\n## Correlación\n\nLa correlación es una medida de distancia muy útil cuando la definición de simulitud se hace en términos de patrón o forma y no de desplazamiento o magnitud ¿Qué quiere decir esto?\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](lec_week9_files/figure-revealjs/unnamed-chunk-38-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Correlación: continuación\n\nLa correlación la definimos como:\n\n\n$$d_{cor}(p,q)=1-\\text{correlación}(p,q)$$\n\n\nSegún esta definición, tendremos que a una correlación de magnitud 1, la similitud o distancia será cero, y por tanto las consideraremos iguales.\n\nExisten más distancias que podemos ocupar, entre ellas *Jackknife correlation*,*Simple matching coefficient* (para datos binarios), índice Jacard, distancia coseno, entre otras.\n\n## K-medias\n\nEl método de K-medias o $K-means$ agrupa las observaciones en $K$ grupos distintos donde $K$ se determina antes de realizar el análisis. Una vez definido el valor de $k$, esta metodología encuentra los $k$ mejores clusters, entendiendo como mejor *cluster* aquel cuya varianza interna (varianza intra-cluster) sea lo más pequeña posible.\n\nAsí, este método es un problema de optimización, en el que se reparten las observaciones en $K$ *clusters* de forma que la suma de las varianzas internas de todos ellos sea la menor posible. Esto requiere definir un modo de cuantificar la varianza interna.\n\n## K-medias: continuación{.small}\n\nConsideremos $C_1,\\dots,C_k$ como los conjuntos formados por los índices de las observaciones de cada uno de los *clusters*. Esto es, $C_1$ contiene los índices de las observaciones agrupadas en el *cluster* 1. Para indicar la pertenencia de una observación a un *cluster* particular es $i\\in C_k$. Todos estos conjuntos satisfacen las siguientes dos propiedades:\n\n1.  $C_1 \\cup C_2 \\dots \\cup C_k=\\{1,\\dots,n\\}$\n2.  $C_k \\cap C_{k'}=\\emptyset$\n\n. . .\n\nDos medidas frecuentemente utilizadas para definir la varianza interna de cada *cluster* $W(C_k)$ son:\n\n1.  $W(C_k)=\\sum_{x_i\\in C_k} (x_i-\\mu_k)^2$\n2.  $W(C_k)=\\dfrac{1}{|C_k|}\\sum_{i,i'\\in C_k}\\sum_{j=1}^{p}(x_{ij}-x_{i'j})^2$\n\n. . .\n\nMinimizar la suma total de la varianza interna $\\sum_{k=1}^{K}W(C_k)$ de forma exacta (encontrar un mínimo global) es un proceso complejo debido a la inmensa cantidad de formas en las que $n$ observaciones se pueden dividir en $K$ grupos. Sin embargo, es posible obtener una solución que si bien no es la óptima, nos entrega un óptimo local.\n\n## K-medias: continuación\n\nEl algoritmo empleado en este caso es:\n\n1.  Asignar aleatoriamente un número entre 1 y $K$ a cada observación. Esto sirve como asignación inicial aleatoria de las observaciones a los *clusters*.\n2.  Iterar los siguientes pasos hasta que la asignación de las observaciones de los *clusters* no cambie o se alcance un número máximo de iteraciones preestablecido.\n    -   Para cada uno de los *clusters* calcular su centroide (centro de gravedad)\n    -   Asignar cada observación al *cluster* cuyo centroide está más próximo.\n\n. . .\n\nEste algoritmo garantiza que, en cada paso, se reduzca la intra-varianza total de los *clusters* hasta alcanzar un óptimo local.\n\n## Figura: K-medias\n\n![](images/week9/k_means.png){fig-align=\"center\"}\n\n## K-medias: continuación\n\nOtra forma de implementar el algoritmo de K-means clustering es la siguiente:\n\n1.  Especificar el número K de *clusters* que se quieren crear.\n2.  Seleccionar de forma aleatoria k observaciones del set de datos como centroides iniciales.\n3.  Asignar cada una de las observaciones al centroide más cercano.\n4.  Para cada uno de los K *clusters* recalcular su centroide.\n5.  Repetir los pasos 3 y 4 hasta que las asignaciones no cambien o se alcance el número máximo de iteraciones establecido.\n\n. . .\n\nDebido a que el algoritmo de *K-means* no evalúa todas las posibles distribuciones de las observaciones sino solo parte de ellas, los resultados obtenidos dependen de la asignación aleatoria inicial (paso 1). Por esta razón, es importante ejecutar el algoritmo varias veces (25-50), cada una con una asignación aleatoria inicial distinta, y seleccionar aquella que haya conseguido un menor valor de varianza total.\n\n\n## K-means: Ventajas y desventajas\n\n*K-means* es uno de los métodos de clustering más utilizados. Destaca por la sencillez y velocidad de su algoritmo, sin embargo, presenta una serie de limitaciones que se deben tener en cuenta.\n\n1.  Requiere que se indique de antemano el número de *clusters* que se van a crear. Esto puede ser complicado si no se dispone de información adicional sobre los datos con los que se trabaja. Se han desarrollado varias estrategias para ayudar a identificar potenciales valores óptimos de K, aunque todas ellas son orientativas.\n\n2.  Las agrupaciones resultantes pueden variar dependiendo de la asignación aleatoria inicial de los centroides. Para minimizar este problema se recomienda repetir el proceso de clustering entre 25-50 veces y seleccionar como resultado definitivo el que tenga menor suma total de varianza interna. Aun así, solo se puede garantizar la reproducibilidad de los resultados si se emplean semillas.\n\n3. Presenta problemas de robustez frente a outliers. La única solución es excluirlos o recurrir a otros métodos de clustering más robustos como *K-medoids (partitioning around medoids; PAM)*.\n\n## Ejemplo\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(ggpubr)\nset.seed(411)\ndatos <- matrix(rnorm(n = 100*2), nrow = 100, ncol = 2,\n                dimnames = list(NULL,c(\"x\", \"y\")))\ndatos <- as.data.frame(datos)\nmedia_grupos <- matrix(rnorm(n = 8, mean = 0, sd = 4), nrow = 4, ncol = 2,\n                       dimnames = list(NULL, c(\"media_x\", \"media_y\")))\nmedia_grupos <- as.data.frame(media_grupos)\nmedia_grupos <- media_grupos %>% mutate(grupo = c(\"a\",\"b\",\"c\",\"d\"))\ndatos <- datos %>% mutate(grupo = sample(x = c(\"a\",\"b\",\"c\",\"d\"),\n                                         size = 100,\n                                         replace = TRUE))\ndatos <- left_join(datos, media_grupos, by = \"grupo\")\ndatos <- datos %>% mutate(x = x + media_x,\n                          y = y + media_y)\n\nkm1<-ggplot(data = datos, aes(x = x, y = y, color = grupo)) +\n  geom_point(size = 2.5) +\n  theme_bw()\n```\n:::\n\n\n## Ejemplo: continuación\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](lec_week9_files/figure-revealjs/unnamed-chunk-42-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Ejemplo: continuación\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(411)\nkm_clusters <- kmeans(x = datos[, c(\"x\", \"y\")], centers = 4, nstart = 50)\nkm_clusters\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nK-means clustering with 4 clusters of sizes 24, 25, 33, 18\n\nCluster means:\n          x          y\n1 -4.840884 -5.5399183\n2  2.970914  0.3436634\n3  3.119718 -3.9643128\n4 -5.188403 -0.9255762\n\nClustering vector:\n  [1] 4 1 3 2 4 3 1 2 2 3 2 3 3 3 3 4 3 2 3 4 1 3 2 3 2 1 1 2 1 3 3 2 1 1 1 3 3\n [38] 3 2 3 1 3 1 2 2 4 4 2 2 1 1 3 2 2 4 1 1 2 4 1 4 4 3 2 2 4 1 3 1 4 4 2 3 4\n [75] 3 3 3 3 2 4 1 4 2 3 3 3 2 1 4 4 1 3 1 3 2 1 3 3 1 2\n\nWithin cluster sum of squares by cluster:\n[1] 39.08613 55.16049 61.76208 42.71183\n (between_SS / total_SS =  91.4 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n```\n:::\n:::\n\n\n## Ejemplo: continuación\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndatos <- datos %>% mutate(cluster = km_clusters$cluster)\ndatos <- datos %>% mutate(cluster = as.factor(cluster),grupo=as.factor(grupo))\n\nkm2<-ggplot(data = datos, aes(x = x, y = y, color = grupo)) +  geom_text(aes(label = cluster), size = 5) + theme_bw() + theme(legend.position = \"none\")\n```\n:::\n\n\n## Ejemplo: continuación\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](lec_week9_files/figure-revealjs/unnamed-chunk-48-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Ejemplo: continuación\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n       grupo real\ncluster  a  b  c  d\n      1  0  0 24  0\n      2  0 25  0  0\n      3 33  0  0  0\n      4  0  0  1 17\n```\n:::\n:::\n\n\n## Ejemplo: continuación\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndatos <- datos %>% dplyr::select(x,y)\nset.seed(411)\nkm_clusters_2 <- kmeans(x = datos, centers = 2, nstart = 50)\ndatos <- datos %>% mutate(cluster = km_clusters_2$cluster)\np1 <- ggplot(data = datos, aes(x = x, y = y, color = as.factor(cluster))) +\n      geom_point(size = 3) +\n      labs(title = \"Kmeans con k=2\") +\n      theme_bw() +\n      theme(legend.position = \"none\")\n\ndatos <- datos %>% dplyr::select(x, y)\n\nkm_clusters_6 <- kmeans(x = datos, centers = 6, nstart = 50)\ndatos <- datos %>% mutate(cluster = km_clusters_6$cluster)\np2 <- ggplot(data = datos, aes(x = x, y = y, color = as.factor(cluster))) +\n      geom_point(size = 3) +\n      labs(title = \"Kmeans con k=6\") +\n      theme_bw() +\n      theme(legend.position = \"none\")\n```\n:::\n\n\n## Ejemplo: continuación\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(gridExtra)\nggarrange(p1, p2)\n```\n\n::: {.cell-output-display}\n![](lec_week9_files/figure-revealjs/unnamed-chunk-54-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Agrupamiento jerárquico\n\nAgrupamiento jerárquico o *Hierarchical clustering* es una alternativa no requiere que se pre-especifique el número de *clusters*. Los métodos que engloba el *hierarchical clustering* se subdividen en dos tipos dependiendo de la estrategia seguida para crear los grupos:\n\n- *Agglomerative clustering (bottom-up)*: el agrupamiento se inicia en la base del árbol, donde cada observación forma un *cluster* individual. Los *clusters* se van combinado a medida que la estructura crece hasta converger en una única \"rama\" central.\n\n- *Divisive clustering (top-down)*: es la estrategia opuesta al *agglomerative clustering*, se inicia con todas las observaciones contenidas en un mismo *cluster* y se suceden divisiones hasta que cada observación forma un *cluster* individual.\n\n. . .\n\nEn ambos casos, los resultados pueden representarse de forma muy intuitiva en una estructura de árbol llamada dendrograma.\n\n## Agglomerative\n\nLa estructura resultante de un *agglomerative hierarchical clustering* se obtiene mediante un algoritmo sencillo.\n\n1. El proceso se inicia considerando cada una de las observaciones como un cluster individual, formando así la base del dendrograma (hojas).\n\n2. Se inicia un proceso iterativo hasta que todas las observaciones pertenecen a un único cluster:\n\n    - Se calcula la distancia entre cada posible par de los $n$ clusters. El investigador debe determinar el tipo de medida emplea para cuantificar la similitud entre observaciones o grupos (distancia y linkage).\n\n    - Los dos clusters más similares se fusionan, de forma que quedan $n-1$ clusters.\n\n3. Determinar dónde cortar la estructura de árbol generada (dendrograma).\n\n## Figura H.C. agglomerative\n\n![](images/week9/agglomerative.png){fig-align=\"center\"}\n\n## Agglomerative: continuación{.small}\n\nPara que el proceso de agrupamiento pueda llevarse a cabo tal como indica el algoritmo anterior, es necesario definir cómo se cuantifica la similitud entre dos *clusters*. Es decir, se tiene que extender el concepto de distancia entre pares de observaciones para que sea aplicable a pares de grupos, cada uno formado por varias observaciones. A este proceso se le conoce como **linkage**. A continuación, se describen los tipos de linkage más empleados y sus definiciones.\n\n1. **Complete or Maximum**: Se calcula la distancia entre todos los posibles pares formados por una observación del *cluster* A y una del *cluster* B. La mayor de todas ellas se selecciona como la distancia entre los dos *clusters*. Se trata de la medida más conservadora (*maximal intercluster dissimilarity*).\n\n2. **Single or Minimum**: Se calcula la distancia entre todos los posibles pares formados por una observación del *cluster* A y una del *cluster* B. La menor de todas ellas se selecciona como la distancia entre los dos clusters. Se trata de la medida menos conservadora (*minimal intercluster dissimilarity*).\n\n3. **Average**: Se calcula la distancia entre todos los posibles pares formados por una observación del *cluster* A y una del *cluster* B. El valor promedio de todas ellas se selecciona como la distancia entre los dos clusters (*mean intercluster dissimilarity*).\n\n## Agglomerative: continuación\n\n\n4. **Centroid**: Se calcula el centroide de cada uno de los *clusters* y se selecciona la distancia entre ellos como la distancia entre los dos clusters.\n\n5. **Ward**: Se trata de un método general. La selección del par de *clusters* que se combinan en cada paso del *agglomerative hierarchical clustering* se basa en el valor óptimo de una función objetivo, pudiendo ser esta última cualquier función definida por el analista. El conocido método *Ward’s minimum variance* es un caso particular en el que el objetivo es minimizar la suma total de varianza intra-cluster.\n\n. . .\n\nLos métodos de *linkage complete, average* y *Ward’s minimum variance* suelen ser los preferidos por los analistas debido a que generan dendrogramas más compensados. Sin embargo, no se puede determinar que uno sea mejor que otro, ya que depende del caso de estudio en cuestión.\n\n\n## Divisive\n\nEl algoritmo más conocido de **divisive hierarchical clustering** es DIANA (*DIvisive ANAlysis Clustering*). Este algoritmo se inicia con un único *cluster* que contiene todas las observaciones, a continuación, se van sucediendo divisiones hasta que cada observación forma un *cluster* independiente. En cada iteración, se selecciona el *cluster* con mayor diámetro, entendiendo por diámetro de un *cluster* la mayor de las diferencias entre dos de sus observaciones. \n\nUna vez seleccionado el *cluster*, se identifica la observación más dispar, que es aquella con mayor distancia promedio respecto al resto de observaciones que forman el *cluster*, esta observación inicia el nuevo *cluster*. Se reasignan las observaciones en función de si están más próximas al nuevo *cluster* o al resto de la partición, dividiendo así el *cluster* seleccionado en dos nuevos *clusters*.\n\n\n## Divisive: continuación\n\n1. Todas las n observaciones forman un único cluster.\n\n2. Repetir hasta que hayan n clusters:\n\n    - Calcular para cada *cluster* la mayor de las distancias entre pares de observaciones (diámetro del *cluster*).\n    - Seleccionar el *cluster* con mayor diámetro.\n    - Calcular la distancia media de cada observación respecto a las demás.\n    - La observación más distante inicia un nuevo *cluster*.\n    - Se reasignan las observaciones restantes al nuevo *cluster* o al viejo dependiendo de cual está más próximo.\n    \n. . .\n\nA diferencia del *clustering* aglomerativo, en el que hay que elegir un tipo de distancia y un método de *linkage*, en el *clustering* divisivo sólo hay que elegir la distancia.\n\n## Dendograma\n\nPara ilustrar cómo se interpreta un dendograma, se simula un set de datos y se somete a un proceso de hierarchical clustering.\n\nSupongamos que se dispone de 45 observaciones en un espacio de dos dimensiones, que pertenecen a 3 grupos. Aunque se ha coloreado de forma distinta cada uno de los grupos, se va a suponer que se desconoce esta información y que se desea aplicar el método de *hierarchical clustering* para intentar reconocer los grupos.\n\n## Dendograma: continuación\n\n![](images/week9/hierarchical.png){fig-align=\"center\"}\n\n## Dendograma: continuación\n\nAl aplicar *hierarchical clustering*, empleando como medida de similitud la distancia euclídea y *linkage complete*, se obtiene el siguiente dendrograma. Como los datos se han simulado en aproximamdamente la misma escala, no es necesario estandarizarlos, de no ser así, sí se tendrían que estandarizar.\n\n![](images/week9/hierarchical2.png){fig-align=\"center\"}\n\n\n## Verificación del árbol resultante\n\nUna vez creado el dendrograma, hay que evaluar hasta qué punto su estructura refleja las distancias originales entre observaciones. Una forma de hacerlo es empleando el coeficiente de correlación entre las distancias *cophenetic* del dendrograma (altura de los nodos) y la matriz de distancias original. \n\nCuanto más cercano es el valor a 1, mejor refleja el dendrograma la verdadera similitud entre las observaciones. Valores superiores a 0.75 suelen considerarse como buenos.\n\n## Ejemplo\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(USArrests)\ndatos <- scale(USArrests)\nmat_dist <- dist(x = datos, method = \"euclidean\")\nhc_euclidiana_complete <- hclust(d = mat_dist, method = \"complete\")\nhc_euclidiana_average  <- hclust(d = mat_dist, method = \"average\")\ncor(x = mat_dist, cophenetic(hc_euclidiana_complete))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.6979437\n```\n:::\n\n```{.r .cell-code}\ncor(x = mat_dist, cophenetic(hc_euclidiana_average))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.7180382\n```\n:::\n:::\n\n\nPara estos datos, el método de *linkage average* consigue representar un poco mejor la similitud entre observaciones.\n\n## Corte del árbol\n\nAdemás de representar en un dendrograma la similitud entre observaciones, se tiene que poder identificar el número de *clusters* creados y qué observaciones forman parte de cada uno. Si se realiza un corte horizontal a una determinada altura del dendrograma, el **número de ramas que sobrepasan** (en sentido ascendente) dicho corte se corresponde con el número de clusters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(factoextra)\ndatos <- USArrests\ndatos <- scale(datos)\nset.seed(411)\n\nhc_euclidiana_completo <- hclust(d = dist(x = datos, method = \"euclidean\"),\n                               method = \"complete\")\n\ndendograma1<-fviz_dend(x = hc_euclidiana_completo, k = 2, cex = 0.6) +\n  geom_hline(yintercept = 5.5, linetype = \"dashed\") +\n  labs(title = \"Herarchical clustering\",\n       subtitle = \"Distancia euclídea, Linkage complete, K=2\")\n```\n:::\n\n\n\n## Corte del árbol: continuación\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](lec_week9_files/figure-revealjs/unnamed-chunk-60-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Corte del árbol: continuación\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndendograma2<-fviz_dend(x = hc_euclidiana_completo, k = 4, cex = 0.6) +\n  geom_hline(yintercept = 3.5, linetype = \"dashed\") +\n  labs(title = \"Herarchical clustering\",\n       subtitle = \"Distancia euclídea, Linkage complete, K=4\")\n```\n:::\n\n\n## Corte del árbol: continuación\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](lec_week9_files/figure-revealjs/unnamed-chunk-64-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Corte del árbol: continuación\n\nDos propiedades adicionales se derivan de la forma en que se generan los *clusters* en el método de *hierarchical clustering*:\n\n1. Dada la longitud variable de las ramas, siempre existe un intervalo de altura para el que cualquier corte da lugar al mismo número de *clusters*. \n\n2. Con un solo dendrograma se dispone de la flexibilidad para generar cualquier número de clusters desde 1 a $n$. La selección del número óptimo puede valorarse de forma visual, tratando de identificar las ramas principales en base a la altura a la que ocurren las uniones. \n\n# ¿Qué veremos la próxima semana?\n\n- Certamen #2\n\n# ¿Que deben preparar para la próxima semana?\n\n- Estudiar para el certamen\n\n",
    "supporting": [
      "lec_week9_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n\r\n      // dispatch for htmlwidgets\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for reveal\r\n    if (window.Reveal) {\r\n      window.Reveal.addEventListener(\"slidechanged\", function(event) {\r\n        fireSlideChanged(event.previousSlide, event.currentSlide);\r\n      });\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}