{
  "hash": "bfc2da53dfee72450379f67ce4572a81",
  "result": {
    "markdown": "---\ntitle: \"Redes neuronales convolucionales\"\nsubtitle: \"IND 163 - 2022/02\"\nauthor: \"Eloy Alvarado Narváez\"\ninstitute: \"Universidad Técnica Federico Santa María\"\ndate: 18/11/22\nformat: \n  revealjs:\n    theme: slides.scss\n    touch: true\n    slide-level: 2\n    code-copy: true\nincremental: true\nslide-number: true\nlang: es\nhighlight-style: github\nwidth: 1600\nheight: 900\nlogo: images/logo_usm.png\ntransition: fade\nfooter: \"IND 163 - Semana 12\"\nexecute:\n  freeze: auto\neditor: \n  markdown: \n    wrap: 72\n---\n\n\n# Red Neuronal Convolucional\n\n**Convolutional Neural Networks** o **CNN**, son un tipo especial de redes neuronales para procesar datos que tienen una topología en forma de cuadrícula conocida. Como por ejemplo, series de tiempo, que puedes ser pensados como una malla 1-dimensional que toman datos a intervalos regulares, datos de imagen, que pueden ser pensados como una malla 2-dimensional.\n\nEste tipo de redes neuronales ha sido **muy** exitoso en la industria y aplicaciones generales.\n\nEl nombre **red neuronal convolucional** indica que la red utiliza una operación matemática especifica: la **convolución**, esta es un tipo especial de operación lineal.\n\n**Las redes neuronales convolucionales son simplemente redes neuronales que usan convolución en lugar de una multiplicación matricial en al menos una de sus capas**\n\n## Convolución\n\nEn su forma general, la convolución es una operación sobre dos funciones con argumentos reales.\n\nSupongamos que estamos rastreando la ubicación de una nave espacial con un sensor láser. Nuestro sensor láser nos entrega una sola salida $x(t)$, la posición de la nave espacial en el tiempo $t$, en donde $x$ y $t$ son valores reales.\n\nAhora supongamos que nuestro sensor láser es *algo ruidoso*. Para obtener una estimación menos ruidosa de la posición de la nave, podríamos promediar muchas mediciones, siendo las mediciones más recientes más relevantes, por lo que sería un promedio ponderado que otorga más peso a las observaciones más recientes.\n\n## Convolución: continuación\n\nPodemos hacer esto con una función $w(a)$, donde $a$ es la *edad* de la medición. Si deseamos aplicar la operación de ponderación en cada momento, debemos obtener una nueva función $s$ que entregue una estimación suavizada de la posición de la nave:\n\n$$s(t)=\\int x(a)w(t-a)da$$ \n\nEsta operación es llamada **convolución**. La operación de convolución se denota típicamente como:\n\n$$s(t)=(x * w)(t)$$\n\n## Convolución: continuación\n\nEn nuestro caso, $w$ necesita ser una función de densidad de probabilidad válida, sino la salida no sería una ponderación. Además, $w$ necesita ser 0 para todos los argumentos negativos, o esta función \"*mirará en el futuro*''. Estas limitaciones son particulares de nuestro ejemplo.\n\nEn general, la convolución está definida para cualquier función para la que la integral anterior está bien definida, y puede ser ocupada con otros fines.\n\nEn este contexto, el primer argumento ($x$) se le llama **input** y el segundo argumento ($w$) se le llama **kernel**, y a la salida se le llama **feature map**.\n\n\n## Convolución: continuación{.small}\n\nEn nuestro ejemplo, la idea de que el sensor láser entregue medidas en cada instante de tiempo no es realista, pues trabajamos con una discretización del tiempo, usualmente a tiempos regulares. Así, tendremos:\n\n$$s(t)=(x*w)(t)=\\sum_{a=-\\infty}^{\\infty}x(a)w(t-a)$$\n\nFrecuentemente usamos convoluciones sobre más de un eje en un tiempo especifico. Por ejemplo, si usamos una imagen 2-dimensional $I$ como **input**, probablemente desearemos usar un kernel $K$ 2-dimensional:\n\n$$S(i,j)=(I*K)(i,j)=\\sum_m \\sum_n I(m,n)K(i-m,j-n)$$\n\nLa convolución es **conmutativa**, esto significa que equivalentemente podemos escribir:\n\n$$S(i,j)=(K*I)(i,j)=\\sum_m \\sum_n I(i-m,j-n)K(m,n)$$\n\n## Motivación\n\nLa convolución aprovecha tres ideas importantes que pueden ayudar a mejorar el aprendizaje de una *máquina*:\n\n-   **sparse interactions**\n-   **parameter sharing**\n-   **equivariant representation**\n\n. . .\n\nAdemás de permitir trabajar con entradas de tamaño variable.\n\n\n## Sparse interaction\n\nInteracciones escasas o sparse interactions (que también se le refiere como **sparse connectivity** o **sparse weights**), viene desde la idea: Las capas de una red neuronal tradicional usan multiplicación de matrices por una matriz de parámetros con un parámetro separado que describe la interacción entre cada unidad de entrada y cada unidad de salida.\n\nEste implica que cada unidad de salida interactúa con cada unidad de entrada. Las redes convolucionales, en cambio, no necesariamente. Este es logrado utilizando **kernels** más pequeños que la entrada.\n\nPor ejemplo, cuando se procesa una imagen, la entrada podría tener millones de pixeles, pero podemos detectar unas pequeñas pero relevantes características, que al interactuar con el *kernel* ocupan sólo cientos de pixeles. Esto implica que tendremos que guardar menos parámetros, que reduce la memoria requerida del modelo y mejora su eficiencia estadística.\n\n\n## Sparse interaction: continuación\n\nEn términos más formales, en una red neuronal con $y\\in \\mathbb{R}^{n}, x\\in \\mathbb{R}^m$. Necesitamos realizar la multiplicación matricial $y=Wx$ para calcular las *activaciones* para cada capa, en donde cada salida interactúa con cada entrada.\n\nDebido a que las redes convolucionales tienen *interacciones más escasas* al usar *kernels* más pequeños, el calculo de la red pasada de necesitar $O(m\\times n)$ a $O(k\\times x)$ operaciones.\n\n## Sparse interaction: continuación\n\n\n::: {layout-ncol=2}\n\n![](images/week12/sparse.png){fig-align=\"center\"} \n\n![](images/week12/sparse2.png){fig-align=\"center\"}\n\n:::\n\n## Sparse interaction: continuación\n\n![](images/week12/sparse3.png){fig-align=\"center\"}\n\n\n## Parameter sharing y equivariance representation\n\nEsta característica hace referencia a usar los mismos parámetros para más de una función de activación, reduciendo así, el número de parámetros a optimizar y mejorando la eficiencia estadística.\n\nConfigurando particularmente los parámetros, podemos obtener la propiedad de representación de equivalencia, que refiere a que si las entradas cambian, las salidas cambian **en la misma manera**.\n\n## Ejemplo\n\nCargamos las librerías de keras y tensorflow\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\nlibrary(tensorflow)\n#tensorflow::install_tensorflow()\n#tensorflow::tf_config()\n```\n:::\n\n\n\n## Ejemplo: continuación\n\n60000 imágenes de entrenamiento y 10000 de prueba de números escritos a mano.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmnist <- dataset_mnist()\nx_train <- mnist$train$x\ny_train <- mnist$train$y\nx_test <- mnist$test$x\ny_test <- mnist$test$y\ndim(x_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 60000    28    28\n```\n:::\n:::\n\n\n## Ejemplo: continuación\n\n![](images/week12/zip_codes.png){fig-align=\"center\"}\n\n## Ejemplo: continuación\n\nArreglamos la forma y reescalamos.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# reshape\nx_train <- array_reshape(x_train, c(nrow(x_train), 784))\nx_test <- array_reshape(x_test, c(nrow(x_test), 784))\n\n# rescale\nx_train <- x_train / 255\nx_test <- x_test / 255\n\n# categorización de la variable respuesta\n\ny_train <- to_categorical(y_train, 10)\ny_test <- to_categorical(y_test, 10)\n```\n:::\n\n\n## Ejemplo: continuación\n\nDefinimos el modelo\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential() \nmodel %>% \n  layer_dense(units = 256, activation = 'relu', input_shape = c(784)) %>% \n  layer_dropout(rate = 0.4) %>% \n  layer_dense(units = 128, activation = 'relu') %>%\n  layer_dropout(rate = 0.3) %>%\n  layer_dense(units = 10, activation = 'softmax')\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"sequential\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n dense_2 (Dense)                    (None, 256)                     200960      \n dropout_1 (Dropout)                (None, 256)                     0           \n dense_1 (Dense)                    (None, 128)                     32896       \n dropout (Dropout)                  (None, 128)                     0           \n dense (Dense)                      (None, 10)                      1290        \n================================================================================\nTotal params: 235,146\nTrainable params: 235,146\nNon-trainable params: 0\n________________________________________________________________________________\n```\n:::\n:::\n\n\n## Ejemplo: continuación\n\nCompilamos y entrenamos el modelo:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel %>% compile(\n  loss = 'categorical_crossentropy',\n  optimizer = optimizer_rmsprop(),\n  metrics = c('accuracy')\n)\n\nhistory <- model %>% fit(\n  x_train, y_train, \n  epochs = 30, batch_size = 128, \n  validation_split = 0.2\n)\nhistory\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nFinal epoch (plot to see history):\n        loss: 0.05107\n    accuracy: 0.9859\n    val_loss: 0.1283\nval_accuracy: 0.978 \n```\n:::\n:::\n\n\n## Ejemplo: continuación\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(history)\n```\n\n::: {.cell-output-display}\n![](lec_week12_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Ejemplo: continuación\n\nEvaluamos el modelo en el conjunto de prueba:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>% evaluate(x_test, y_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    loss accuracy \n0.101623 0.981200 \n```\n:::\n:::\n\n\nGeneramos predicciones:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred <- model %>% predict(x_test) %>% k_argmax()\n```\n:::\n\n\n\n## Ejemplo 2\n\n50000 imágenes de 32x32 pixeles a color clasificado en 10 categorías.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncifar <- dataset_cifar10()\nclass_names <- c('airplane', 'automobile', 'bird', 'cat', 'deer',\n               'dog', 'frog', 'horse', 'ship', 'truck')\n\nindex <- 1:30\n```\n:::\n\n\n\n## Ejemplo 2: continuación\n\n![](images/week12/cifar.png){fig-align=\"center\"}\n\n## Ejemplo: continuación\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lec_week12_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n\n## Ejemplo 2: continuación\n\nDefinimos el modelo\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential() %>% \n  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = \"relu\", \n                input_shape = c(32,32,3)) %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = \"relu\") %>% \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = \"relu\")\n\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"sequential_1\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n conv2d_2 (Conv2D)                  (None, 30, 30, 32)              896         \n max_pooling2d_1 (MaxPooling2D)     (None, 15, 15, 32)              0           \n conv2d_1 (Conv2D)                  (None, 13, 13, 64)              18496       \n max_pooling2d (MaxPooling2D)       (None, 6, 6, 64)                0           \n conv2d (Conv2D)                    (None, 4, 4, 64)                36928       \n================================================================================\nTotal params: 56,320\nTrainable params: 56,320\nNon-trainable params: 0\n________________________________________________________________________________\n```\n:::\n:::\n\n\n## Ejemplo 2: continuación\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>% \n  layer_flatten() %>% \n  layer_dense(units = 64, activation = \"relu\") %>% \n  layer_dense(units = 10, activation = \"softmax\")\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"sequential_1\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n conv2d_2 (Conv2D)                  (None, 30, 30, 32)              896         \n max_pooling2d_1 (MaxPooling2D)     (None, 15, 15, 32)              0           \n conv2d_1 (Conv2D)                  (None, 13, 13, 64)              18496       \n max_pooling2d (MaxPooling2D)       (None, 6, 6, 64)                0           \n conv2d (Conv2D)                    (None, 4, 4, 64)                36928       \n flatten (Flatten)                  (None, 1024)                    0           \n dense_4 (Dense)                    (None, 64)                      65600       \n dense_3 (Dense)                    (None, 10)                      650         \n================================================================================\nTotal params: 122,570\nTrainable params: 122,570\nNon-trainable params: 0\n________________________________________________________________________________\n```\n:::\n:::\n\n\n## Ejemplo 2: continuación\n\nCompilamos y entrenamos el modelo:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel %>% compile(\n  optimizer = \"adam\",\n  loss = \"sparse_categorical_crossentropy\",\n  metrics = \"accuracy\"\n)\n\nhistory <- model %>% \n  fit(\n    x = cifar$train$x, y = cifar$train$y,\n    epochs = 10,\n    validation_data = unname(cifar$test),\n    verbose = 2\n  )\nhistory\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nFinal epoch (plot to see history):\n        loss: 0.741\n    accuracy: 0.743\n    val_loss: 1.117\nval_accuracy: 0.6435 \n```\n:::\n:::\n\n\n## Ejemplo 2: continuación\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(history)\n```\n\n::: {.cell-output-display}\n![](lec_week12_files/figure-revealjs/unnamed-chunk-14-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Ejemplo 2: continuación\n\n\n::: {.cell}\n\n```{.r .cell-code}\nevaluate(model, cifar$test$x, cifar$test$y, verbose = 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    loss accuracy \n1.116902 0.643500 \n```\n:::\n:::\n\n\n\n\n\n# ¿Qué veremos la próxima semana?\n\n- Otros tópicos de ML para industria\n\n# ¿Que deben preparar para la próxima semana?\n\n- Preparar informe escrito de avance de proyecto\n\n",
    "supporting": [
      "lec_week12_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n\r\n      // dispatch for htmlwidgets\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for reveal\r\n    if (window.Reveal) {\r\n      window.Reveal.addEventListener(\"slidechanged\", function(event) {\r\n        fireSlideChanged(event.previousSlide, event.currentSlide);\r\n      });\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}