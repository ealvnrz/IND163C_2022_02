{
  "hash": "dbb87e719e791fa878c3c553d5daca7b",
  "result": {
    "markdown": "---\ntitle: \"Métodos supervisados: continuación\"\nsubtitle: \"IND 163 - 2022/02\"\nauthor: \"Eloy Alvarado Narváez\"\ninstitute: \"Universidad Técnica Federico Santa María\"\ndate: 14/10/22\nformat: \n  revealjs:\n    theme: slides.scss\n    touch: true\n    slide-level: 2\n    code-copy: true\nincremental: true\nslide-number: true\nlang: es\nhighlight-style: github\nwidth: 1600\nheight: 900\nlogo: images/logo_usm.png\ntransition: fade\nfooter: \"IND 163 - Semana 8\"\nexecute:\n  freeze: auto\neditor: \n  markdown: \n    wrap: 72\n---\n\n\n# Métodos supervisados: continuación\n\n## K-vecinos cercanos\n\nComo sabemos, un clasificador Bayesiano tiene la forma:\n\n\n$$\n\\mathbb{P}(Y=j|X=x_o)\n$$\n\n\nque es simplemente una probabilidad condicional. Sin embargo, tomamos este clasificador como el idóneo no-obtenible, debido a que nos entrega el error de testeo. En la práctica, este clasificador no es alcanzable debido a que no sabemos la distribución condicional de $Y$ dado $X$.\n\n## K-vecinos cercanos: continuación\n\nUna metodología que intenta estimar la probabilidad condicional para luego asignar la clase $k-$ésima a la observación que tenga la mayor probabilidad condicional es **K-vecinos cercanos** o **KNN** por sus siglas en inglés.\n\nDada un entero positivo $K$, una observación de prueba $x_0$, el clasificador $KNN$ primero identifica los $K$ puntos más cercanos a $x_0$ pertenecientes al conjunto de entrenamiento, representados por $\\mathcal{N}_0$. Luego estima la probabilidad condicional para la clase $j-$ésima como una fracción de puntos en $\\mathcal{N}_0$ cuyas respuestas son igual a la de $j$, esto es:\n\n\n$$\n\\mathbb{P}(Y=j|X=x_0)=\\dfrac{1}{K}\\sum_{i\\in \\mathcal{N}_0}I(y_i = j)\n$$\n\n\nFinalmente, **KNN** aplica la regla de Bayes y clasifica la observación de prueba/testeo $x_0$ a la clase con la mayor probabilidad\n\n## RL vs LDA vs QDA vs KNN\n\nEs natural preguntarse que técnica utilizar en distintas circunstancias, pues todas ellas tienen por finalidad clasificar observaciones. En lo que sigue se lista comentarios respecto a los nexos entre estas metodologías.\n\n-   Debido a que RL y LDA producen límites de decisión lineales, usualmente entregan resultados similares.\n\n-   Debido a los supuestos distribucionales de LDA, si estos se cumplen, suele entregar mejores resultados que una regresión logística. De no cumplirse los supuestos, la regresión puede superar a LDA.\n\n-   KNN al tener un enfoque enteramente no-paramétrico, esto es: no asume nada sobre la forma del límite de decisión. Si el límite de decisión es altamente no-lineal, KNN superará a la regresión logística y LDA. Sin embargo, no tendremos información de cuales predictores son importantes.\n\n## RL vs LDA vs QDA vs KNN: continuación \n\n-   QDA puede ser visto como un punto medio entre KNN y LDA/RL. Como el QDA asume un límite de decisión cuadrático, puede modelar más problemas que al asumir linealidad.\n\n-   Si bien QDA no es tan flexible como KNN, puede entregar mejores resultados bajo un número limitado de observaciones de entrenamiento debido a que se hacen ciertos supuestos sobre la forma del límite de decisión.\n\n# Ejemplos para un mismo conjunto de datos\n\nUtilizaremos un conjunto de datos de rendimientos porcentuales de las acciones **S&P 500** a lo largo de 1250 días, desde principios de 2001 hasta finales de 2005. Para cada día, se registraron los rendimientos porcentuals para cada uno de los 5 días hábiles previos (`lag1` a `lag5`). También se registró el volumen de acciones tranzadas en el día anterior (en billones) (variable `Volume`), el rendimiento porcentual del día en cuestión (variable `Today`) y la dirección, que representa si el mercado va hacia la alta o baja.\n\n## Análisis exploratiorio\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ISLR)\nnames(Smarket)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Year\"      \"Lag1\"      \"Lag2\"      \"Lag3\"      \"Lag4\"      \"Lag5\"     \n[7] \"Volume\"    \"Today\"     \"Direction\"\n```\n:::\n\n```{.r .cell-code}\ndim(Smarket)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1250    9\n```\n:::\n\n```{.r .cell-code}\nhead(Smarket)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Year   Lag1   Lag2   Lag3   Lag4   Lag5 Volume  Today Direction\n1 2001  0.381 -0.192 -2.624 -1.055  5.010 1.1913  0.959        Up\n2 2001  0.959  0.381 -0.192 -2.624 -1.055 1.2965  1.032        Up\n3 2001  1.032  0.959  0.381 -0.192 -2.624 1.4112 -0.623      Down\n4 2001 -0.623  1.032  0.959  0.381 -0.192 1.2760  0.614        Up\n5 2001  0.614 -0.623  1.032  0.959  0.381 1.2057  0.213        Up\n6 2001  0.213  0.614 -0.623  1.032  0.959 1.3491  1.392        Up\n```\n:::\n:::\n\n\n## Análisis exploratorio: continuación\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\ncor(Smarket[,-9])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             Year         Lag1         Lag2         Lag3         Lag4\nYear   1.00000000  0.029699649  0.030596422  0.033194581  0.035688718\nLag1   0.02969965  1.000000000 -0.026294328 -0.010803402 -0.002985911\nLag2   0.03059642 -0.026294328  1.000000000 -0.025896670 -0.010853533\nLag3   0.03319458 -0.010803402 -0.025896670  1.000000000 -0.024051036\nLag4   0.03568872 -0.002985911 -0.010853533 -0.024051036  1.000000000\nLag5   0.02978799 -0.005674606 -0.003557949 -0.018808338 -0.027083641\nVolume 0.53900647  0.040909908 -0.043383215 -0.041823686 -0.048414246\nToday  0.03009523 -0.026155045 -0.010250033 -0.002447647 -0.006899527\n               Lag5      Volume        Today\nYear    0.029787995  0.53900647  0.030095229\nLag1   -0.005674606  0.04090991 -0.026155045\nLag2   -0.003557949 -0.04338321 -0.010250033\nLag3   -0.018808338 -0.04182369 -0.002447647\nLag4   -0.027083641 -0.04841425 -0.006899527\nLag5    1.000000000 -0.02200231 -0.034860083\nVolume -0.022002315  1.00000000  0.014591823\nToday  -0.034860083  0.01459182  1.000000000\n```\n:::\n:::\n\n## Análisis exploratorio: continuación\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(DataExplorer)\nplot_correlation(Smarket[,-9])\n```\n\n::: {.cell-output-display}\n![](lec_week8_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Análisis exploratorio: continuación\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nattach(Smarket)\nggplot(Smarket) + aes(x = 1:nrow(Smarket), y = Volume) +geom_point(shape = \"circle\", size = 1.5, colour = \"#4682B4\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](lec_week8_files/figure-revealjs/unnamed-chunk-8-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Regresión logística\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,data = Smarket , family = binomial )\nsummary(glm.fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + \n    Volume, family = binomial, data = Smarket)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-1.446  -1.203   1.065   1.145   1.326  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)\n(Intercept) -0.126000   0.240736  -0.523    0.601\nLag1        -0.073074   0.050167  -1.457    0.145\nLag2        -0.042301   0.050086  -0.845    0.398\nLag3         0.011085   0.049939   0.222    0.824\nLag4         0.009359   0.049974   0.187    0.851\nLag5         0.010313   0.049511   0.208    0.835\nVolume       0.135441   0.158360   0.855    0.392\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1731.2  on 1249  degrees of freedom\nResidual deviance: 1727.6  on 1243  degrees of freedom\nAIC: 1741.6\n\nNumber of Fisher Scoring iterations: 3\n```\n:::\n:::\n\n\n## Regresión logística: continuación\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm.probs=predict(glm.fit,type=\"response\")\nglm.probs[1:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        1         2         3         4         5         6         7         8 \n0.5070841 0.4814679 0.4811388 0.5152224 0.5107812 0.5069565 0.4926509 0.5092292 \n        9        10 \n0.5176135 0.4888378 \n```\n:::\n\n```{.r .cell-code}\ncontrasts(Direction)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Up\nDown  0\nUp    1\n```\n:::\n\n```{.r .cell-code}\nglm.pred=rep(\"Down\",1250)\nglm.pred[glm.probs>.5]=\"Up\"\ntable(glm.pred,Direction)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        Direction\nglm.pred Down  Up\n    Down  145 141\n    Up    457 507\n```\n:::\n\n```{.r .cell-code}\n(507+145)/1250\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.5216\n```\n:::\n\n```{.r .cell-code}\nmean(glm.pred==Direction)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.5216\n```\n:::\n:::\n\n\n## Regresión logística: continuación\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(caret)\nconfusionMatrix(table(glm.probs >= 0.5, Smarket$Direction == \"Up\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n       \n        FALSE TRUE\n  FALSE   145  141\n  TRUE    457  507\n                                          \n               Accuracy : 0.5216          \n                 95% CI : (0.4935, 0.5496)\n    No Information Rate : 0.5184          \n    P-Value [Acc > NIR] : 0.4216          \n                                          \n                  Kappa : 0.0237          \n                                          \n Mcnemar's Test P-Value : <2e-16          \n                                          \n            Sensitivity : 0.2409          \n            Specificity : 0.7824          \n         Pos Pred Value : 0.5070          \n         Neg Pred Value : 0.5259          \n             Prevalence : 0.4816          \n         Detection Rate : 0.1160          \n   Detection Prevalence : 0.2288          \n      Balanced Accuracy : 0.5116          \n                                          \n       'Positive' Class : FALSE           \n                                          \n```\n:::\n:::\n\n\n## Regresión logística: continuación\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain=(Year <2005)\nSmarket.2005=Smarket[!train,]\ndim(Smarket.2005)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 252   9\n```\n:::\n\n```{.r .cell-code}\nDirection.2005=Direction[!train]\nglm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,data = Smarket, family = binomial, subset = train)\nglm.probs=predict(glm.fit,Smarket.2005,type=\"response\")\nglm.pred=rep(\"Down\",252)\nglm.pred[glm.probs >.5]=\"Up\"\ntable(glm.pred, Direction.2005)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        Direction.2005\nglm.pred Down Up\n    Down   77 97\n    Up     34 44\n```\n:::\n\n```{.r .cell-code}\nmean(glm.pred==Direction.2005)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.4801587\n```\n:::\n\n```{.r .cell-code}\nmean(glm.pred!=Direction.2005)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.5198413\n```\n:::\n:::\n\n\n## Regresión logística: continuación\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfusionMatrix(table(glm.probs >= 0.5, Smarket.2005$Direction == \"Up\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n       \n        FALSE TRUE\n  FALSE    77   97\n  TRUE     34   44\n                                         \n               Accuracy : 0.4802         \n                 95% CI : (0.417, 0.5437)\n    No Information Rate : 0.5595         \n    P-Value [Acc > NIR] : 0.9952         \n                                         \n                  Kappa : 0.0054         \n                                         \n Mcnemar's Test P-Value : 6.062e-08      \n                                         \n            Sensitivity : 0.6937         \n            Specificity : 0.3121         \n         Pos Pred Value : 0.4425         \n         Neg Pred Value : 0.5641         \n             Prevalence : 0.4405         \n         Detection Rate : 0.3056         \n   Detection Prevalence : 0.6905         \n      Balanced Accuracy : 0.5029         \n                                         \n       'Positive' Class : FALSE          \n                                         \n```\n:::\n:::\n\n\n## Regresión logística: continuación\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm.fit=glm(Direction~Lag1+Lag2,data=Smarket,family=binomial,subset=train)\nglm.probs=predict(glm.fit,Smarket.2005,type=\"response\")\nglm.pred=rep(\"Down\",252)\nglm.pred[glm.probs >.5]=\"Up\"\ntable(glm.pred,Direction.2005)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        Direction.2005\nglm.pred Down  Up\n    Down   35  35\n    Up     76 106\n```\n:::\n\n```{.r .cell-code}\nmean(glm.pred==Direction.2005)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.5595238\n```\n:::\n\n```{.r .cell-code}\n106/(106+76)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.5824176\n```\n:::\n:::\n\n## Regresión logística: continuación\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfusionMatrix(table(glm.probs >= 0.5, Smarket.2005$Direction == \"Up\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n       \n        FALSE TRUE\n  FALSE    35   35\n  TRUE     76  106\n                                          \n               Accuracy : 0.5595          \n                 95% CI : (0.4959, 0.6218)\n    No Information Rate : 0.5595          \n    P-Value [Acc > NIR] : 0.5262856       \n                                          \n                  Kappa : 0.0698          \n                                          \n Mcnemar's Test P-Value : 0.0001467       \n                                          \n            Sensitivity : 0.3153          \n            Specificity : 0.7518          \n         Pos Pred Value : 0.5000          \n         Neg Pred Value : 0.5824          \n             Prevalence : 0.4405          \n         Detection Rate : 0.1389          \n   Detection Prevalence : 0.2778          \n      Balanced Accuracy : 0.5335          \n                                          \n       'Positive' Class : FALSE           \n                                          \n```\n:::\n\n```{.r .cell-code}\npredict(glm.fit,newdata=data.frame(Lag1=c(1.2,1.5),Lag2=c(1.1,-0.8)), type=\"response\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        1         2 \n0.4791462 0.4960939 \n```\n:::\n:::\n\n\n## Análisis discriminante lineal\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MASS)\nlda.fit=lda(Direction~Lag1+Lag2,data=Smarket,subset=train)\nlda.fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCall:\nlda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)\n\nPrior probabilities of groups:\n    Down       Up \n0.491984 0.508016 \n\nGroup means:\n            Lag1        Lag2\nDown  0.04279022  0.03389409\nUp   -0.03954635 -0.03132544\n\nCoefficients of linear discriminants:\n            LD1\nLag1 -0.6420190\nLag2 -0.5135293\n```\n:::\n:::\n\n\n## Análisis discriminante lineal: continuación\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(lda.fit)\n```\n\n::: {.cell-output-display}\n![](lec_week8_files/figure-revealjs/unnamed-chunk-26-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Análisis discriminante lineal: continuación\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlda.pred=predict(lda.fit,Smarket.2005)\nnames(lda.pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"class\"     \"posterior\" \"x\"        \n```\n:::\n\n```{.r .cell-code}\nlda.class=lda.pred$class\ntable(lda.class,Direction.2005)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         Direction.2005\nlda.class Down  Up\n     Down   35  35\n     Up     76 106\n```\n:::\n\n```{.r .cell-code}\nmean(lda.class==Direction.2005)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.5595238\n```\n:::\n:::\n\n## Análisis discriminante lineal: continuación\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfusionMatrix(table(lda.class,Direction.2005))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n         Direction.2005\nlda.class Down  Up\n     Down   35  35\n     Up     76 106\n                                          \n               Accuracy : 0.5595          \n                 95% CI : (0.4959, 0.6218)\n    No Information Rate : 0.5595          \n    P-Value [Acc > NIR] : 0.5262856       \n                                          \n                  Kappa : 0.0698          \n                                          \n Mcnemar's Test P-Value : 0.0001467       \n                                          \n            Sensitivity : 0.3153          \n            Specificity : 0.7518          \n         Pos Pred Value : 0.5000          \n         Neg Pred Value : 0.5824          \n             Prevalence : 0.4405          \n         Detection Rate : 0.1389          \n   Detection Prevalence : 0.2778          \n      Balanced Accuracy : 0.5335          \n                                          \n       'Positive' Class : Down            \n                                          \n```\n:::\n:::\n\n\n\n## Análisis discriminante lineal: continuación\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Aplicando un umbral del 50%\nsum(lda.pred$posterior[,1] >=.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 70\n```\n:::\n\n```{.r .cell-code}\nsum(lda.pred$posterior[,1] <.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 182\n```\n:::\n\n```{.r .cell-code}\n# Primeros 20 datos\nlda.pred$posterior[1:20,1]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      999      1000      1001      1002      1003      1004      1005      1006 \n0.4901792 0.4792185 0.4668185 0.4740011 0.4927877 0.4938562 0.4951016 0.4872861 \n     1007      1008      1009      1010      1011      1012      1013      1014 \n0.4907013 0.4844026 0.4906963 0.5119988 0.4895152 0.4706761 0.4744593 0.4799583 \n     1015      1016      1017      1018 \n0.4935775 0.5030894 0.4978806 0.4886331 \n```\n:::\n\n```{.r .cell-code}\nlda.class[1:20]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] Up   Up   Up   Up   Up   Up   Up   Up   Up   Up   Up   Down Up   Up   Up  \n[16] Up   Up   Down Up   Up  \nLevels: Down Up\n```\n:::\n\n```{.r .cell-code}\n# Aplicando un umbral del 90%\nsum(lda.pred$posterior[,1] >.9)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n:::\n\n\n## Análisis discriminante cuadrático\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqda.fit=qda(Direction~Lag1+Lag2,data=Smarket,subset=train)\nqda.fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCall:\nqda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)\n\nPrior probabilities of groups:\n    Down       Up \n0.491984 0.508016 \n\nGroup means:\n            Lag1        Lag2\nDown  0.04279022  0.03389409\nUp   -0.03954635 -0.03132544\n```\n:::\n\n```{.r .cell-code}\nqda.class=predict(qda.fit,Smarket.2005)$class\ntable(qda.class,Direction.2005)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         Direction.2005\nqda.class Down  Up\n     Down   30  20\n     Up     81 121\n```\n:::\n\n```{.r .cell-code}\nmean(qda.class==Direction.2005)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.5992063\n```\n:::\n:::\n\n\n## Análisis discriminante cuadrático: continuación\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfusionMatrix(table(qda.class,Direction.2005))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n         Direction.2005\nqda.class Down  Up\n     Down   30  20\n     Up     81 121\n                                          \n               Accuracy : 0.5992          \n                 95% CI : (0.5358, 0.6602)\n    No Information Rate : 0.5595          \n    P-Value [Acc > NIR] : 0.1138          \n                                          \n                  Kappa : 0.1364          \n                                          \n Mcnemar's Test P-Value : 2.369e-09       \n                                          \n            Sensitivity : 0.2703          \n            Specificity : 0.8582          \n         Pos Pred Value : 0.6000          \n         Neg Pred Value : 0.5990          \n             Prevalence : 0.4405          \n         Detection Rate : 0.1190          \n   Detection Prevalence : 0.1984          \n      Balanced Accuracy : 0.5642          \n                                          \n       'Positive' Class : Down            \n                                          \n```\n:::\n:::\n\n\n## K-vecinos cercanos\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(class)\ntrain.X=cbind(Lag1,Lag2)[train,]\ntest.X=cbind(Lag1,Lag2)[!train,]\ntrain.Direction=Direction[train]\nhead(train.X)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       Lag1   Lag2\n[1,]  0.381 -0.192\n[2,]  0.959  0.381\n[3,]  1.032  0.959\n[4,] -0.623  1.032\n[5,]  0.614 -0.623\n[6,]  0.213  0.614\n```\n:::\n\n```{.r .cell-code}\nknn.pred=knn(train.X,test.X,train.Direction,k=1)\ntable(knn.pred,Direction.2005)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        Direction.2005\nknn.pred Down Up\n    Down   43 58\n    Up     68 83\n```\n:::\n\n```{.r .cell-code}\n(83+43)/252\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.5\n```\n:::\n:::\n\n\n## K-vecinos cercanos: continuación\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfusionMatrix(table(knn.pred,Direction.2005))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n        Direction.2005\nknn.pred Down Up\n    Down   43 58\n    Up     68 83\n                                          \n               Accuracy : 0.5             \n                 95% CI : (0.4366, 0.5634)\n    No Information Rate : 0.5595          \n    P-Value [Acc > NIR] : 0.9751          \n                                          \n                  Kappa : -0.0242         \n                                          \n Mcnemar's Test P-Value : 0.4227          \n                                          \n            Sensitivity : 0.3874          \n            Specificity : 0.5887          \n         Pos Pred Value : 0.4257          \n         Neg Pred Value : 0.5497          \n             Prevalence : 0.4405          \n         Detection Rate : 0.1706          \n   Detection Prevalence : 0.4008          \n      Balanced Accuracy : 0.4880          \n                                          \n       'Positive' Class : Down            \n                                          \n```\n:::\n:::\n\n\n## K-vecinos cercanos: continuación\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknn.pred=knn(train.X,test.X,train.Direction,k=3)\ntable(knn.pred,Direction.2005)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        Direction.2005\nknn.pred Down Up\n    Down   48 55\n    Up     63 86\n```\n:::\n\n```{.r .cell-code}\nmean(knn.pred==Direction.2005)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.531746\n```\n:::\n:::\n\n\n## K-vecinos cercanos: continuación\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfusionMatrix(table(knn.pred,Direction.2005))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n        Direction.2005\nknn.pred Down Up\n    Down   48 55\n    Up     63 86\n                                          \n               Accuracy : 0.5317          \n                 95% CI : (0.4681, 0.5946)\n    No Information Rate : 0.5595          \n    P-Value [Acc > NIR] : 0.8294          \n                                          \n                  Kappa : 0.0427          \n                                          \n Mcnemar's Test P-Value : 0.5193          \n                                          \n            Sensitivity : 0.4324          \n            Specificity : 0.6099          \n         Pos Pred Value : 0.4660          \n         Neg Pred Value : 0.5772          \n             Prevalence : 0.4405          \n         Detection Rate : 0.1905          \n   Detection Prevalence : 0.4087          \n      Balanced Accuracy : 0.5212          \n                                          \n       'Positive' Class : Down            \n                                          \n```\n:::\n:::\n\n\n## Ejercicio\n\nRealizar **KNN** con el conjunto `Caravan` del paquete `ISLR`. El conjunto de datos corresponde a 85 predictores que miden características demográficas para 5822 personas, en donde la variable respuesta es `Purchase` que indica si la persona adquirió un póliza de seguros. Realice el ejercicio con y sin estandarización de variables.\n\n# Árbol de decisión\n\nLos árboles de decisión son una metodología que estratifica o segmenta el espacio de los predictores en distintas regiones, en donde se utiliza una serie de reglas de división para segmentar los espacios.\n\nEn general, esta metodología no es lo suficientemente competitiva en contraste con otras técnicas supervisadas (en términos de su precisión).\n\nLos árboles de decisión pueden ser aplicados a problemas de regresión y clasificación, en lo que sigue sólo nos concentramos en esta metodología para problemas de clasificación.\n\n## Definición\n\nEn un árbol de decisión para problemas de clasificación se predice que cada observación pertenece a la clase más frecuente entre las observaciones de entrenamiento en la región a la que pertenece.\n\nUtilizamos la **tasa de error de clasificación** para separar los espacios a lo largo del árbol de decisión. Debido a que se planea asignar una observación en una región particular a la *clase más frecuente* en el conjunto de entrenamiento, este error se define como:\n\n\n$$\nE=1-\\max_{k}(\\hat{p}_{mk})\n$$\n\n\nen donde $\\hat{p}_{mk}$ representa la proporción de observaciones de entrenamiento en la región $m-$ésima que son de la clase $k-$ésima.\n\n## Diagrama de ejemplo\n\n![](images/week8/tree1.png){fig-align=\"center\"}\n\n## Definición: continuación\n\nEn general, usar sólo la tasa de error de clasifición no es lo suficientemente sensible para esta metodología, y se opta por dos medidas alternativas: índice de Gini y entropía cruzada.\n\nEl **índice de Gini** está definido como:\n\n\n$$\nG=\\sum_{k=1}^{K} \\hat{p}_{mk}(1-\\hat{p}_{mk})\n$$\n\n\nque es una medida de la varianza total a lo largo de las $K$ clases. Es claro ver que este índice toma valores pequeños si todos los $\\hat{p}_{mk}$ son cercanos a cero.\n\n## Definición: continuación\n\nUna alternativa al índice anterior es la **entropía cruzada**, dada por:\n\n\n$$\nD=-\\sum_{k=1}^{K} \\hat{p}_{mk}\\log \\hat{p}_{mk}.\n$$\n\n\nDebido a que $0 \\leq \\hat{p}_{mk}\\leq 1$, sigue que $0\\leq -\\hat{p}_{mk}\\log\\hat{p}_{mk}$. Se puede mostrar que la entropía cruzada tomará valores cercanos a cero si todos los $\\hat{p}_{mk}$ están cercano a cero o a uno. Por lo que ambos índices tomaran valores pequeños si la $m-$ésimo *nodo* es *puro*.\n\n## Ejemplo \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tree)\nattach(Carseats)\nrequire(ISLR)\nhead(Carseats)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Sales CompPrice Income Advertising Population Price ShelveLoc Age Education\n1  9.50       138     73          11        276   120       Bad  42        17\n2 11.22       111     48          16        260    83      Good  65        10\n3 10.06       113     35          10        269    80    Medium  59        12\n4  7.40       117    100           4        466    97    Medium  55        14\n5  4.15       141     64           3        340   128       Bad  38        13\n6 10.81       124    113          13        501    72       Bad  78        16\n  Urban  US\n1   Yes Yes\n2   Yes Yes\n3   Yes Yes\n4   Yes Yes\n5   Yes  No\n6    No Yes\n```\n:::\n:::\n\n\n## Ejemplo: continuación\n\n\n::: {.cell}\n\n```{.r .cell-code}\nHigh=as.factor(ifelse(Sales<=8,\"No\",\"Yes\"))\nCarseats=data.frame(Carseats,High)\nCarseats_tree=tree(High~ . -Sales, data=Carseats)\nsummary(Carseats_tree)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nClassification tree:\ntree(formula = High ~ . - Sales, data = Carseats)\nVariables actually used in tree construction:\n[1] \"ShelveLoc\"   \"Price\"       \"Income\"      \"CompPrice\"   \"Population\" \n[6] \"Advertising\" \"Age\"         \"US\"         \nNumber of terminal nodes:  27 \nResidual mean deviance:  0.4575 = 170.7 / 373 \nMisclassification error rate: 0.09 = 36 / 400 \n```\n:::\n:::\n\n## Ejemplo: continuación\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(Carseats_tree)\ntext(Carseats_tree, pretty=0)\n```\n\n::: {.cell-output-display}\n![](lec_week8_files/figure-revealjs/unnamed-chunk-50-1.png){fig-align='center' width=960}\n:::\n:::\n\n## Ejemplo: continuación\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Alternativa\nlibrary(rpart)\nlibrary(rpart.plot)\nCarseats_tree2<-rpart(formula=High~ . -Sales, data=Carseats)\nsummary(Carseats_tree2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCall:\nrpart(formula = High ~ . - Sales, data = Carseats)\n  n= 400 \n\n          CP nsplit rel error    xerror       xstd\n1 0.28658537      0 1.0000000 1.0000000 0.05997967\n2 0.10975610      1 0.7134146 0.7134146 0.05547692\n3 0.04573171      2 0.6036585 0.6463415 0.05382112\n4 0.03658537      4 0.5121951 0.6402439 0.05365767\n5 0.02743902      5 0.4756098 0.6158537 0.05298128\n6 0.02439024      7 0.4207317 0.6158537 0.05298128\n7 0.01219512      8 0.3963415 0.6158537 0.05298128\n8 0.01000000     10 0.3719512 0.6219512 0.05315381\n\nVariable importance\n      Price   ShelveLoc         Age Advertising   CompPrice      Income \n         34          25          11          11           9           5 \n Population   Education \n          3           1 \n\nNode number 1: 400 observations,    complexity param=0.2865854\n  predicted class=No   expected loss=0.41  P(node) =1\n    class counts:   236   164\n   probabilities: 0.590 0.410 \n  left son=2 (315 obs) right son=3 (85 obs)\n  Primary splits:\n      ShelveLoc   splits as  LRL,       improve=28.991900, (0 missing)\n      Price       < 92.5  to the right, improve=19.463880, (0 missing)\n      Advertising < 6.5   to the left,  improve=17.277980, (0 missing)\n      Age         < 61.5  to the right, improve= 9.264442, (0 missing)\n      Income      < 60.5  to the left,  improve= 7.249032, (0 missing)\n\nNode number 2: 315 observations,    complexity param=0.1097561\n  predicted class=No   expected loss=0.3111111  P(node) =0.7875\n    class counts:   217    98\n   probabilities: 0.689 0.311 \n  left son=4 (269 obs) right son=5 (46 obs)\n  Primary splits:\n      Price       < 92.5  to the right, improve=15.930580, (0 missing)\n      Advertising < 7.5   to the left,  improve=11.432570, (0 missing)\n      ShelveLoc   splits as  L-R,       improve= 7.543912, (0 missing)\n      Age         < 50.5  to the right, improve= 6.369905, (0 missing)\n      Income      < 60.5  to the left,  improve= 5.984509, (0 missing)\n  Surrogate splits:\n      CompPrice < 95.5  to the right, agree=0.873, adj=0.13, (0 split)\n\nNode number 3: 85 observations,    complexity param=0.03658537\n  predicted class=Yes  expected loss=0.2235294  P(node) =0.2125\n    class counts:    19    66\n   probabilities: 0.224 0.776 \n  left son=6 (12 obs) right son=7 (73 obs)\n  Primary splits:\n      Price       < 142.5 to the right, improve=7.745608, (0 missing)\n      US          splits as  LR,        improve=5.112440, (0 missing)\n      Income      < 35    to the left,  improve=4.529433, (0 missing)\n      Advertising < 6     to the left,  improve=3.739996, (0 missing)\n      Education   < 15.5  to the left,  improve=2.565856, (0 missing)\n  Surrogate splits:\n      CompPrice < 154.5 to the right, agree=0.882, adj=0.167, (0 split)\n\nNode number 4: 269 observations,    complexity param=0.04573171\n  predicted class=No   expected loss=0.2453532  P(node) =0.6725\n    class counts:   203    66\n   probabilities: 0.755 0.245 \n  left son=8 (224 obs) right son=9 (45 obs)\n  Primary splits:\n      Advertising < 13.5  to the left,  improve=10.400090, (0 missing)\n      Age         < 49.5  to the right, improve= 8.083998, (0 missing)\n      ShelveLoc   splits as  L-R,       improve= 7.023150, (0 missing)\n      CompPrice   < 124.5 to the left,  improve= 6.749986, (0 missing)\n      Price       < 126.5 to the right, improve= 5.646063, (0 missing)\n\nNode number 5: 46 observations,    complexity param=0.02439024\n  predicted class=Yes  expected loss=0.3043478  P(node) =0.115\n    class counts:    14    32\n   probabilities: 0.304 0.696 \n  left son=10 (10 obs) right son=11 (36 obs)\n  Primary splits:\n      Income      < 57    to the left,  improve=4.000483, (0 missing)\n      ShelveLoc   splits as  L-R,       improve=3.189762, (0 missing)\n      Advertising < 9.5   to the left,  improve=1.388592, (0 missing)\n      Price       < 80.5  to the right, improve=1.388592, (0 missing)\n      Age         < 64.5  to the right, improve=1.172885, (0 missing)\n\nNode number 6: 12 observations\n  predicted class=No   expected loss=0.25  P(node) =0.03\n    class counts:     9     3\n   probabilities: 0.750 0.250 \n\nNode number 7: 73 observations\n  predicted class=Yes  expected loss=0.1369863  P(node) =0.1825\n    class counts:    10    63\n   probabilities: 0.137 0.863 \n\nNode number 8: 224 observations,    complexity param=0.02743902\n  predicted class=No   expected loss=0.1830357  P(node) =0.56\n    class counts:   183    41\n   probabilities: 0.817 0.183 \n  left son=16 (96 obs) right son=17 (128 obs)\n  Primary splits:\n      CompPrice   < 124.5 to the left,  improve=4.881696, (0 missing)\n      Age         < 49.5  to the right, improve=3.960418, (0 missing)\n      ShelveLoc   splits as  L-R,       improve=3.654633, (0 missing)\n      Price       < 126.5 to the right, improve=3.234428, (0 missing)\n      Advertising < 6.5   to the left,  improve=2.371276, (0 missing)\n  Surrogate splits:\n      Price      < 115.5 to the left,  agree=0.741, adj=0.396, (0 split)\n      Age        < 50.5  to the right, agree=0.634, adj=0.146, (0 split)\n      Population < 405   to the right, agree=0.629, adj=0.135, (0 split)\n      Education  < 11.5  to the left,  agree=0.585, adj=0.031, (0 split)\n      Income     < 22.5  to the left,  agree=0.580, adj=0.021, (0 split)\n\nNode number 9: 45 observations,    complexity param=0.04573171\n  predicted class=Yes  expected loss=0.4444444  P(node) =0.1125\n    class counts:    20    25\n   probabilities: 0.444 0.556 \n  left son=18 (20 obs) right son=19 (25 obs)\n  Primary splits:\n      Age       < 54.5  to the right, improve=6.722222, (0 missing)\n      CompPrice < 121.5 to the left,  improve=4.629630, (0 missing)\n      ShelveLoc splits as  L-R,       improve=3.250794, (0 missing)\n      Income    < 99.5  to the left,  improve=3.050794, (0 missing)\n      Price     < 127   to the right, improve=2.933429, (0 missing)\n  Surrogate splits:\n      Population  < 363.5 to the left,  agree=0.667, adj=0.25, (0 split)\n      Income      < 39    to the left,  agree=0.644, adj=0.20, (0 split)\n      Advertising < 17.5  to the left,  agree=0.644, adj=0.20, (0 split)\n      CompPrice   < 106.5 to the left,  agree=0.622, adj=0.15, (0 split)\n      Price       < 135.5 to the right, agree=0.622, adj=0.15, (0 split)\n\nNode number 10: 10 observations\n  predicted class=No   expected loss=0.3  P(node) =0.025\n    class counts:     7     3\n   probabilities: 0.700 0.300 \n\nNode number 11: 36 observations\n  predicted class=Yes  expected loss=0.1944444  P(node) =0.09\n    class counts:     7    29\n   probabilities: 0.194 0.806 \n\nNode number 16: 96 observations\n  predicted class=No   expected loss=0.0625  P(node) =0.24\n    class counts:    90     6\n   probabilities: 0.938 0.062 \n\nNode number 17: 128 observations,    complexity param=0.02743902\n  predicted class=No   expected loss=0.2734375  P(node) =0.32\n    class counts:    93    35\n   probabilities: 0.727 0.273 \n  left son=34 (107 obs) right son=35 (21 obs)\n  Primary splits:\n      Price     < 109.5 to the right, improve=9.764582, (0 missing)\n      ShelveLoc splits as  L-R,       improve=6.320022, (0 missing)\n      Age       < 49.5  to the right, improve=2.575061, (0 missing)\n      Income    < 108.5 to the right, improve=1.799546, (0 missing)\n      CompPrice < 143.5 to the left,  improve=1.741982, (0 missing)\n\nNode number 18: 20 observations\n  predicted class=No   expected loss=0.25  P(node) =0.05\n    class counts:    15     5\n   probabilities: 0.750 0.250 \n\nNode number 19: 25 observations\n  predicted class=Yes  expected loss=0.2  P(node) =0.0625\n    class counts:     5    20\n   probabilities: 0.200 0.800 \n\nNode number 34: 107 observations,    complexity param=0.01219512\n  predicted class=No   expected loss=0.1869159  P(node) =0.2675\n    class counts:    87    20\n   probabilities: 0.813 0.187 \n  left son=68 (65 obs) right son=69 (42 obs)\n  Primary splits:\n      Price     < 126.5 to the right, improve=2.9643900, (0 missing)\n      CompPrice < 147.5 to the left,  improve=2.2337090, (0 missing)\n      ShelveLoc splits as  L-R,       improve=2.2125310, (0 missing)\n      Age       < 49.5  to the right, improve=2.1458210, (0 missing)\n      Income    < 60.5  to the left,  improve=0.8025853, (0 missing)\n  Surrogate splits:\n      CompPrice   < 129.5 to the right, agree=0.664, adj=0.143, (0 split)\n      Advertising < 3.5   to the right, agree=0.664, adj=0.143, (0 split)\n      Population  < 53.5  to the right, agree=0.645, adj=0.095, (0 split)\n      Age         < 77.5  to the left,  agree=0.636, adj=0.071, (0 split)\n      US          splits as  RL,        agree=0.626, adj=0.048, (0 split)\n\nNode number 35: 21 observations\n  predicted class=Yes  expected loss=0.2857143  P(node) =0.0525\n    class counts:     6    15\n   probabilities: 0.286 0.714 \n\nNode number 68: 65 observations\n  predicted class=No   expected loss=0.09230769  P(node) =0.1625\n    class counts:    59     6\n   probabilities: 0.908 0.092 \n\nNode number 69: 42 observations,    complexity param=0.01219512\n  predicted class=No   expected loss=0.3333333  P(node) =0.105\n    class counts:    28    14\n   probabilities: 0.667 0.333 \n  left son=138 (22 obs) right son=139 (20 obs)\n  Primary splits:\n      Age         < 49.5  to the right, improve=5.4303030, (0 missing)\n      CompPrice   < 137.5 to the left,  improve=2.1000000, (0 missing)\n      Advertising < 5.5   to the left,  improve=1.8666670, (0 missing)\n      ShelveLoc   splits as  L-R,       improve=1.4291670, (0 missing)\n      Population  < 382   to the right, improve=0.8578431, (0 missing)\n  Surrogate splits:\n      Income      < 46.5  to the left,  agree=0.595, adj=0.15, (0 split)\n      Education   < 12.5  to the left,  agree=0.595, adj=0.15, (0 split)\n      CompPrice   < 131.5 to the right, agree=0.571, adj=0.10, (0 split)\n      Advertising < 5.5   to the left,  agree=0.571, adj=0.10, (0 split)\n      Population  < 221.5 to the left,  agree=0.571, adj=0.10, (0 split)\n\nNode number 138: 22 observations\n  predicted class=No   expected loss=0.09090909  P(node) =0.055\n    class counts:    20     2\n   probabilities: 0.909 0.091 \n\nNode number 139: 20 observations\n  predicted class=Yes  expected loss=0.4  P(node) =0.05\n    class counts:     8    12\n   probabilities: 0.400 0.600 \n```\n:::\n:::\n\n\n## Ejemplo: continuación\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_plot<-rpart.plot(Carseats_tree2)\n```\n\n::: {.cell-output-display}\n![](lec_week8_files/figure-revealjs/unnamed-chunk-54-1.png){width=960}\n:::\n:::\n\n## Ejemplo: continuación\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2)\ntrain=sample(1:nrow(Carseats), 200)\nCarseats_test=Carseats[-train,]\nHigh_test=High[-train]\nCarseats_tree=tree(High~ .-Sales ,Carseats ,subset = train)\ntree_pred=predict(Carseats_tree ,Carseats_test , type =\"class\")\ntable(tree_pred,High_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         High_test\ntree_pred  No Yes\n      No  104  33\n      Yes  13  50\n```\n:::\n\n```{.r .cell-code}\n(104+50)/200\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.77\n```\n:::\n:::\n\n## Ejemplo: continuación\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfusionMatrix(table(tree_pred,High_test))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n         High_test\ntree_pred  No Yes\n      No  104  33\n      Yes  13  50\n                                          \n               Accuracy : 0.77            \n                 95% CI : (0.7054, 0.8264)\n    No Information Rate : 0.585           \n    P-Value [Acc > NIR] : 2.938e-08       \n                                          \n                  Kappa : 0.5091          \n                                          \n Mcnemar's Test P-Value : 0.005088        \n                                          \n            Sensitivity : 0.8889          \n            Specificity : 0.6024          \n         Pos Pred Value : 0.7591          \n         Neg Pred Value : 0.7937          \n             Prevalence : 0.5850          \n         Detection Rate : 0.5200          \n   Detection Prevalence : 0.6850          \n      Balanced Accuracy : 0.7456          \n                                          \n       'Positive' Class : No              \n                                          \n```\n:::\n:::\n\n\n# ¿Qué veremos la próxima semana?\n\n- Métodos supervisados: continuación\n  - SVM\n- Métodos no supervisados: introducción\n\n# ¿Que deben preparar para la próxima semana?\n\n- Capítulo 8 y 9 , Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. Géron, Aurélien.\n- Capítulo 9, An Introduction to Statistical Learning with Applications in R.\n\n\n\n",
    "supporting": [
      "lec_week8_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n\r\n      // dispatch for htmlwidgets\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for reveal\r\n    if (window.Reveal) {\r\n      window.Reveal.addEventListener(\"slidechanged\", function(event) {\r\n        fireSlideChanged(event.previousSlide, event.currentSlide);\r\n      });\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}