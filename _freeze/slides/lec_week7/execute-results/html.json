{
  "hash": "8751d15f7b6ae8fe974b7c0edfe14291",
  "result": {
    "markdown": "---\ntitle: \"Métodos supervisados\"\nsubtitle: \"IND 163 - 2022/02\"\nauthor: \"Eloy Alvarado Narváez\"\ninstitute: \"Universidad Técnica Federico Santa María\"\ndate: 14/10/22\nformat: \n  revealjs:\n    theme: slides.scss\n    touch: true\n    slide-level: 2\n    code-copy: true\nincremental: true\nslide-number: true\nlang: es\nhighlight-style: github\nwidth: 1600\nheight: 900\nlogo: images/logo_usm.png\ntransition: fade\nfooter: \"IND 163 - Semana 7\"\nexecute:\n  freeze: auto\neditor: \n  markdown: \n    wrap: 72\n---\n\n\n# Métodos supervisados\n\n## Introducción\n\nComo hemos mencionado a lo largo del curso, una regresión lineal simple asume que la variable respuesta $Y$ es **cuantitativa**, pero en muchas situaciones esta es **cualitativa** (también referida como categórica). En lo que sigue, veremos métodos para predecir respuestas cualitativas, más comúnmente llamado **clasificación**.\n\nExisten mucha técnicas de clasificación o **clasificadores**, que se pueden usar para predecir una variable cualitativa. Entre ellos se encuentras:\n\n-   Regresión logística\n\n-   Análisis discriminante lineal\n\n-   *k-NN (k- nearest neighbors / k-vecinos cercanos)*\n\n-   Modelos generalizados aditivos\n\n-   Árboles y bosques aleatorios\n\n-   Boosting\n\n-   SVM\n\n## Ejemplo\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ISLR)\ndata<-Default\nhead(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  default student   balance    income\n1      No      No  729.5265 44361.625\n2      No     Yes  817.1804 12106.135\n3      No      No 1073.5492 31767.139\n4      No      No  529.2506 35704.494\n5      No      No  785.6559 38463.496\n6      No     Yes  919.5885  7491.559\n```\n:::\n:::\n\n\n## Ejemplo: continuación\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nggplot(data) + aes(x = balance, y = income, colour = default) + geom_point(shape = \"bullet\", size = 1.5) +\n scale_color_hue(direction = -1) + theme_gray()\n```\n\n::: {.cell-output-display}\n![](lec_week7_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Ejemplo: continuación\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(data) +  aes(x = default, y = balance, fill = default) +  geom_boxplot(shape = \"circle\") +\n  scale_fill_hue(direction = -1) +  theme_gray()\n```\n\n::: {.cell-output-display}\n![](lec_week7_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=960}\n:::\n:::\n\n## Ejemplo: continuación\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(data) +  aes(x = default, y = income, fill = default) +  geom_boxplot(shape = \"circle\") +\n  scale_fill_hue(direction = -1) +  theme_gray()\n```\n\n::: {.cell-output-display}\n![](lec_week7_files/figure-revealjs/unnamed-chunk-8-1.png){fig-align='center' width=960}\n:::\n:::\n\n## ¿Por qué no usar una regresión lineal?\n\nSupongamos que se intenta predecir la condición médica de un paciente en la sala de emergencia con base a sus síntomas. Para simplificar, imaginemos que sólo que tienen 3 posibles diagnósticos: accidente cardiovascular, sobredosis y ataque epiléptico. Por lo que podríamos clasificar la variable respuesta como\n\n\n$$\nY=\\begin{cases} 1 \\quad \\text{si Accidente cardiovascular}\\\\\n2 \\quad \\text{si Sobredosis} \\\\\n3 \\quad \\text{si Ataque epiléptico}\n\\end{cases}\n$$\n\n\nUsando esta codificación, se puede usar el método de mínimos cuadrados para ajustar una regresión lineal para predecir $Y$ en base a los predictores $X_1,\\dots, X_p$.\n\n## ¿Por qué no usar una regresión lineal?: continuación\n\nDesafortunadamente, esta codificación implica un ordenamiento de las salidas, estableciendo sobredosis entre accidente cardiovascular y Ataque epiléptico, e inherentemente afirmando que la **diferencia entre categorías contiguas son la misma**.\n\nEs claro notar que si usamos otra codificación, el ajuste de regresión lineal obtenido será diferente al primero. En general, **no hay una forma natural de convertir una variable respuesta cualitativa con más de dos niveles en una variable cuantitativa que esté lista para hacer una regresión lineal**.\n\n## ¿Por qué no usar una regresión lineal?: continuación\n\nEn el caso de variable respuesta binaria, la situación es algo más favorable, debido a que si se cambia la codificación, el ajuste de regresión obtenido será el mismo. Sin embargo, el método de mínimos cuadrados no tiene sentido, provocando que algunas de nuestras estimación estén fuera del intervalo [0,1], haciendo difícil la interpretación de las probabilidades.\n\nLo anterior debido a que se puede mostrar que el $X\\hat{\\beta}$ obtenido con la regresión lineal con codificación binaria, es simplemente una estimación de $\\mathbb{P}(\\text{Sobredosis})$ si la codificación es\n\n\n$$\nY = \\begin{cases} 0 \\quad \\text{si Accidente cardiovascular}\\\\\n1 \\quad \\text{si Sobredosis} \n\\end{cases}\n$$\n\n\n# Regresión logística\n\nUsando el mismo conjunto de datos `Default`, donde la variable respuesta `default` cae dentro de dos categorías `Yes` y `No`. En vez de modelar la respuesta $Y$ directamente, la **regresión logística** modela la probabilidad que $Y$ pertenezca a una categoría particular.\n\nPara el conjunto de datos `Default`, la regresión logística modela la probabilidad de que haya *default* (morosidad). Por ejemplo, la probabilidad de *default* dado cierto `balance` puede ser escrito como\n\n\n$$\n\\mathbb{P}( \\text{default}=\\text{Yes}|\\text{balance})\n$$\n\n\nLos valores de esta probabilidad, que la abreviamos como $p(\\text{balance})$, estarán entre 0 y 1. Por lo que para un valor particular de `balance`, se puede hacer una predicción para `default`. Por ejemplo, se podría predecir que `default=Yes` para cualquier individuo cuyo $p(\\text{balance})>0.5$. Alternativamente, si una compañía quisiese ser más conservador en la predicción, podría definir $p(\\text{balance})>0.1$.\n\n## Modelo logístico\n\n¿Cómo deberíamos modelar la relación entre $p(X)=\\mathbb{P}(Y=1|X)$ y $X$?\n\nPodemos utilizar un enfoque de regresión lineal para representar estar probabilidades, esto es:\n\n\n$$\np(X)=\\beta_0 + \\beta_1 X\n$$\n\n\nSi usamos este enfoque para predecir `default=Yes` usando `balance`, entonces obtendremos el siguiente modelo.\n\n## Modelo logístico: continuación\n\n![](images/week7/lin_reg.png){fig-align=\"center\"}\n\n## Modelo logístico: continuación\n\nPara evitar lo anterior, debemos modelar $p(X)$ usando una función que entregue salidas entre 0 y 1 para todos los valores de $X$. Muchas funciones cumplen estas condiciones. En una **regresión logística**, usamos la *función logística*.\n\n\n$$\np(X)=\\dfrac{\\exp(\\beta_0 + \\beta_1 X)}{1+\\exp(\\beta_0 + \\beta_1 X)}\n$$\n\nPara ajustar el modelo anterior, usamos el método de **máxima verosimilitud**.\n\n## Modelo logístico: continuación\n\n![](images/week7/log_reg.png){fig-align=\"center\"}\n\n## Modelo logístico: continuación\n\nManipulando un poco la fórmula anterior, se tiene que\n\n\n$$\n\\dfrac{p(X)}{1-p(X)}=\\exp(\\beta_0 + \\beta_1 X)\n$$\n\n\nLa cantidad ${p(X) \\over 1-p(X)}$ se le llaman **odds**, que pueden toman cualquier valor en $\\mathbb{R}^{+}$. Valores cercanos a cero y tendiendo a infinito, indican muy baja y alta probabilidad de `default`, respectivamente.\n\n\n## Modelo logístico: continuación\n\nTomando el logaritmo en ambos lados, se tiene\n\n\n$$\n\\log \\left(\\dfrac{p(X)}{1-p(X)}\\right)=\\beta_0 + \\beta_1 X\n$$\n\n\na esta cantidad la llamamos **log-odds** o **logit**. Notamos que el modelo de regresión logística tiene un logit lineal en $X$.\n\n## Estimación de los coeficientes de regresión\n\nLos coeficiente $\\beta_0$ y $\\beta_1$ en la ecuación\n\n\n$$\np(X)=\\dfrac{\\exp(\\beta_0 + \\beta_1 X)}{1+\\exp(\\beta_0 + \\beta_1 X)}\n$$\n\n\nson desconocidos, por lo que deben ser estimados basándose en los datos de entrenamiento. Si bien podríamos ocupar una metodología de métodos cuadrados no lineales para ajustar el modelo:\n\n\n$$\n\\log \\left(\\dfrac{p(X)}{1-p(X)}\\right)=\\beta_0 + \\beta_1 X\n$$\n\n\nLa metodología de máxima verosimilitud es usualmente preferida, debido a que tiene mejores propiedades estadísticas.\n\n## Estimación de los coeficientes de regresión: continuación\n\nFormalmente, definimos la **función de verosimilitud** como:\n\n\n$$\n\\ell(\\beta_0,\\beta_1)=\\prod_{i:y_i=1}p(x_i)\\prod_{i':y_{i'}=0}(1-p(x_{i'}))\n$$\n\n\nLas estimaciones $\\hat{\\beta}_0$ y $\\hat{\\beta}_1$ son escogidos para maximizar la función de verosimilitud.\n\n## Ejemplo\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogit <- glm(default ~ balance, data = data, family = \"binomial\")\nsummary(logit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = default ~ balance, family = \"binomial\", data = data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.2697  -0.1465  -0.0589  -0.0221   3.7589  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -1.065e+01  3.612e-01  -29.49   <2e-16 ***\nbalance      5.499e-03  2.204e-04   24.95   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2920.6  on 9999  degrees of freedom\nResidual deviance: 1596.5  on 9998  degrees of freedom\nAIC: 1600.5\n\nNumber of Fisher Scoring iterations: 8\n```\n:::\n:::\n\n\n## Predicciones\n\nUna vez que los coeficientes han sido estimados, lo que resta es calcular la probabilidad de `default` para una `balance` dado. Por ejemplo, la predicción para una persona con balance $\\$1000$ es\n\n\n$$\n\\hat{p}(X)=\\dfrac{\\exp(-10.65+ 0.0055 \\times 1000)}{1+\\exp(-10.65+ 0.0055 \\times 1000)}\\approx 0.00576\n$$\n\n\nque es bajo $1\\%$. En contraste con alguien que adeuda $\\$2000$, en cuyo caso $\\hat{p}(X)=0.586$.\n\n## Predicciones: continuación\n\nSi utilizamos *dummy variables* para el predictor `student` codificado como 0 y 1. tendremos el siguiente ajuste\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = default ~ student, family = \"binomial\", data = data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-0.2970  -0.2970  -0.2434  -0.2434   2.6585  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -3.50413    0.07071  -49.55  < 2e-16 ***\nstudentYes   0.40489    0.11502    3.52 0.000431 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2920.6  on 9999  degrees of freedom\nResidual deviance: 2908.7  on 9998  degrees of freedom\nAIC: 2912.7\n\nNumber of Fisher Scoring iterations: 6\n```\n:::\n:::\n\n\n## Predicciones: continuación\n\nAsí, podemos calcular las probabilidades\n\n\n$$\n\\mathbb{P}\\left( \\text{default=Yes }| \\text{ student=Yes}\\right)=\\dfrac{\\exp(-3.5041+ 0.4049 \\times 1)}{1+\\exp(-3.5041+ 0.4049 \\times 1)}\\approx 0.0431\n$$\n\n\ny,\n\n\n$$\n\\mathbb{P}\\left( \\text{default=Yes }| \\text{ student=No}\\right)=\\dfrac{\\exp(-3.5041+ 0.4049 \\times 0)}{1+\\exp(-3.5041+ 0.4049 \\times 0)}\\approx 0.0292\n$$\n\n## Regresión logística múltiple\n\nAhora consideramos el problema de predecir una respuesta binaria usando múltiples predictores. La extensión natural del modelo de regresión es\n\n\n$$\n\\log \\left(\\dfrac{p(X)}{1-p(X)}\\right)=\\beta_0 + \\beta_1 X_1 +\\dots + \\beta_p X_p\n$$\n\n\ndonde $X=(X_1,\\dots,X_p)$ son $p$ predictores. La ecuación anterior la podemos reescribir como\n\n\n$$\np(X)=\\dfrac{\\exp(\\beta_0 + \\beta_1 X_1 +\\dots + \\beta_p X_p)}{1+ \\exp(\\beta_0 + \\beta_1 X_1 +\\dots + \\beta_p X_p)}\n$$\n\n\nAl igual que antes, usamos el método de máxima verosimilitud para estimar $\\mathbf{\\beta}$.\n\n## Ejemplo\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = default ~ balance + student + income, family = \"binomial\", \n    data = data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.4691  -0.1418  -0.0557  -0.0203   3.7383  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -1.087e+01  4.923e-01 -22.080  < 2e-16 ***\nbalance      5.737e-03  2.319e-04  24.738  < 2e-16 ***\nstudentYes  -6.468e-01  2.363e-01  -2.738  0.00619 ** \nincome       3.033e-06  8.203e-06   0.370  0.71152    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2920.6  on 9999  degrees of freedom\nResidual deviance: 1571.5  on 9996  degrees of freedom\nAIC: 1579.5\n\nNumber of Fisher Scoring iterations: 8\n```\n:::\n:::\n\n\n## Ejemplo: continuación\n\n![](images/week7/log_compare.png){fig-align=\"center\"}\n\n## Regresión logística para $>2$ clases en la respuesta\n\nEn el caso en que tengamos más de dos clases en la variable respuesta, es posible extender la regresión lineal. En el ejemplo de determinación de diagnóstico en una sala de emergencia se tenían las categorías accidente cardiovascular, sobredosis y ataque epiléptico, por lo que se desearía modelar\n\n\n$$\n\\mathbb{P}\\left( Y= \\text{ acc. card. }| X\\right)\n$$\n\n\ny \n\n\n$$\n\\mathbb{P}\\left( Y= \\text{ sobredosis }| X\\right)\n$$\n\n\nsiendo el remanente,\n\n\n$$\n\\mathbb{P}\\left( Y= \\text{ ataque epiléptico }| X\\right)= 1-\\mathbb{P}\\left( Y= \\text{ acc. card }| X\\right)-\\mathbb{P}\\left( Y= \\text{ sobredosis }| X\\right) \n$$\n\n\nSi bien es posible la extensión, en la práctica no es frecuentemente usado, pues se prefiere realizar un **análisis discriminante**.\n\n# Análisis discriminante lineal\n\n## Introducción\n\nLa regresión logística que vimos antes involucra modelar directamente $\\mathbb{P}\\left( Y=k|X=x\\right)$ usando la función logística dada por\n\n\n$$\np(X)=\\dfrac{\\exp(\\beta_0 + \\beta_1 X_1 +\\dots + \\beta_p X_p)}{1+ \\exp(\\beta_0 + \\beta_1 X_1 +\\dots + \\beta_p X_p)}\n$$\n\n\npara el caso de dos clases en la variable respuesta. En lo que sigue, consideramos una manera alternativa y menos directa para estimar estas probabilidades. En esta metodología, modelamos la distribución de los predictores $X$ por separado en cada una de las categorías de la variable respuesta $(Y)$, y luego usamos el teorema de Bayes para convertir estos resultados en estimaciones de $\\mathbb{P}\\left(Y=k|X=x\\right)$.\n\nCuando estas distribuciones se asumen normales, la forma de este modelo es muy similar a una regresión logística.\n\n## Teorema de Bayes para clasificación\n\nSupongamos que queremos clasificar una observación entre $K$ clases, donde $K\\geq 2$. Esto es, que la variable respuesta $Y$ puede tomar $K$ posibles valores distintos y no-ordenados.\n\nSea $\\pi_k$ la probabilidad *apriori* que una observación escogida aleatoriamente provenga de la clase $k-$ésima. Sea $f_k(X)=\\mathbb{P}(X=x|Y=k)$ la **función de densidad** de $X$ para una observación que proviene de la clase $k-$ésima. Luego, por el teorema de Bayes se tiene\n\n\n$$\n\\mathbb{P}(Y=k|X=x)=\\dfrac{\\pi_k f_k(x)}{\\sum_{l=1}^{K} \\pi_l f_l(x)}\n$$\n\n\nal igual que antes usamos la notación $p_k(X)=\\mathbb{P}(Y=k|X)$.\n\n## Teorema de Bayes para clasificación: continuación\n\nLa idea general, es no estimar $p_k(X)$ directamente, sino estimar $\\pi_k$ y $f_k$ para obtener lo deseado.\n\nUsualmente $\\pi_k$ es fácil de obtener si se tiene una muestra aleatoria de $Y$, pues obtenemos estas estimaciones como las proporciones de cada clase.\n\nEn cambio, estimar $f_k(X)$ tiende a ser más difícil, a menos que se asuman formas simples para las densidades.\n\nLlamamos a la cantidad $p_k(x)$ la probabilidad *posterior* que una observación $X=x$ pertenezca a la clase $k-$ésima.\n\n\n## Análisis discriminante lineal con $p=1$\n\nPrimero asumiremos que $p=1$, es decir, sólo tenemos un predictor. Deseamos obtener una estimación para $f_k(x)$ para utilizarlo en la ecuación\n\n\n$$\n\\mathbb{P}(Y=k|X=x)=\\dfrac{\\pi_k f_k(x)}{\\sum_{l=1}^{K} \\pi_l f_l(x)}\n$$\n\n\ny así poder estimar $p_k(x)$. Para poder estimar $f_k$, primero debemos asumir su forma, por lo que asumiremos que $f_k$ es *Gaussiana*. Por lo que,\n\n\n$$\nf_k(x)=\\dfrac{1}{\\sqrt{2\\pi}\\sigma_k}\\exp\\left( -\\dfrac{1}{2\\sigma_{k}^{2}}(x-\\mu_k)^2\\right)\n$$\n\n\ndonde $\\mu_k$ y $\\sigma_{k}^{2}$ son la media y la varianza de la clase $k-$ésima. Por ahora, asumiremos que $\\sigma_{1}^{2}=\\dots=\\sigma_{K}^{2}=\\sigma^2$\n\n\n## Análisis discriminante lineal con $p=1$: continuación\n\nPor lo anterior, se tendrá\n\n\n$$\np_k(x)=\\dfrac{\\pi_k \\dfrac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left( -\\dfrac{1}{2\\sigma^{2}}(x-\\mu_k)^2\\right)}{\\sum_{l=1}^{K}\\pi_l\\dfrac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left( -\\dfrac{1}{2\\sigma^{2}}(x-\\mu_l)^2\\right) }\n$$\n\n\nEl clasificador Bayesiano asigna una observacion $X=x$ a la clase que su $p_k(x)$ es más grande. Si tomamos el logaritmo y arreglamos términos en la expresión anterior, se tiene que el proceso es equivalente a asignar la observación a la clase en la que\n\n\n$$\n\\delta_k(x)=x \\dfrac{\\mu_k}{\\sigma^2}-\\dfrac{\\mu_{k}^{2}}{2\\sigma^2}+\\log \\pi_k\n$$\n\n\nes más grande.\n\n\n## Análisis discriminante lineal con $p=1$: continuación\n\nPor ejemplo, si $K=2$ Y $\\pi_1=\\pi_2$, entonces el clasificador Bayesiano asigna una observación a la clase 1 si $2x(\\mu_1-\\mu_2)>\\mu_{1}^{2}-\\mu_{2}^{2}$ y a la clase 2 en caso contrario. En este caso, el límite de decisión de Bayes (*Bayes decision boundary*) corresponde al punto donde\n\n\n$$\nx=\\dfrac{\\mu_{1}^{2}-\\mu_{2}^{2}}{2(\\mu_1-\\mu_2)}=\\dfrac{\\mu_1+\\mu_2}{2}\n$$\n\n\nLlamamos a este, el punto (o área) en donde la clasificación es ambigua.\n\n## Análisis discriminante lineal con $p=1$: continuación\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(data.frame(x = c(-4, 4)), aes(x)) + stat_function(fun = dnorm, args = list(mean = -1.25, sd = 1), color = \"firebrick\") + stat_function(fun = dnorm, args = list(mean = 1.25, sd = 1), color = \"green3\") +geom_vline(xintercept = 0, linetype = \"longdash\") + theme_bw()\n```\n\n::: {.cell-output-display}\n![](lec_week7_files/figure-revealjs/unnamed-chunk-16-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Análisis discriminante lineal con $p=1$: continuación\n\nEl análisis discriminante lineal (LDA) aproxima el clasificador bayesiano ingresando estimaciones para $pi_k,\\mu_k$ y $\\sigma^2$ en $\\delta_k(x)$. Particularmente, las siguientes estimaciones son usadas.\n\n\n$$\n\\hat{\\mu}_k=\\dfrac{1}{n_k}\\sum_{i:y_i=k}x_i\n$$\n\n\ny,\n\n\n$$\n\\hat{\\sigma}^{2}=\\dfrac{1}{n-K}\\sum_{k=1}^{K}\\sum_{i:y_i=K}(x_i-\\hat{\\mu}_k)^2\n$$\n\n\ndonde $n$ es el número total de observaciones en el conjunto de entrenamiento, $n_k$ es el número de observaciones en el conjunto de entrenamiento en la clase $k-$ésima.\n\n\n## Análisis discriminante lineal con $p=1$: continuación\n\nEn el caso de que no tengamos información de $\\pi_1,\\dots,\\pi_K$, el análisis discriminante lineal estima $\\pi_k$ usando la proporción de las observaciones en el conjunto de entrenamiento que pertenece a la clase $k-$ésima. Esto es,\n\n\n$$\n\\hat{\\pi}_k=\\dfrac{n_k}{n}\n$$\n\n\nEl clasificador **LDA** reemplaza las estimaciones anteriores en $\\delta_k(x)$ y asigna una observación $X=x$ a la clase en la cual \n\n\n$$\n\\hat{\\delta}_k=x\\dfrac{\\hat{\\mu}_k}{\\hat{\\sigma}^2}-\\dfrac{\\hat{\\mu}_{k}^{2}}{\\hat{2\\sigma}^2}+\\log \\hat{\\pi}_k\n$$\n\n\nes más grande. El nombre de **lineal** viene de la linealidad de la *función discriminante* $\\hat{\\delta}_k$ para $x$.\n\n\n## Análisis discriminante lineal con $p=1$: continuación\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](lec_week7_files/figure-revealjs/unnamed-chunk-18-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Análisis discriminante lineal con $p>1$\n\nEn lo que sigue, vamos a extender las nociones de análisis discriminante cuando se tienen múltiples predictores, para ello asumiremos que $X=(X_1,X_2,\\dots,X_p)$ es obtenido desde una distribución normal multivariada, con medias por clase e igual matriz de varianza-covarianza.\n\nRecordar que si $X\\sim N(\\mu,\\Sigma)$ con $\\mathbb{E}(X)=\\mu$ (vector de medias) y $Cov(X)=\\Sigma$ la matriz $p\\times p$ de covarianza de $X$. Formalmente, la densidad de $X$ se define como:\n\n\n$$\nf(x)=\\dfrac{1}{(2\\pi)^{p/2}|\\Sigma|^{1/2}}\\exp\\left( -\\dfrac{1}{2}(x-\\mu)^{T} \\Sigma^{-1}(x-\\mu)\\right)\n$$\n\n\nEn el caso de $p>1$ predictores, el análisis discriminante lineal asume que las observaciones en la clase $k-$ésima son obtenidos desde una distribución normal multivariada.\n\n## Análisis discriminante lineal con $p>1$: continuación\n\nSi reemplazamos la función de densidad para la clase $k-$ésima, $f_k(X=x)$ en la ecuación\n\n\n$$\n\\mathbb{P}(Y=k|X=x)=\\dfrac{\\pi_k f_k(x)}{\\sum_{l=1}^{K} \\pi_l f_l(x)}\n$$\n\n\ny usando un poco de álgebra, se puede reescribir $\\delta_k(x)$ como\n\n\n$$\n\\delta_k(x)=x^T\\Sigma^{-1} \\mu_k-\\dfrac{1}{2}\\mu_{k}^{T} \\Sigma^{-1} \\mu_k +\\log \\pi_k\n$$\n\n\ny el clasificador bayesiano asigna la observación $X=x$ a la clase que tienen mayor $\\delta_{k}(x)$.\n\n## Análisis discriminante lineal con $p>1$: continuación\n\n![](images/week7/lda_1.png){fig-align=\"center\"}\n\n## Análisis discriminante lineal con $p>1$: continuación\n\nEn la figura anterior, las elipses representan las regiones que contienen $95\\%$ de la probabilidad de cada una de las clases. Al igual que antes, la línea punteada es el **Límite de decisión Bayes**. Es decir, representan el conjunto de valores $x$ para los cuales $\\delta_k(x)=\\delta_\\ell(x)$, esto es:\n\n\n$$\nx^T\\Sigma^{-1} \\mu_k-\\dfrac{1}{2}\\mu_{k}^{T} \\Sigma^{-1} \\mu_k=x^T\\Sigma^{-1} \\mu_l-\\dfrac{1}{2}\\mu_{l}^{T} \\Sigma^{-1} \\mu_l\n$$\n\n\npara $k\\neq l$.\n\n## Ejemplo\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MASS)\nmod_lda <- lda(Species ~ Sepal.Width + Sepal.Length + Petal.Length + Petal.Width, data = iris)\nmod_lda\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCall:\nlda(Species ~ Sepal.Width + Sepal.Length + Petal.Length + Petal.Width, \n    data = iris)\n\nPrior probabilities of groups:\n    setosa versicolor  virginica \n 0.3333333  0.3333333  0.3333333 \n\nGroup means:\n           Sepal.Width Sepal.Length Petal.Length Petal.Width\nsetosa           3.428        5.006        1.462       0.246\nversicolor       2.770        5.936        4.260       1.326\nvirginica        2.974        6.588        5.552       2.026\n\nCoefficients of linear discriminants:\n                    LD1         LD2\nSepal.Width   1.5344731  2.16452123\nSepal.Length  0.8293776  0.02410215\nPetal.Length -2.2012117 -0.93192121\nPetal.Width  -2.8104603  2.83918785\n\nProportion of trace:\n   LD1    LD2 \n0.9912 0.0088 \n```\n:::\n:::\n\n\n## Ejemplo: continuación\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredicciones <- predict(object = mod_lda, newdata = iris[, -5])\ntable(iris$Species, predicciones$class, dnn = c(\"Clase real\", \"Clase predicha\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Clase predicha\nClase real   setosa versicolor virginica\n  setosa         50          0         0\n  versicolor      0         48         2\n  virginica       0          1        49\n```\n:::\n:::\n\n\n## Ejemplo: continuación\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(klaR)\npartimat(Species ~ Sepal.Width + Sepal.Length + Petal.Length + Petal.Width, data = iris, method = \"lda\", prec = 200,  image.colors = c(\"darkgoldenrod1\", \"snow2\", \"skyblue2\"), col.mean = \"firebrick\")\n```\n\n::: {.cell-output-display}\n![](lec_week7_files/figure-revealjs/unnamed-chunk-24-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Métricas para clasificación\n\nEn problemas de clasificación, existen un gran número de métricas para evaluar el desempeño de un modelo. Por ejemplo, para el ejemplo de morosidad:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(caret)\nconfusionMatrix(table(predict(logit2, type=\"response\") >= 0.5, data$default == \"Yes\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n       \n        FALSE TRUE\n  FALSE  9627  228\n  TRUE     40  105\n                                          \n               Accuracy : 0.9732          \n                 95% CI : (0.9698, 0.9763)\n    No Information Rate : 0.9667          \n    P-Value [Acc > NIR] : 0.0001044       \n                                          \n                  Kappa : 0.4278          \n                                          \n Mcnemar's Test P-Value : < 2.2e-16       \n                                          \n            Sensitivity : 0.9959          \n            Specificity : 0.3153          \n         Pos Pred Value : 0.9769          \n         Neg Pred Value : 0.7241          \n             Prevalence : 0.9667          \n         Detection Rate : 0.9627          \n   Detection Prevalence : 0.9855          \n      Balanced Accuracy : 0.6556          \n                                          \n       'Positive' Class : FALSE           \n                                          \n```\n:::\n:::\n\n\n## Métricas para clasificación: continuación\n\nLa función `confusionMatrix()` nos entrega la **matriz de confusión** junto con varias métricas asociadas. Esta matriz en su forma más esencial es:\n\n![](images/week7/confusion.png){fig-align=\"center\"}\n\n\n## Análisis discriminante cuadrático\n\nEl **análisis discriminante cuadrático** es una alternativa a *LDA*, en la que se asumía distribución normal e igual varianza en cada una de las clases. Si bien, en el análisis discriminante cuadrático (**QDA**) también se asume que los datos provienen desde una distribución normal y estima los parámetros para predecir. Sin embargo, el **QDA** asume que cada clase tienen su propia matriz de covarianza.\n\nEsto es, se asume que una observación proveniente de la clase $k-$ésima es de la forma $X\\sim N(\\mu_k,\\Sigma_k)$, donde $\\Sigma_k$ es la matriz de covarianza para la clase $k-$ésima.\n\n\n## Análisis discriminante cuadrático: continuación\n\nBajo estos supuestos, el clasificador bayesiano asigna una observación $X=x$ a la clase en la que\n\n\n```{=tex}\n\\begin{align*}\n\\delta_k(x)&=-\\dfrac{1}{2}(x-\\mu_k)^{T}\\Sigma_{k}^{-1}(x-\\mu_k)+\\log \\pi_k \\\\\n&=-\\dfrac{1}{2}x^{T} \\Sigma_{k}^{-1}x+x^{T}\\Sigma_{k}^{-1}\\mu_k-\\dfrac{1}{2}\\mu_{k}^{T}\\mu_k+\\log \\pi_k\n\\end{align*}\n```\n\n\nes mayor. Así, se requerirá estimar $\\Sigma_k,\\mu_k$ y $\\pi_k$. El nombre de cuadrático viene debido a que $x$ aparece como una función cuadrática en la ecuación anterior.\n\n## ¿LDA o QDA?\n\nSi tenemos $p$ predictores, estimar la matriz de covarianza requiere estimar $p(p+1)/2$ parámetros. En el caso de **QDA** se estima una matriz de covarianza para cada clase, por lo que se deben estimar $Kp(p+1)/2$ parámetros. Si asumimos que las $K$ clases comparten la misma matriz de covarianza, el modelo de **LDA** es lineal en $x$, lo que significa que se debe estimar $Kp$ parámetros.\n\nEn general, el discriminante lineal es menos flexible que su contraparte cuadrática, y tiene una varianza sustancialmente menor. Sin embargo, si el supuesto de igualdad de matrices de covarianza entre las clases es erróneo, provocará que el discriminante lineal tenga un enorme sesgo.\n\n## ¿LDA o QDA?: continuación\n\nUsualmente, **LDA** tiende a ser mejor que **QDA** si se tienen pocas observaciones en el conjunto de entrenamiento, por lo que reducir la varianza es particularmente importante.\n\nEn contraste, **QDA** es recomendado si el conjunto de entrenamiento es grande, de manera que la varianza del clasificador no sea tan relevante, o si el supuesto de igual matriz de covarianza en las distintas clases es claramente insostenible.\n\n## ¿LDA o QDA?: continuación\n\n![](images/week7/lda_qda.png){fig-align=\"center\"}\n\n# ¿Qué veremos la próxima semana?\n\n- Métodos supervisados: continuación\n\n\n# ¿Que deben preparar para la próxima semana?\n\n- Capítulo 3 , Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. Géron, Aurélien.\n- Capítulo 4, An Introduction to Statistical Learning with Applications in R.\n\n\n\n",
    "supporting": [
      "lec_week7_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n\r\n      // dispatch for htmlwidgets\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for reveal\r\n    if (window.Reveal) {\r\n      window.Reveal.addEventListener(\"slidechanged\", function(event) {\r\n        fireSlideChanged(event.previousSlide, event.currentSlide);\r\n      });\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}