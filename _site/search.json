[
  {
    "objectID": "slides/lec_week2.html#escuelas-de-probabilidad",
    "href": "slides/lec_week2.html#escuelas-de-probabilidad",
    "title": "Conceptos Básicos",
    "section": "Escuelas de probabilidad",
    "text": "Escuelas de probabilidad\n\nEnfoque clásico\nEnfoque frecuentista\nEnfoque bayesiano"
  },
  {
    "objectID": "slides/lec_week2.html#enfoque-clásico",
    "href": "slides/lec_week2.html#enfoque-clásico",
    "title": "Conceptos Básicos",
    "section": "Enfoque clásico",
    "text": "Enfoque clásico\nEste enfoque también llamado enfoque apriori tiene por característica principal la asignación igualitaria de una medida de ocurrencia para un resultado de un experimento aleatorio (experimento equiprobable).\n\nEsta asignación de probabilidad se determina antes de observar los resultados experimentales.\n\n\n\n¿Algún ejemplo?"
  },
  {
    "objectID": "slides/lec_week2.html#enfoque-frecuentista",
    "href": "slides/lec_week2.html#enfoque-frecuentista",
    "title": "Conceptos Básicos",
    "section": "Enfoque frecuentista",
    "text": "Enfoque frecuentista\nEste enfoque también llamado enfoque empírico, determina la medida de ocurrencia con base en la proporción de veces que ocurre un resultado favorable en un determinado número de observaciones o experimentos. Este enfoque no asigna probabilidades a priori a los posibles resultados de un experimento aleatorio.\n\n\n¿Algún ejemplo?"
  },
  {
    "objectID": "slides/lec_week2.html#enfoque-bayesiano",
    "href": "slides/lec_week2.html#enfoque-bayesiano",
    "title": "Conceptos Básicos",
    "section": "Enfoque bayesiano",
    "text": "Enfoque bayesiano\nEste enfoque también llamado enfoque subjetivo, determina la medida de ocurrencia en base a una expectativa razonable basado en el conocimiento del investigador.\nEl enfoque bayesiano es particularmente útil cuando se tiene poca información del experimento, y este puede ser realizado para actualizador mis probabilidades, esto debido a que cada realización del experimento aleatorio me otorgará información adicional para determinar correctamente mis probabilidades."
  },
  {
    "objectID": "slides/lec_week2.html#conceptos-fundamentales",
    "href": "slides/lec_week2.html#conceptos-fundamentales",
    "title": "Conceptos Básicos",
    "section": "Conceptos fundamentales",
    "text": "Conceptos fundamentales\n\nEspacio muestral: Se define como el conjunto de todos los posibles resultados del experimento, lo anotamos por \\(\\Omega\\).\nSuceso o evento: Es cualquier subconjunto de \\(\\Omega\\), usualmente lo anotamos con letras mayúsculas. \\((A,B,C,\\dots)\\).\nEspacio de sucesos: Es el conjunto de todos los subconjuntos de \\(\\Omega\\). Lo anotamos por \\(2^{\\Omega}\\).\n\\(\\sigma\\)-álgebra: Es una familia de subconjuntos del espacio de sucesos, \\(\\Sigma \\subset 2^{\\Omega}\\), y que cumplen con ciertas propiedades."
  },
  {
    "objectID": "slides/lec_week2.html#clasificación-del-espacio-muestral",
    "href": "slides/lec_week2.html#clasificación-del-espacio-muestral",
    "title": "Conceptos Básicos",
    "section": "Clasificación del espacio muestral",
    "text": "Clasificación del espacio muestral\n\nDiscreto\n\nNumerable: Finito o Infinito.\n\nContinuo\n\nNo numerable: Acotado o No acotado."
  },
  {
    "objectID": "slides/lec_week2.html#definición-formal-de-probabilidad",
    "href": "slides/lec_week2.html#definición-formal-de-probabilidad",
    "title": "Conceptos Básicos",
    "section": "Definición formal de probabilidad",
    "text": "Definición formal de probabilidad\nEl par \\((\\Omega,\\Sigma)\\) se dice espacio medible, y la función \\(\\mathbb{P}:\\Sigma \\rightarrow \\mathbb{R}^{+}\\), es una medida de probabilidad si satisface:\n\n\\(0\\leq \\mathbb{P}[A] \\leq 1, \\forall A \\in \\Sigma\\)\n\\(\\mathbb{P}[\\Omega]=1\\)\nDados \\(\\displaystyle A_1,A_2,\\dots \\in \\Sigma \\Rightarrow \\mathbb{P}\\left[ \\bigcup_{i=1}^{n} A_n \\right] = \\sum_{i=1}^{n} \\mathbb{P}[A_i], \\hspace{5pt} \\forall i\\)"
  },
  {
    "objectID": "slides/lec_week2.html#algunas-propiedades",
    "href": "slides/lec_week2.html#algunas-propiedades",
    "title": "Conceptos Básicos",
    "section": "Algunas propiedades",
    "text": "Algunas propiedades\n\n\\(\\mathbb{P}[A]+\\mathbb{P}[A^c]=\\mathbb{P}[\\Omega]\\)\n\\(\\mathbb{P}[\\phi]=1-\\mathbb{P}[\\phi^c]=1-\\mathbb{P}[\\Omega]=0\\)\n\\(\\mathbb{P}[A \\cup B]=\\mathbb{P}[A]+\\mathbb{P}[B] - \\mathbb{P}[A\\cap B]\\) . Si este último término \\((\\mathbb{P}[A\\cap B])\\) es cero, se dice que \\(A\\) y \\(B\\) son eventos mutuamente excluyentes.\n\\(\\mathbb{P}[A-B]=\\mathbb{P}[A\\cap B^c]\\)\n\\(\\mathbb{P}[A \\cap B]=\\mathbb{P}[A]\\mathbb{P}[B]\\). Si \\(A\\) y \\(B\\) son independientes."
  },
  {
    "objectID": "slides/lec_week2.html#definición-básica",
    "href": "slides/lec_week2.html#definición-básica",
    "title": "Conceptos Básicos",
    "section": "Definición Básica",
    "text": "Definición Básica\nUna Variable aleatoria, es una función que permite trabajar cualquier espacio muestral de manera cuantitativa. Se dice que \\(X\\) es una variable aleatoria si es una función que toma los elementos de \\(\\Omega\\) y los transforma en puntos sobre la recta de los reales. Esto es:\n\\[\\begin{align*}\n  X: \\quad &\\Omega \\longrightarrow \\mathbb{R}\\\\\n           &\\omega \\longrightarrow X(\\omega)\n\\end{align*}\\]\n\nEl conjunto de todas las posibles realizaciones es llamado el soporte y lo denotamos por \\(R_X\\)."
  },
  {
    "objectID": "slides/lec_week2.html#tipos-de-variables-aleatorias",
    "href": "slides/lec_week2.html#tipos-de-variables-aleatorias",
    "title": "Conceptos Básicos",
    "section": "Tipos de variables aleatorias",
    "text": "Tipos de variables aleatorias\nSe dice que \\(X\\) es una Variable Aleatoria si es una función que toma valores en probabilidad, es decir, no se puede predecir con certeza sus resultados.\nUna variable aleatoria es siempre cuantitativa y se puede clasificar en los siguientes grupos:\n\\[X(\\omega) \\begin{cases}\n\\text{Discreto}\n\\begin{cases}\n\\text{Finito}\\\\\n\\text{Infinito}\n\\end{cases}\\\\\n\\text{Continuo}\n\\begin{cases}\n\\text{Acotados}\\\\\n\\text{No Acotados}\n\\end{cases}\n\\end{cases}\\]"
  },
  {
    "objectID": "slides/lec_week2.html#variables-aleatorias-discretas",
    "href": "slides/lec_week2.html#variables-aleatorias-discretas",
    "title": "Conceptos Básicos",
    "section": "Variables aleatorias discretas",
    "text": "Variables aleatorias discretas\nUna variable aleatoria \\(X\\) es llamada discreta si:\n\nSu soporte \\(R_X\\) es un conjunto numerable.\nExiste una función \\(p_X:\\mathbb{R}\\rightarrow [0,1]\\), llamada la función de masa de probabilidad de \\(X\\), tal que, para cualquier \\(x\\in \\mathbb{R}\\):\n\n\n\\[p_X(x)\\begin{cases} \\mathbb{P}(X=x) \\quad &\\text{si } x\\in R_X\\\\ 0 \\quad &\\text{si } x\\notin R_X\\end{cases}\\]\n\n\nEsta función tiene dos características principales:\n\nno-negatividad: \\(p_X(x)\\geq 0\\) para cualquier \\(x\\in \\mathbb{R}\\).\nSuma sobre su soporte es 1: \\(\\sum_{x\\in R_X}p_X(x)=1\\)"
  },
  {
    "objectID": "slides/lec_week2.html#variables-aleatorias-continuas",
    "href": "slides/lec_week2.html#variables-aleatorias-continuas",
    "title": "Conceptos Básicos",
    "section": "Variables aleatorias continuas",
    "text": "Variables aleatorias continuas\nUna variable aleatoria \\(X\\) es llamada continua si:\n\nSu soporte \\(R_X\\) es un conjunto no-numerable.\nExiste una función \\(f_X:\\mathbb{R}\\rightarrow [0,1]\\), llamada función de densidad de probabilidad de \\(X\\), tal que, para cualquier intervalo \\([a,b]\\subseteq \\mathbb{R}\\):\n\n\n\\[\\mathbb{P}(X\\in [a,b])=\\int_{a}^{b}f_X(x)dx\\]\n\n\nEsta función tiene dos características principales:\n\nno-negatividad: \\(f_X(x)\\geq 0\\) para cualquier \\(x\\in \\mathbb{R}\\).\nIntegral sobre \\(\\mathbb{R}\\) es 1: \\(\\int_{-\\infty}^{\\infty} f_X(x)dx=1\\)."
  },
  {
    "objectID": "slides/lec_week2.html#función-de-distribución",
    "href": "slides/lec_week2.html#función-de-distribución",
    "title": "Conceptos Básicos",
    "section": "Función de distribución",
    "text": "Función de distribución\nLas variables aleatorias son usualmente caracterizadas en términos de sus funciones de distribución.\n\nSea \\(X\\) una variable aleatoria. La función de distribución de \\(X\\) es una función \\(F_X:\\mathbb{R}\\rightarrow [0,1]\\) tal que:\n\\[F_X(x)=\\mathbb{P}(X\\leq x), \\forall x\\in \\mathbb{R}\\]\n\n\nSi conocemos la función de distribución de una variable aleatoria \\(X\\), entonces podemos fácilmente calcular la probabilidad que \\(X\\) pertenezca a un intervalo \\((a,b] \\subseteq \\mathbb{R}\\) como:\n\\[\\mathbb{P}(a<X<b)=F_X(b)-F_X(a)\\]"
  },
  {
    "objectID": "slides/lec_week2.html#valores-esperados",
    "href": "slides/lec_week2.html#valores-esperados",
    "title": "Conceptos Básicos",
    "section": "Valores esperados",
    "text": "Valores esperados\nSea \\(X\\) una variable aleatoria, entonces se define el valor esperado de una función real \\(g(X)\\), como:\n\\[\\mathbb{E}[g(X)]= \\begin{cases} \\sum_{x\\in \\mathbb{R}} g(X)P(X=x)\\\\ \\int_{x\\in \\mathbb{R}} g(X)f(x)dx \\end{cases}\\]\nSi \\(g(X)=X\\), diremos que el valor esperado o esperanza matemática de \\(X\\) es:\n\\[\\mathbb{E}(X)=\\begin{cases}\\sum_{x\\in \\mathbb{R}} x P(X=x)\\\\ \\int_{x\\in \\mathbb{R}} x f(x)dx \\end{cases}\\]\nPara variables de tipo discreta y continua, respectivamente."
  },
  {
    "objectID": "slides/lec_week2.html#propiedades-de-los-valores-esperados",
    "href": "slides/lec_week2.html#propiedades-de-los-valores-esperados",
    "title": "Conceptos Básicos",
    "section": "Propiedades de los valores esperados",
    "text": "Propiedades de los valores esperados\nSean \\(a\\) y \\(b\\) constantes, \\(X\\) una variable aleatoria entonces se cumple que:\n\n\\(\\mathbb{E}(a)=a\\)\n\\(\\mathbb{E}(X)=\\mu=\\) constante\n\\(\\mathbb{E}(aX)=a\\mathbb{E}(X)\\)\n\\(\\mathbb{E}(aX+b)=\\mathbb{E}(aX)+\\mathbb{E}(b)=a\\mathbb{E}(X)+b\\)"
  },
  {
    "objectID": "slides/lec_week2.html#varianza",
    "href": "slides/lec_week2.html#varianza",
    "title": "Conceptos Básicos",
    "section": "Varianza",
    "text": "Varianza\nSea \\(X\\) una variable aleatoria, se define el la varianza de \\(X\\) como:\n\\[\\mathbb{E}[(X-\\mathbb{E}(X))^2]=V(X)=\\begin{cases}\\sum_{x\\in\\mathbb{R}} (X-\\mathbb{E}(X))^2P(X=x)\\\\ \\int_{x\\in\\mathbb{R}}(X-\\mathbb{E}(X))^2f_{X}(x)dx\\end{cases}\\]\nPara variables de tipo discreta y continua, respectivamente."
  },
  {
    "objectID": "slides/lec_week2.html#propiedades-de-la-varianza",
    "href": "slides/lec_week2.html#propiedades-de-la-varianza",
    "title": "Conceptos Básicos",
    "section": "Propiedades de la varianza",
    "text": "Propiedades de la varianza\nSea \\(a\\) y \\(b\\) constantes, \\(X\\) una variable aleatoria, entonces se cumple:\n\n\\(\\mathbb{V}(a)=0\\)\n\\(\\mathbb{V}(X)=\\sigma^2=\\) constante\n\\(\\mathbb{V}(aX)=a^2 \\mathbb{V}(X)\\)\n\\(\\mathbb{V}(aX+b)=\\mathbb{V}(aX)+\\mathbb{V}(b)=a^2\\mathbb{V}(X)+0=a^2\\mathbb{V}(X)\\)\n\\(\\mathbb{V}(X)=\\mathbb{E}(X^2)-(\\mathbb{E}(X))^2\\)"
  },
  {
    "objectID": "slides/lec_week2.html#distribución-binomial",
    "href": "slides/lec_week2.html#distribución-binomial",
    "title": "Conceptos Básicos",
    "section": "Distribución binomial",
    "text": "Distribución binomial\nSea \\(X\\) una variable aleatoria que representa el número de éxitos en \\(n\\) ensayos y \\(p\\) la probabilidad de éxito con cualquiera de éstos. Se dice entonces que \\(X\\) tiene una distribución binomial con función de probabilidad:\n\\[\\mathbb{P}(X=k)= {{n}\\choose{k}}p^k(1-p)^{n-k} \\hspace{20pt} k=1,2,\\cdots,n\\]\nEn donde \\({{n}\\choose{k}}\\) es el coeficiente binomial, esto es:\n\\[{{n}\\choose{k}}=\\dfrac{n!}{k!(n-k)!}\\]\nSi \\(n=1\\) diremos que \\(X\\) sigue una distribución Bernoulli."
  },
  {
    "objectID": "slides/lec_week2.html#propiedades-de-la-distribución-binomial",
    "href": "slides/lec_week2.html#propiedades-de-la-distribución-binomial",
    "title": "Conceptos Básicos",
    "section": "Propiedades de la distribución binomial",
    "text": "Propiedades de la distribución binomial\nSi \\(X\\) tiene una distribución binomial, entonces se cumple que:\n\n\\(\\mathbb{E}[X]=np\\)\n\\(\\mathbb{V}[X]=np(1-p)\\)\n\n\nEs claro ver que si \\(X\\) tiene una distribución bernoulli, entonces:\n\n\\(\\mathbb{E}[X]=p\\)\n\\(\\mathbb{V}[X]=p(1-p)\\)"
  },
  {
    "objectID": "slides/lec_week2.html#distribución-binomial-en-r",
    "href": "slides/lec_week2.html#distribución-binomial-en-r",
    "title": "Conceptos Básicos",
    "section": "Distribución binomial en R",
    "text": "Distribución binomial en R\n\n\n\n\nCódigoSalidas\n\n\n\nset.seed(163)\ndbinom(x = 2, size = 10, prob = 0.3)\nmean(rbinom(n = 10000, size = 10, prob = 0.3) == 2)\npbinom(q = 5, size = 10, p = 0.3, lower.tail = TRUE)\nmean(rbinom(n = 10000, size = 10, prob = 0.3) <= 5)\npbinom(q = 4, size = 10, p = 0.3, lower.tail  = FALSE)\nmean(rbinom(n = 10000, size = 10, prob = 0.3) >= 5)\n\n\n\n\nset.seed(163)\ndbinom(x = 2, size = 10, prob = 0.3)\n\n[1] 0.2334744\n\nmean(rbinom(n = 10000, size = 10, prob = 0.3) == 2)\n\n[1] 0.2417\n\npbinom(q = 5, size = 10, p = 0.3, lower.tail = TRUE)\n\n[1] 0.952651\n\nmean(rbinom(n = 10000, size = 10, prob = 0.3) <= 5)\n\n[1] 0.9529\n\npbinom(q = 4, size = 10, p = 0.3, lower.tail  = FALSE)\n\n[1] 0.1502683\n\nmean(rbinom(n = 10000, size = 10, prob = 0.3) >= 5)\n\n[1] 0.1497"
  },
  {
    "objectID": "slides/lec_week2.html#distribución-binomial-en-python",
    "href": "slides/lec_week2.html#distribución-binomial-en-python",
    "title": "Conceptos Básicos",
    "section": "Distribución binomial en Python",
    "text": "Distribución binomial en Python\n\nCódigoSalidas\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import binom\n\nnp.random.seed(163)\nbinom.pmf(2, n=10, p=0.3)\nsum(np.random.binomial(10, 0.3, 10000) == 2)/10000\nbinom.cdf(5, n=10, p=0.3)\nsum(np.random.binomial(10, 0.3, 10000) <= 5)/10000\n1-binom.cdf(4, n=10, p=0.3)\nsum(np.random.binomial(10, 0.3, 10000) >= 5)/10000\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import binom\n\nnp.random.seed(163)\nbinom.pmf(2, n=10, p=0.3)\n\n0.2334744405000001\n\nsum(np.random.binomial(10, 0.3, 10000) == 2)/10000\n\n0.2332\n\nbinom.cdf(5, n=10, p=0.3)\n\n0.9526510126000001\n\nsum(np.random.binomial(10, 0.3, 10000) <= 5)/10000\n\n0.9517\n\n1-binom.cdf(4, n=10, p=0.3)\n\n0.15026833259999994\n\nsum(np.random.binomial(10, 0.3, 10000) >= 5)/10000\n\n0.1478"
  },
  {
    "objectID": "slides/lec_week2.html#distribución-de-poisson",
    "href": "slides/lec_week2.html#distribución-de-poisson",
    "title": "Conceptos Básicos",
    "section": "Distribución de Poisson",
    "text": "Distribución de Poisson\nSea \\(X\\) una variable aleatoria que representa el número de eventos aleatorios independientes que ocurren a una rapidez constante sobre el tiempo o el espacio. Se dice entonces que la variable aleatoria \\(X\\) tiene una distribución de Poisson con función de probabilidad:\n\\[\\mathbb{P}(X=k)=\\dfrac{e^{-\\lambda}\\lambda^k}{k!} \\hspace{20pt} k=0,1,\\cdots,n,\\cdots\\]\nEn donde \\(\\lambda>0\\) representa el número promedio de ocurrencias del evento aleatorio por unidad de tiempo. Además, si \\(X\\) sigue una distribución de Poisson se cumple que:\n\n\\(\\mathbb{E}[X]=\\lambda\\)\n\\(\\mathbb{V}[X]=\\lambda\\)"
  },
  {
    "objectID": "slides/lec_week2.html#distribución-de-poisson-en-r",
    "href": "slides/lec_week2.html#distribución-de-poisson-en-r",
    "title": "Conceptos Básicos",
    "section": "Distribución de Poisson en R",
    "text": "Distribución de Poisson en R\n\nCódigoSalidas\n\n\n\nset.seed(163)\ndpois(x = 7, 5)\nmean(rpois(n = 10000, 5) == 7)\nppois(q = 5, 5, lower.tail = TRUE)\nmean(rpois(n = 10000, 5) <= 5)\nppois(q = 4, 5, lower.tail  = FALSE)\nmean(rpois(n = 10000,5) >= 5)\n\n\n\n\nset.seed(163)\ndpois(x = 7, 5)\n\n[1] 0.1044449\n\nmean(rpois(n = 10000, 5) == 7)\n\n[1] 0.1107\n\nppois(q = 5, 5, lower.tail = TRUE)\n\n[1] 0.6159607\n\nmean(rpois(n = 10000, 5) <= 5)\n\n[1] 0.6109\n\nppois(q = 4, 5, lower.tail  = FALSE)\n\n[1] 0.5595067\n\nmean(rpois(n = 10000,5) >= 5)\n\n[1] 0.5542"
  },
  {
    "objectID": "slides/lec_week2.html#distribución-de-poisson-en-python",
    "href": "slides/lec_week2.html#distribución-de-poisson-en-python",
    "title": "Conceptos Básicos",
    "section": "Distribución de Poisson en Python",
    "text": "Distribución de Poisson en Python\n\nCódigoSalidas\n\n\nimport numpy as np\nfrom scipy.stats import poisson\n\nnp.random.seed(163)\npoisson.pmf(7, 5)\nsum(np.random.poisson(5, 10000) == 7)/10000\npoisson.cdf(7, 5)\nsum(np.random.poisson(5, 10000) <= 7)/10000\n1-poisson.cdf(6, 5)\nsum(np.random.poisson(5, 10000) >= 7)/10000\n\n\n\nimport numpy as np\nfrom scipy.stats import poisson\n\nnp.random.seed(163)\npoisson.pmf(7, 5)\n\n0.10444486295705395\n\nsum(np.random.poisson(5, 10000) == 7)/10000\n\n0.1024\n\npoisson.cdf(7, 5)\n\n0.8666283259299925\n\nsum(np.random.poisson(5, 10000) <= 7)/10000\n\n0.8662\n\n1-poisson.cdf(6, 5)\n\n0.2378165370270613\n\nsum(np.random.poisson(5, 10000) >= 7)/10000\n\n0.2369"
  },
  {
    "objectID": "slides/lec_week2.html#distribución-geométrica",
    "href": "slides/lec_week2.html#distribución-geométrica",
    "title": "Conceptos Básicos",
    "section": "Distribución geométrica",
    "text": "Distribución geométrica\nSea \\(X\\) una variable aleatoria que representa el número de fallas que ocurren antes de que se presente el primer éxito.Se dice entonces que la variable aleatoria \\(X\\) tiene una distribución geométrica con función de probabilidad:\n\\[\\mathbb{P}(X=k)=(1-p)^{k-1}p \\quad \\quad k=1,2,\\cdots\\]\nEn donde \\(p\\) es la probabilidad de éxito. Además, Si \\(X\\) sigue una distribución geométrica, entonces se cumple que:\n\n\\(\\displaystyle E[X]=\\dfrac{1}{p}\\)\n\\(V[X]=\\dfrac{(1-p)}{p^2}\\)"
  },
  {
    "objectID": "slides/lec_week2.html#distribución-geométrica-en-r",
    "href": "slides/lec_week2.html#distribución-geométrica-en-r",
    "title": "Conceptos Básicos",
    "section": "Distribución geométrica en R",
    "text": "Distribución geométrica en R\n\nCódigoSalidas\n\n\n\nset.seed(163)\np = 0.2\nn = 3\ndgeom(x = n, prob = p)\nmean(rgeom(n = 10000, prob = p) == n)\npgeom(q = n, prob = p, lower.tail = TRUE)\nmean(rgeom(n = 10000, prob = p) <= n)\npgeom(q = n, prob = p, lower.tail  = FALSE)\nmean(rgeom(n = 10000, prob = p) > n)\n\n\n\n\nset.seed(163)\np = 0.2\nn = 3\ndgeom(x = n, prob = p)\n\n[1] 0.1024\n\nmean(rgeom(n = 10000, prob = p) == n)\n\n[1] 0.1028\n\npgeom(q = n, prob = p, lower.tail = TRUE)\n\n[1] 0.5904\n\nmean(rgeom(n = 10000, prob = p) <= n)\n\n[1] 0.5905\n\npgeom(q = n, prob = p, lower.tail  = FALSE)\n\n[1] 0.4096\n\nmean(rgeom(n = 10000, prob = p) > n)\n\n[1] 0.4011"
  },
  {
    "objectID": "slides/lec_week2.html#distribución-geométrica-en-python",
    "href": "slides/lec_week2.html#distribución-geométrica-en-python",
    "title": "Conceptos Básicos",
    "section": "Distribución geométrica en Python",
    "text": "Distribución geométrica en Python\n\nCódigoSalidas\n\n\nimport numpy as np\nfrom scipy.stats import geom\n\nnp.random.seed(163)\ngeom.pmf(k=3, p=0.2)\nsum(np.random.geometric(0.2, 10000) == 3)/10000\ngeom.cdf(k=3, p=0.2)\nsum(np.random.geometric(0.2, 10000) <= 3)/10000\n1-geom.cdf(k=3, p=0.2)\nsum(np.random.geometric(0.2, 10000) > 3)/10000\n\n\n\nimport numpy as np\nfrom scipy.stats import geom\n\nnp.random.seed(163)\ngeom.pmf(k=3, p=0.2)\n\n0.12800000000000003\n\nsum(np.random.geometric(0.2, 10000) == 3)/10000\n\n0.1291\n\ngeom.cdf(k=3, p=0.2)\n\n0.488\n\nsum(np.random.geometric(0.2, 10000) <= 3)/10000\n\n0.4938\n\n1-geom.cdf(k=3, p=0.2)\n\n0.512\n\nsum(np.random.geometric(0.2, 10000) > 3)/10000\n\n0.5113"
  },
  {
    "objectID": "slides/lec_week2.html#distribución-hipergeométrica",
    "href": "slides/lec_week2.html#distribución-hipergeométrica",
    "title": "Conceptos Básicos",
    "section": "Distribución hipergeométrica",
    "text": "Distribución hipergeométrica\nSea \\(N\\) el número total de objetos de una población finita, de manera tal que \\(k\\) de éstos es de un tipo y \\(N-k\\) de otros. Si se selecciona una muestra aleatoria de la población constituida por \\(n\\) objetos de la probabilidad de que \\(x\\) sea de un tipo exactamente y \\(n-x\\) sea del otro, está dada por la función de probabilidad hipergeométrica:\n\\[\\displaystyle \\mathbb{P}(X=x)= \\dfrac{{{k}\\choose{x}} {{N-k}\\choose{n-x}}  }{  {{N}\\choose{n}}}\\quad \\quad x=1,2,\\cdots,n \\quad; x \\leq k\\quad ;n-x\\leq N-k\\]\nSi \\(X\\) sigue una distribución hipergeométrica y si \\(p=k/N\\)\n\n\\(E[X]=np\\)\n\\(V[X]=np(1-p)\\left( \\dfrac{N-n}{N-1}\\right)\\)"
  },
  {
    "objectID": "slides/lec_week2.html#distribución-normal",
    "href": "slides/lec_week2.html#distribución-normal",
    "title": "Conceptos Básicos",
    "section": "Distribución normal",
    "text": "Distribución normal\nSea \\(X\\) una variable aleatoria que toma valores reales, diremos que \\(X\\) sigue una distribución normal (o Gaussiana) si su función de densidad está por:\n\\[f_{X}(x)=\\dfrac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left[ -\\dfrac{1}{2}\\left(\\dfrac{x-\\mu}{\\sigma}\\right) ^2\\right],\\]\nEn donde los parámetros de la distribución son \\(\\mu\\) y \\(\\sigma\\) satisfacen las condiciones:\n\\[-\\infty<\\mu<\\infty, \\quad \\sigma^2>0\\]"
  },
  {
    "objectID": "slides/lec_week2.html#distribución-normal-en-r",
    "href": "slides/lec_week2.html#distribución-normal-en-r",
    "title": "Conceptos Básicos",
    "section": "Distribución normal en R",
    "text": "Distribución normal en R\n\nCódigoSalidas\n\n\n\nset.seed(163)\nmedia=100\nds=16\n\npnorm(q = 90, mean = media, sd = ds, lower.tail = TRUE)\nmean(rnorm(n = 10000, mean = media, sd = ds) <= 90)\npnorm(q = 140, mean = media, sd = ds, lower.tail = FALSE)\nmean(rnorm(n = 10000, mean = media, sd = ds) > 140)\n\n\n\n\nset.seed(163)\nmedia=100\nds=16\n\npnorm(q = 90, mean = media, sd = ds, lower.tail = TRUE)\n\n[1] 0.2659855\n\nmean(rnorm(n = 10000, mean = media, sd = ds) <= 90)\n\n[1] 0.2592\n\npnorm(q = 140, mean = media, sd = ds, lower.tail = FALSE)\n\n[1] 0.006209665\n\nmean(rnorm(n = 10000, mean = media, sd = ds) > 140)\n\n[1] 0.005"
  },
  {
    "objectID": "slides/lec_week2.html#distribución-normal-en-python",
    "href": "slides/lec_week2.html#distribución-normal-en-python",
    "title": "Conceptos Básicos",
    "section": "Distribución normal en Python",
    "text": "Distribución normal en Python\n\nCódigoSalidas\n\n\nimport numpy as np\nfrom scipy.stats import norm\n\nnp.random.seed(163)\nmu, sigma = 100, 16\nnorm.cdf(90, mu , sigma)\nsum(np.random.normal(mu, sigma, 10000) <= 90)/10000\n1-norm.cdf(140, mu , sigma)\nsum(np.random.normal(mu, sigma, 10000) > 140)/10000\n\n\n\nimport numpy as np\nfrom scipy.stats import norm\n\nnp.random.seed(163)\nmu, sigma = 100, 16\nnorm.cdf(90, mu , sigma)\n\n0.26598552904870054\n\nsum(np.random.normal(mu, sigma, 10000) <= 90)/10000\n\n0.2656\n\n1-norm.cdf(140, mu , sigma)\n\n0.006209665325776159\n\nsum(np.random.normal(mu, sigma, 10000) > 140)/10000\n\n0.0068"
  },
  {
    "objectID": "slides/lec_week2.html#distribución-uniforme",
    "href": "slides/lec_week2.html#distribución-uniforme",
    "title": "Conceptos Básicos",
    "section": "Distribución uniforme",
    "text": "Distribución uniforme\nSea \\(X\\) una variable aleatoria continua, diremos que \\(X\\) sigue una distribución uniforme sobre el intervalo \\((a,b)\\) si su función de densidad de probabilidad está dada por:\n\\[f_{X}(x)=\\begin{cases}1/(b-a) \\quad &a\\leq x \\leq b\\\\0 \\quad &e.o.c\\end{cases}\\]\nLos parámetros de la distribución cumplen las condiciones:\n\\[-\\infty<a<\\infty,\\quad -\\infty<b<\\infty\\]\n\n\\(E[X]=\\dfrac{(a+b)}{2}\\)\n\n\\(V[X]=\\dfrac{(b-a)^2}{12}\\)"
  },
  {
    "objectID": "slides/lec_week2.html#distribución-uniforme-en-r",
    "href": "slides/lec_week2.html#distribución-uniforme-en-r",
    "title": "Conceptos Básicos",
    "section": "Distribución uniforme en R",
    "text": "Distribución uniforme en R\n\nCódigoSalidas\n\n\n\nset.seed(163)\n\npunif(0.3, 0 , 1 , lower.tail = TRUE)\nmean(runif(n = 10000, 0, 1) <= 0.3)\npunif(0.3, 0 , 1 , lower.tail = FALSE)\nmean(runif(n = 10000, 0, 1) > 0.3)\n\n\n\n\nset.seed(163)\n\npunif(0.3, 0 , 1 , lower.tail = TRUE)\n\n[1] 0.3\n\nmean(runif(n = 10000, 0, 1) <= 0.3)\n\n[1] 0.298\n\npunif(0.3, 0 , 1 , lower.tail = FALSE)\n\n[1] 0.7\n\nmean(runif(n = 10000, 0, 1) > 0.3)\n\n[1] 0.7082"
  },
  {
    "objectID": "slides/lec_week2.html#distribución-uniforme-en-python",
    "href": "slides/lec_week2.html#distribución-uniforme-en-python",
    "title": "Conceptos Básicos",
    "section": "Distribución uniforme en Python",
    "text": "Distribución uniforme en Python\n\nCódigoSalidas\n\n\nimport numpy as np\nfrom scipy.stats import uniform\n\nnp.random.seed(163)\nuniform.cdf(0.3, 0 , 1)\nsum(np.random.uniform(0, 1, 10000) <= 0.3)/10000\n1-uniform.cdf(0.3, 0 , 1)\nsum(np.random.uniform(0, 1, 10000) > 0.3)/10000\n\n\n\nimport numpy as np\nfrom scipy.stats import uniform\n\nnp.random.seed(163)\nuniform.cdf(0.3, 0 , 1)\n\n0.3\n\nsum(np.random.uniform(0, 1, 10000) <= 0.3)/10000\n\n0.3048\n\n1-uniform.cdf(0.3, 0 , 1)\n\n0.7\n\nsum(np.random.uniform(0, 1, 10000) > 0.3)/10000\n\n0.699"
  },
  {
    "objectID": "slides/lec_week2.html#distribución-exponencial",
    "href": "slides/lec_week2.html#distribución-exponencial",
    "title": "Conceptos Básicos",
    "section": "Distribución exponencial",
    "text": "Distribución exponencial\nSea \\(X\\) una variable aleatoria continua que toma valores positivos, diremos que \\(X\\) sigue una distribución exponencial con parámetro \\(\\alpha>0\\) si su función de densidad está dada por:\n\\[f_{X}(x)=\\begin{cases}\\alpha e^{-\\alpha x} \\quad &x\\geq 0 \\\\0 \\quad &e.o.c\\end{cases}\\]\nAdemás se cumple que:\n\n\\(E[X]=\\dfrac{1}{\\alpha}\\)\n\n\\(V[X]=\\dfrac{1}{\\alpha^2}\\)"
  },
  {
    "objectID": "slides/lec_week2.html#distribución-exponencial-en-r",
    "href": "slides/lec_week2.html#distribución-exponencial-en-r",
    "title": "Conceptos Básicos",
    "section": "Distribución exponencial en R",
    "text": "Distribución exponencial en R\n\nCódigoSalidas\n\n\n\nset.seed(163)\n\npexp(2, 0.4, lower.tail = TRUE)\nmean(rexp(0.4, n = 10000) <= 2)\npexp(2, 0.4, lower.tail = FALSE)\nmean(rexp(0.4, n = 10000) > 2)\n\n\n\n\nset.seed(163)\n\npexp(2, 0.4, lower.tail = TRUE)\n\n[1] 0.550671\n\nmean(rexp(0.4, n = 10000) <= 2)\n\n[1] 0.5458\n\npexp(2, 0.4, lower.tail = FALSE)\n\n[1] 0.449329\n\nmean(rexp(0.4, n = 10000) > 2)\n\n[1] 0.446"
  },
  {
    "objectID": "slides/lec_week2.html#distribución-exponencial-en-python",
    "href": "slides/lec_week2.html#distribución-exponencial-en-python",
    "title": "Conceptos Básicos",
    "section": "Distribución exponencial en Python",
    "text": "Distribución exponencial en Python\n\nCódigoSalidas\n\n\nimport numpy as np\nfrom scipy.stats import expon\n\nnp.random.seed(163)\nexpon.cdf(2, scale=1/0.4)\nsum(np.random.exponential(1/0.4, 10000) <= 2)/10000\n1-expon.cdf(2, scale=1/0.4)\nsum(np.random.exponential(1/0.4, 10000) > 2)/10000\n\n\n\nimport numpy as np\nfrom scipy.stats import expon\n\nnp.random.seed(163)\nexpon.cdf(2, scale=1/0.4)\n\n0.5506710358827784\n\nsum(np.random.exponential(1/0.4, 10000) <= 2)/10000\n\n0.5572\n\n1-expon.cdf(2, scale=1/0.4)\n\n0.44932896411722156\n\nsum(np.random.exponential(1/0.4, 10000) > 2)/10000\n\n0.4416"
  },
  {
    "objectID": "slides/lec_week2.html#función-gamma",
    "href": "slides/lec_week2.html#función-gamma",
    "title": "Conceptos Básicos",
    "section": "Función gamma",
    "text": "Función gamma\nLa función gamma denotada por \\(\\Gamma\\) está definida por:\n\\[\\Gamma(p)=\\int_{0}^{\\infty} x^{p-1} e^{-x}dx \\hspace{20pt} p>0\\]\nEsta función cumple las siguientes propiedades:\n\n\\(\\Gamma(n)=(n-1)!\\)\n\n\\(\\Gamma(1/2)=\\sqrt{\\pi}\\)"
  },
  {
    "objectID": "slides/lec_week2.html#distribución-gamma",
    "href": "slides/lec_week2.html#distribución-gamma",
    "title": "Conceptos Básicos",
    "section": "Distribución gamma",
    "text": "Distribución gamma\nSea \\(X\\) una variable aleatoria continua que toma valores positivos. Diremos que \\(X\\) sigue una distribución Gamma si su función de densidad está dada por:\n\\[f_{X}(x)=\\begin{cases}\\dfrac{\\alpha}{\\Gamma(r)}(\\alpha x)^{r-1}e^{-\\alpha x} \\quad &x>0\\\\0 \\quad &e.o.c,\\end{cases}\\]\nEn donde los parámetros \\(r\\) y \\(\\alpha\\) son positivos.\nEs claro ver que un caso particular de la distribución Gamma es la distribución exponencial (\\(r=1\\)). Si \\(X\\) se distribuye Gamma entonces se cumple:\n\n\\(E[X]=r/\\alpha\\)\n\n\\(V[X]=r/\\alpha^2\\)"
  },
  {
    "objectID": "slides/lec_week2.html#distribución-gamma-en-r",
    "href": "slides/lec_week2.html#distribución-gamma-en-r",
    "title": "Conceptos Básicos",
    "section": "Distribución gamma en R",
    "text": "Distribución gamma en R\n\nCódigoSalidas\n\n\n\nset.seed(163)\n\npgamma(q = 3, shape = 10, scale = 1/4)\nmean(rgamma(shape=10, scale= 1/4, n = 10000) <= 3)\npgamma(q = 3, shape = 10, scale = 1/4, lower.tail = FALSE)\nmean(rgamma(shape=10, scale= 1/4, n = 10000) > 3)\n\n\n\n\nset.seed(163)\n\npgamma(q = 3, shape = 10, scale = 1/4)\n\n[1] 0.7576078\n\nmean(rgamma(shape=10, scale= 1/4, n = 10000) <= 3)\n\n[1] 0.7542\n\npgamma(q = 3, shape = 10, scale = 1/4, lower.tail = FALSE)\n\n[1] 0.2423922\n\nmean(rgamma(shape=10, scale= 1/4, n = 10000) > 3)\n\n[1] 0.2427"
  },
  {
    "objectID": "slides/lec_week2.html#distribución-gamma-en-python",
    "href": "slides/lec_week2.html#distribución-gamma-en-python",
    "title": "Conceptos Básicos",
    "section": "Distribución gamma en Python",
    "text": "Distribución gamma en Python\n\nCódigoSalidas\n\n\nimport numpy as np\nfrom scipy.stats import gamma\n\nnp.random.seed(163)\ngamma.cdf(3, a=10, scale=0.25)\nsum(np.random.gamma(10,0.25, 10000) <= 3)/10000\n1-gamma.cdf(3, a=10, scale=0.25)\nsum(np.random.gamma(10,0.25, 10000) > 3)/10000\n\n\n\nimport numpy as np\nfrom scipy.stats import gamma\n\nnp.random.seed(163)\ngamma.cdf(3, a=10, scale=0.25)\n\n0.7576078383294875\n\nsum(np.random.gamma(10,0.25, 10000) <= 3)/10000\n\n0.7595\n\n1-gamma.cdf(3, a=10, scale=0.25)\n\n0.24239216167051247\n\nsum(np.random.gamma(10,0.25, 10000) > 3)/10000\n\n0.2443"
  },
  {
    "objectID": "slides/lec_week2.html#distribución-t-student",
    "href": "slides/lec_week2.html#distribución-t-student",
    "title": "Conceptos Básicos",
    "section": "Distribución t-student",
    "text": "Distribución t-student\nSea \\(X\\) una variable aleatoria continua que toma valores reales, diremos que \\(X\\) sigue una distribución t-student con \\(\\nu\\) grados de libertad, si su función de densidad de probabilidad está dada por:\n\\[f(t) = \\dfrac{\\Gamma(\\dfrac{\\nu+1}{2})} {\\sqrt{\\nu\\pi}\\,\\Gamma(\\dfrac{\\nu}{2})} \\left(1+\\dfrac{t^2}{\\nu} \\right)^{\\!-\\dfrac{\\nu+1}{2}},\\]\ndonde \\(\\Gamma\\) es la función gamma. Si \\(X\\) se distribuye t-student entonces:\n\n\\(\\mathbb{E}[X]=0\\) para \\(\\nu>1\\). Indefinida para otros valores.\n\n\\(\\mathbb{V}[X]=\\dfrac{\\nu}{\\nu -2}\\) para \\(\\nu>2\\). Indefinida para otros valores."
  },
  {
    "objectID": "slides/lec_week2.html#distribución-t-student-en-r",
    "href": "slides/lec_week2.html#distribución-t-student-en-r",
    "title": "Conceptos Básicos",
    "section": "Distribución t-student en R",
    "text": "Distribución t-student en R\n\nCódigoSalidas\n\n\n\nset.seed(163)\n\npt(q=1.9, df=15, lower.tail = T)\nmean(rt(15, n = 10000) <= 1.9)\npt(q=1.9, df=15, lower.tail = F)\nmean(rt(15, n = 10000) > 1.9)\n\n\n\n\nset.seed(163)\n\npt(q=1.9, df=15, lower.tail = T)\n\n[1] 0.9615845\n\nmean(rt(15, n = 10000) <= 1.9)\n\n[1] 0.9637\n\npt(q=1.9, df=15, lower.tail = F)\n\n[1] 0.03841551\n\nmean(rt(15, n = 10000) > 1.9)\n\n[1] 0.0386"
  },
  {
    "objectID": "slides/lec_week2.html#distribución-t-student-en-python",
    "href": "slides/lec_week2.html#distribución-t-student-en-python",
    "title": "Conceptos Básicos",
    "section": "Distribución t-student en Python",
    "text": "Distribución t-student en Python\n\nCódigoSalidas\n\n\nimport numpy as np\nfrom scipy.stats import t\n\nnp.random.seed(163)\nt.cdf(1.9, 15)\nsum(np.random.standard_t(15, 10000) <= 1.9)/10000\n1-t.cdf(1.9, 15)\nsum(np.random.standard_t(15, 10000) > 1.9)/10000\n\n\n\nimport numpy as np\nfrom scipy.stats import t\n\nnp.random.seed(163)\nt.cdf(1.9, 15)\n\n0.9615844871419956\n\nsum(np.random.standard_t(15, 10000) <= 1.9)/10000\n\n0.9611\n\n1-t.cdf(1.9, 15)\n\n0.038415512858004375\n\nsum(np.random.standard_t(15, 10000) > 1.9)/10000\n\n0.0392"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IND163C - Análisis de Negocios (Business Analytics)",
    "section": "",
    "text": "En este curso se estudian los fundamentos del análisis de información a través del desarrollo de aplicaciones Cloud (nube) mediante lenguajes de programación, vinculando áreas técnicas especializadas en Analytics con otras áreas de la organización, agregando valor al proceso de diagnóstico y toma de decisiones en tiempo real.\nA lo largo del curso se evalúan el uso de Analytics midiendo el impacto de esta herramienta a nivel individual y social, decidiendo de forma ética y socialmente responsable respecto del uso de la información."
  },
  {
    "objectID": "index.html#softwares",
    "href": "index.html#softwares",
    "title": "IND163C - Análisis de Negocios (Business Analytics)",
    "section": "Softwares",
    "text": "Softwares\nPara la mayoría de las aplicaciones utilizaremos R y Python, por lo que se sugiere utilizar un IDE como RStudio o Spyder.\nPara la entrega de informes y talleres que requieran uso de programación, se recomienda el uso de Rmarkdown, Jupyter Notebook o \\(\\LaTeX\\) para la confección de documentos a entregar."
  },
  {
    "objectID": "index.html#bibliografía-principal",
    "href": "index.html#bibliografía-principal",
    "title": "IND163C - Análisis de Negocios (Business Analytics)",
    "section": "Bibliografía principal",
    "text": "Bibliografía principal\nLa bibliografía principal del curso es:\n\n\n\n\n\n\nData Science For Business: What You Need to Know About Data Mining & Data-Analytic Thinking. Provost F., Fawcett, T.\n\n\n\n\n\n\n\nThink like a Data Scientist: Tackle the data science process step-by-step. Godsey B.\n\n\n\n\n\n\n\nNumsense! Data Science for the Layman: No Math Added. Ng, A, Soo K."
  },
  {
    "objectID": "index.html#bibliografía-secundaria-y-de-profundización",
    "href": "index.html#bibliografía-secundaria-y-de-profundización",
    "title": "IND163C - Análisis de Negocios (Business Analytics)",
    "section": "Bibliografía secundaria y de profundización",
    "text": "Bibliografía secundaria y de profundización\nLa bibliografía secundaria y de profundización de las distintas temáticas a estudiar del curso es:\n\n\n\n\n\n\nMachine Learning with R Expert techniques for predictive modeling. Lantz, Brett.\n\n\n\n\n\n\n\nAn Introduction to Statistical Learning with Applications in R. James, Gareth, Witten, Daniela, Hastie, Trevor, Tibshirani Robert.\n\n\n\n\n\n\n\nHands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. Géron, Aurélien."
  },
  {
    "objectID": "pages/week1.html",
    "href": "pages/week1.html",
    "title": "Semana 1",
    "section": "",
    "text": "Leer el programa del curso"
  },
  {
    "objectID": "pages/week1.html#presentación",
    "href": "pages/week1.html#presentación",
    "title": "Semana 1",
    "section": "Presentación",
    "text": "Presentación\nVer presentación en pantalla completa"
  },
  {
    "objectID": "pages/week1.html#material-adicional",
    "href": "pages/week1.html#material-adicional",
    "title": "Semana 1",
    "section": "Material adicional",
    "text": "Material adicional\n\nLeer Capítulo 1, Data Science For Business: What You Need to Know About Data Mining & Data-Analytic Thinking.\nLeer Capítulo 1 y 2, Think like a Data Scientist: Tackle the data science process step-by-step."
  },
  {
    "objectID": "slides/lec_week1.html#de-qué-trata-el-curso",
    "href": "slides/lec_week1.html#de-qué-trata-el-curso",
    "title": "Análisis de Negocios",
    "section": "¿De qué trata el curso?",
    "text": "¿De qué trata el curso?\nA lo largo del curso analizaremos los fundamentos del análisis de datos mediante el uso de lenguajes de programación con el fin de agregar al valor al proceso de diagnóstico y toma de decisiones en tiempo real.\nCon el fin de tener mayor claridad de lo que han estudiado\n\n\n¿Qué herramientas computacionales han visto en cursos anteriores?\n¿Hasta qué vieron en el curso MAT042: Probabilidad y Estadística?\n¿Han tenido alguna experiencia trabajando con R o Python en el análisis de datos?"
  },
  {
    "objectID": "slides/lec_week1.html#horario-de-clases",
    "href": "slides/lec_week1.html#horario-de-clases",
    "title": "Análisis de Negocios",
    "section": "Horario de clases",
    "text": "Horario de clases\n\n\n\n\nDía\nHorario\nLugar\n\n\n\n\nCátedra #1\nViernes\n09:35 am - 10:45 am\nM401-H\n\n\nCátedra #2\nViernes\n10:55 am - 12:05 pm\nM401-H\n\n\n\nPágina del curso\nUtilizaremos el Aula USM y el sitio https://ind163c-2022-02.netlify.app/. Ambas páginas tendrán la misma información, sin embargo, para efectos de entrega de informes el medio oficial será el aula USM."
  },
  {
    "objectID": "slides/lec_week1.html#qué-necesitaremos-a-lo-largo-del-curso",
    "href": "slides/lec_week1.html#qué-necesitaremos-a-lo-largo-del-curso",
    "title": "Análisis de Negocios",
    "section": "¿Qué necesitaremos a lo largo del curso?",
    "text": "¿Qué necesitaremos a lo largo del curso?\n\n\n\n\n\n\n\n\n\n\n\nAdicionalmente, se recomienda utilizar un IDE como RStudio o Spyder."
  },
  {
    "objectID": "slides/lec_week1.html#bibliografía-principal",
    "href": "slides/lec_week1.html#bibliografía-principal",
    "title": "Análisis de Negocios",
    "section": "Bibliografía principal",
    "text": "Bibliografía principal\n\n\n\n\n\n\nData Science For Business: What You Need to Know About Data Mining & Data-Analytic Thinking. Provost F., Fawcett, T.\n\n\n\n\n \n\n\n\n\n\nThink like a Data Scientist: Tackle the data science process step-by-step. Godsey B.\n\n\n\n\n \n\n\n\n\n\nNumsense! Data Science for the Layman: No Math Added. Ng, A, Soo K."
  },
  {
    "objectID": "slides/lec_week1.html#bibliografía-secundaria-y-de-profundización",
    "href": "slides/lec_week1.html#bibliografía-secundaria-y-de-profundización",
    "title": "Análisis de Negocios",
    "section": "Bibliografía secundaria y de profundización",
    "text": "Bibliografía secundaria y de profundización\n\n\n\n\n\n\nMachine Learning with R Expert techniques for predictive modeling. Lantz, Brett.\n\n\n\n\n \n\n\n\n\n\nAn Introduction to Statistical Learning with Applications in R. James, Gareth, Witten, Daniela, Hastie, Trevor, Tibshirani Robert.\n\n\n\n\n \n\n\n\n\n\nHands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. Géron, Aurélien."
  },
  {
    "objectID": "slides/lec_week1.html#introducción-a-analytics",
    "href": "slides/lec_week1.html#introducción-a-analytics",
    "title": "Análisis de Negocios",
    "section": "Introducción a Analytics",
    "text": "Introducción a Analytics\n\nDel sistema de información de marketing al analytics ¿Qué es Analytics? ¿Para qué sirve? ¿En qué contexto es necesario?\nAnalytics en el mundo y en Chile.\nImpacto social de Analytics en Chile y el mundo.\nComposición de perfiles de un área genérica de analytics en una empresa nacional."
  },
  {
    "objectID": "slides/lec_week1.html#conceptos-básicos",
    "href": "slides/lec_week1.html#conceptos-básicos",
    "title": "Análisis de Negocios",
    "section": "Conceptos Básicos",
    "text": "Conceptos Básicos\n\nEscuelas de probabilidad\nDistribuciones probabilistas\nTest de hipótesis e intervalos de confianza\nRegresión lineal\nEstimadores blandos y robustos\nTécnicas básicas de segmentación\nReglas de asociación\nÁrboles de decisión\nDeep learning, machine learning e inteligencia artificial."
  },
  {
    "objectID": "slides/lec_week1.html#lenguajes-de-programación-y-aplicaciones-en-la-nube",
    "href": "slides/lec_week1.html#lenguajes-de-programación-y-aplicaciones-en-la-nube",
    "title": "Análisis de Negocios",
    "section": "Lenguajes de programación y aplicaciones en la nube",
    "text": "Lenguajes de programación y aplicaciones en la nube\n\nIntroducción a bases de datos\nIntroducción a Python y R\nIntroducción a Google Cloud y AWS."
  },
  {
    "objectID": "slides/lec_week1.html#planificación-de-proyectos-de-analytics-y-aplicaciones-de-negocios",
    "href": "slides/lec_week1.html#planificación-de-proyectos-de-analytics-y-aplicaciones-de-negocios",
    "title": "Análisis de Negocios",
    "section": "Planificación de proyectos de analytics y aplicaciones de negocios",
    "text": "Planificación de proyectos de analytics y aplicaciones de negocios\n\nAplicaciones transversales a la empresa."
  },
  {
    "objectID": "slides/lec_week1.html#ponderaciones",
    "href": "slides/lec_week1.html#ponderaciones",
    "title": "Análisis de Negocios",
    "section": "Ponderaciones",
    "text": "Ponderaciones\nLa metodología de evaluación es la siguiente:\n\n\n\n\n\n\n\nTipo de evaluación\nPorcentaje que corresponde\n\n\n\n\nCertamen 1 (\\(C_1\\))\n20%\n\n\nCertamen 2 (\\(C_2\\))\n20%\n\n\nInforme escrito de avance de proyecto (\\(P_1\\))\n20%\n\n\nInforme escrito y oral final de proyecto (\\(P_2\\))\n40%\n\n\n\nLa nota final (\\(NF\\)) de la asignatura se calculará según:\n\\[\nNF= 0.2*C_1+0.2*C_2+0.2*P_1+0.4*P_2\n\\]"
  },
  {
    "objectID": "slides/lec_week1.html#requerimientos-mínimos-de-aprobación",
    "href": "slides/lec_week1.html#requerimientos-mínimos-de-aprobación",
    "title": "Análisis de Negocios",
    "section": "Requerimientos mínimos de aprobación",
    "text": "Requerimientos mínimos de aprobación\n\nPromedio Certámenes (\\(C_1\\) y \\(C_2\\)): \\(\\overline{C}_{1,2}\\geq 55\\)\nPromedio Proyecto (\\(P_1\\) y \\(P_2\\)): \\(\\overline{P}_{1,2}\\geq 55\\)"
  },
  {
    "objectID": "slides/lec_week1.html#metodología-del-curso",
    "href": "slides/lec_week1.html#metodología-del-curso",
    "title": "Análisis de Negocios",
    "section": "Metodología del curso",
    "text": "Metodología del curso\n\nAntes de cada sesión, se mandará una lectura de preparación para la sesión\nEl enfoque principal será aplicado, pero sin dejar de lado los fundamentos matemáticos\nSe pondrá a disposición material adicional para estudiar:\n\nEjemplos y ejercicios teóricos\nCódigos\n\nEl curso será autocontenido, pero requiere al menos conocimiento básico de probabilidad y estadística."
  },
  {
    "objectID": "slides/lec_week1.html#ayudantía",
    "href": "slides/lec_week1.html#ayudantía",
    "title": "Análisis de Negocios",
    "section": "Ayudantía",
    "text": "Ayudantía\n\nAyudante: Nicolás Cárdenas\nHorario a definir"
  },
  {
    "objectID": "slides/lec_week1.html#analytics",
    "href": "slides/lec_week1.html#analytics",
    "title": "Análisis de Negocios",
    "section": "Analytics",
    "text": "Analytics\n\n¿Qué noción tienen sobre analytics?\n¿Dé que sirve en la empresa? ¿Algún ejemplo?\n¿En qué contexto es necesario?\n¿Cómo afecta el uso de analytics en Chile y en el mundo?\n¿Cuál es su impacto social?"
  },
  {
    "objectID": "slides/lec_week1.html#data-analytics",
    "href": "slides/lec_week1.html#data-analytics",
    "title": "Análisis de Negocios",
    "section": "Data Analytics",
    "text": "Data Analytics\n\nEl avance tecnológico ha permitido la recolección de datos dentro y fuera de la empresa\nEsta disponibilidad de datos ha fomentado la creación de metodologías para extraer información de los datos\nLa gran mayoría de las empresas tienen equipos especializados en extraer la mayor cantidad de información útil para la empresa\nAntiguamente, la industria exploraba los conjuntos de datos disponibles de manera más o menos manual, pero debido al incremento del volumen de datos, esto ya no es posible.\nEn la actualidad, el proceso de descubrir información relevante en los conjuntos de datos disponibles, se le llama data mining"
  },
  {
    "objectID": "slides/lec_week1.html#data-mining",
    "href": "slides/lec_week1.html#data-mining",
    "title": "Análisis de Negocios",
    "section": "Data mining",
    "text": "Data mining\n\nLa aplicación más frecuente de las técnicas de data mining están en marketing. Por ejemplo:\n\nMarketing dirigido\nPublicidad online\nRecomendaciones de compra\n\nEn el sector financiero, es frecuente encontrar estas técnicas en la creación de puntajes crediticios e identificación de fraude\nMuchas empresas han generado una ventaja comparativa utilizando Data Science estratégicamente"
  },
  {
    "objectID": "slides/lec_week1.html#ejemplo",
    "href": "slides/lec_week1.html#ejemplo",
    "title": "Análisis de Negocios",
    "section": "Ejemplo",
    "text": "Ejemplo\n\nEn una empresa de telecomunicaciones se tiene un problema de retención de clientes\nMuchos de estos clientes se van a la competencia\nA este fenómeno se le conoce como CHURN o tasa de cancelación de clientes.\nTenemos dos formas de abordar la problemática:\n\nAtraer nuevos clientes\nMantener a los clientes actuales\n\nEn general, atraer nuevos clientes es más caro que mantener a los actuales\n¿Cómo podemos identificar clientes que son más propenso a cambiar de compañía?"
  },
  {
    "objectID": "slides/lec_week1.html#automatización-de-decisiones",
    "href": "slides/lec_week1.html#automatización-de-decisiones",
    "title": "Análisis de Negocios",
    "section": "Automatización de decisiones",
    "text": "Automatización de decisiones\nLa automatización de decisiones en una empresa o DDD (Data-driven decision-making), hace referencia a la práctica de basar las decisiones en el análisis de datos en vez de la intuición. Brynjolfsson, Hitt, & Kim, 2011 realizaron un estudio que analizó el rendimiento de distintas empresas, evaluando cada una de estas mediante un índice DDD que cuantificaba cuan predominante eran las decisiones basadas en el análisis de datos.\n\nLos autores mostraron estadísticamente que mientras más decisiones basadas en el análisis de datos, más productiva era la empresa. Más aún, las diferencias entre las empresas con un alto y bajo nivel de índice DDD eran notablemente grandes."
  },
  {
    "objectID": "slides/lec_week1.html#procesamiento-de-datos-y-big-data",
    "href": "slides/lec_week1.html#procesamiento-de-datos-y-big-data",
    "title": "Análisis de Negocios",
    "section": "Procesamiento de datos y “Big Data”",
    "text": "Procesamiento de datos y “Big Data”\nActualmente, dentro de las empresas existen varias áreas que trabajan con datos. No obstante, no todas ellas recaen dentro de lo que conocemos como Data Science. En general, llamamos ciencia de datos a los análisis posteriores al acceso de datos, por lo que el almacenamiento y procesamiento de datos (en el sentido ingenieril) no sería llamado Data Science. Sin embargo, el trabajo hecho por los Ingenieros de Datos es fundamental para el correcto ejercicio del análisis de negocios.\n\nEl concepto Big Data hace referencia al volumen gigante de datos disponibles, y particularmente, a las tecnologías asociadas que permiten su almacenamiento y correcto acceso, como por ejemplo Hadoop o MongoDB. Estas tecnologías son cruciales para tener acceso a los datos que se analizarán. Este tipo de tecnología (y sus alternativas) ya son estándar en la industria."
  },
  {
    "objectID": "slides/lec_week1.html#área-de-analytics",
    "href": "slides/lec_week1.html#área-de-analytics",
    "title": "Análisis de Negocios",
    "section": "Área de analytics",
    "text": "Área de analytics\nExisten distintas formas de segmentar un área de analytics en una empresa, pero por lo general, se adecuan al proceso general de minería de datos CRISP-DM (Cross Industry Standard Process for Data Mining)"
  },
  {
    "objectID": "slides/lec_week1.html#roles-frecuentes",
    "href": "slides/lec_week1.html#roles-frecuentes",
    "title": "Análisis de Negocios",
    "section": "Roles frecuentes",
    "text": "Roles frecuentes\nEntre los roles más frecuentes en el área de analytics de una empresa están:\n\nBI Analyst\nData Engineer\nSoftware Engineer\nData Scientist\nMachine Learning Researcher/Engineer\nProduct Owner"
  },
  {
    "objectID": "slides/lec_week1.html#habilidades-de-algunos-de-estos-roles",
    "href": "slides/lec_week1.html#habilidades-de-algunos-de-estos-roles",
    "title": "Análisis de Negocios",
    "section": "Habilidades de algunos de estos roles",
    "text": "Habilidades de algunos de estos roles"
  },
  {
    "objectID": "slides/lec_week1.html#manejando-un-equipo-de-data-science",
    "href": "slides/lec_week1.html#manejando-un-equipo-de-data-science",
    "title": "Análisis de Negocios",
    "section": "Manejando un equipo de Data Science",
    "text": "Manejando un equipo de Data Science\nEs posible ver un proceso de minería de datos como un proceso de desarollo de software, aplicando las metodologías usuales en aquellas áreas (Agile/Scrum). Pero dependiendo del rubro de la empresa, será una mezcla entre las metodologías ágiles de desarrollo de software y el proceso CRISP-DM."
  },
  {
    "objectID": "slides/lec_week1.html#herramientas-del-análisis-de-negocios",
    "href": "slides/lec_week1.html#herramientas-del-análisis-de-negocios",
    "title": "Análisis de Negocios",
    "section": "Herramientas del análisis de negocios",
    "text": "Herramientas del análisis de negocios\n\nEstadística\nDatabase Querying\nData Warehousing\nMachine Learning"
  },
  {
    "objectID": "slides/lec_week1.html#planteamiento-de-la-problemática",
    "href": "slides/lec_week1.html#planteamiento-de-la-problemática",
    "title": "Análisis de Negocios",
    "section": "Planteamiento de la problemática",
    "text": "Planteamiento de la problemática\n\nSi no sabemos que debemos resolver, no podremos proveer una respuesta al problema\nAntes de intentar solucionar el problema, debemos identificar en su totalidad el contexto del problema\nPreguntas como:\n\n¿El problema es recurrente?¿Requerirá una automatización posterior a encontrar una solución?\n¿Qué magnitud tiene el problema?\n¿Qué herramientas tengo disponibles? ¿Hay personas capacitadas para resolver concretamente el problema?\n¿Qué tipo de datos se tiene a disposición? ¿Es esta suficiente?\n¿Qué tanto tiempo se tiene ha disposición?"
  },
  {
    "objectID": "slides/lec_week1.html#evaluando-la-solución",
    "href": "slides/lec_week1.html#evaluando-la-solución",
    "title": "Análisis de Negocios",
    "section": "Evaluando la solución",
    "text": "Evaluando la solución\n\nUna vez identificado a cabalidad la problemática, resta preguntarnos:\n\n¿Alguien ha resuelto este tipo de problemas? ¿Cómo?\n¿Es posible replicar aquella solución?\n\nFinalmente, debemos plantear y estructurar la forma en que procederemos. Sin embargo, debemos preguntarnos:\n\n¿Es posible hacerlo dentro del contexto de la empresa?\n¿Resuelve concretamente el problema?\n¿Es eficiente la solución que podremos entregar?\n\n\n\nAl planificar los pasos a seguir, debemos ser lo más flexible posible, debido a que usualmente en proyectos de data science, los percanses suelen ocurrir."
  },
  {
    "objectID": "pages/week2.html",
    "href": "pages/week2.html",
    "title": "Semana 2",
    "section": "",
    "text": "Instalar R y Python.\nSe recomienda también instalar Rstudio y Spyder\nRepasar materia MAT042"
  },
  {
    "objectID": "pages/week2.html#presentación",
    "href": "pages/week2.html#presentación",
    "title": "Semana 2",
    "section": "Presentación",
    "text": "Presentación\nVer presentación en pantalla completa"
  },
  {
    "objectID": "pages/week2.html#material-adicional",
    "href": "pages/week2.html#material-adicional",
    "title": "Semana 2",
    "section": "Material adicional",
    "text": "Material adicional\nCapítulo 1 a 8 de Introducción a R en español\nCapítulo 1 y 2 de Python for probability, statistics, and machine learning"
  },
  {
    "objectID": "slides/lec_week2.html#distribución-binomial-en-r-y-python",
    "href": "slides/lec_week2.html#distribución-binomial-en-r-y-python",
    "title": "Conceptos Básicos",
    "section": "Distribución binomial en R y Python",
    "text": "Distribución binomial en R y Python\n\n\n\nIND 163 - Semana 2"
  },
  {
    "objectID": "pages/calendario.html",
    "href": "pages/calendario.html",
    "title": "Calendario",
    "section": "",
    "text": "Semana\nFecha\nPreparación\nPresentación\nMaterial adicional\nCertamen\nTrabajo Final\n\n\n\n\n1\n19/08\n📖\n🖥️\n📋\n\n\n\n\n2\n26/08\n📖\n🖥️\n📋\n\n\n\n\n3\n02/09\n📖\n🖥️\n📋\n\n\n\n\n4\n09/09\n📖\n🖥️\n📋\n\n\n\n\n5\n30/09\n📖\n🖥️\n📋\n📝\n\n\n\n6\n07/10\n📖\n🖥️\n📋\n\n\n\n\n7\n14/10\n📖\n🖥️\n📋\n\n\n\n\n8\n21/10\n📖\n🖥️\n📋\n\n\n\n\n9\n28/10\n📖\n🖥️\n📋\n\n\n\n\n10\n04/11\n📖\n🖥️\n📋\n📝\n📔\n\n\n11\n11/11\n📖\n🖥️\n📋\n\n\n\n\n12\n18/11\n📖\n🖥️\n📋\n\n\n\n\n13\n25/11\n📖\n🖥️\n📋\n\n📔\n\n\n14\n02/12\n📖\n🖥️\n📋\n\n\n\n\n15\n09/12\n📖\n🖥️\n📋\n\n\n\n\n16\n16/12\n📖\n🖥️\n📋\n\n📔"
  },
  {
    "objectID": "pages/evaluaciones.html",
    "href": "pages/evaluaciones.html",
    "title": "Evaluaciones",
    "section": "",
    "text": "Requerimientos mínimos de aprobación\n\nPromedio Certámenes (\\(C_1\\) y \\(C_2\\)): \\(\\overline{C}_{1,2}\\geq 55\\)\nPromedio Proyecto (\\(P_1\\) y \\(P_2\\)): \\(\\overline{P}_{1,2}\\geq 55\\)"
  },
  {
    "objectID": "pages/programa.html",
    "href": "pages/programa.html",
    "title": "Programa del curso",
    "section": "",
    "text": "Introducción a Analytics\n\nDel sistema de información de marketing al analytics ¿Qué es Analytics? ¿Para qué sirve? ¿En qué contexto es necesario?\nAnalytics en el mundo y en Chile.\nImpacto social de Analytics en Chile y el mundo.\nComposición de perfiles de un área genérica de analytics en una empresa nacional.\n\n\n\nConceptos Básicos\n\nEscuelas de probabilidad\nDistribuciones probabilistas\nTest de hipótesis e intervalos de confianza\nRegresión lineal\nEstimadores blandos y robustos\nTécnicas básicas de segmentación\nReglas de asociación\nÁrboles de decisión\nDeep learning, machine learning e inteligencia artificial.\n\n\n\nLenguajes de programación y aplicaciones en la nube\n\nIntroducción a bases de datos\nIntroducción a Python y R\nIntroducción a Google Cloud y AWS.\n\n\n\nPlanificación de proyectos de analitycs y aplicaciones de negocios\n\nAplicaciones transversales a la empresa."
  },
  {
    "objectID": "pages/week10.html#presentación",
    "href": "pages/week10.html#presentación",
    "title": "Semana 10",
    "section": "Presentación",
    "text": "Presentación\nVer presentación en pantalla completa"
  },
  {
    "objectID": "pages/week10.html#material-adicional",
    "href": "pages/week10.html#material-adicional",
    "title": "Semana 10",
    "section": "Material adicional",
    "text": "Material adicional"
  },
  {
    "objectID": "pages/week11.html#presentación",
    "href": "pages/week11.html#presentación",
    "title": "Semana 11",
    "section": "Presentación",
    "text": "Presentación\nVer presentación en pantalla completa"
  },
  {
    "objectID": "pages/week11.html#material-adicional",
    "href": "pages/week11.html#material-adicional",
    "title": "Semana 11",
    "section": "Material adicional",
    "text": "Material adicional"
  },
  {
    "objectID": "pages/week12.html#presentación",
    "href": "pages/week12.html#presentación",
    "title": "Semana 12",
    "section": "Presentación",
    "text": "Presentación\nVer presentación en pantalla completa"
  },
  {
    "objectID": "pages/week12.html#material-adicional",
    "href": "pages/week12.html#material-adicional",
    "title": "Semana 12",
    "section": "Material adicional",
    "text": "Material adicional"
  },
  {
    "objectID": "pages/week13.html#presentación",
    "href": "pages/week13.html#presentación",
    "title": "Semana 13",
    "section": "Presentación",
    "text": "Presentación\nVer presentación en pantalla completa"
  },
  {
    "objectID": "pages/week13.html#material-adicional",
    "href": "pages/week13.html#material-adicional",
    "title": "Semana 13",
    "section": "Material adicional",
    "text": "Material adicional"
  },
  {
    "objectID": "pages/week14.html#presentación",
    "href": "pages/week14.html#presentación",
    "title": "Semana 14",
    "section": "Presentación",
    "text": "Presentación\nVer presentación en pantalla completa"
  },
  {
    "objectID": "pages/week14.html#material-adicional",
    "href": "pages/week14.html#material-adicional",
    "title": "Semana 14",
    "section": "Material adicional",
    "text": "Material adicional"
  },
  {
    "objectID": "pages/week15.html#presentación",
    "href": "pages/week15.html#presentación",
    "title": "Semana 15",
    "section": "Presentación",
    "text": "Presentación\nVer presentación en pantalla completa"
  },
  {
    "objectID": "pages/week15.html#material-adicional",
    "href": "pages/week15.html#material-adicional",
    "title": "Semana 15",
    "section": "Material adicional",
    "text": "Material adicional"
  },
  {
    "objectID": "pages/week16.html#presentación",
    "href": "pages/week16.html#presentación",
    "title": "Semana 16",
    "section": "Presentación",
    "text": "Presentación\nVer presentación en pantalla completa"
  },
  {
    "objectID": "pages/week16.html#material-adicional",
    "href": "pages/week16.html#material-adicional",
    "title": "Semana 16",
    "section": "Material adicional",
    "text": "Material adicional"
  },
  {
    "objectID": "pages/week3.html",
    "href": "pages/week3.html",
    "title": "Semana 3",
    "section": "",
    "text": "Repasar materia MAT042: Test de hipótesis e intervalos de confianza"
  },
  {
    "objectID": "pages/week3.html#presentación",
    "href": "pages/week3.html#presentación",
    "title": "Semana 3",
    "section": "Presentación",
    "text": "Presentación\nVer presentación en pantalla completa"
  },
  {
    "objectID": "pages/week3.html#material-adicional",
    "href": "pages/week3.html#material-adicional",
    "title": "Semana 3",
    "section": "Material adicional",
    "text": "Material adicional\n\nPara contextualización en la industria, capítulo 2 de Data Science For Business: What You Need to Know About Data Mining & Data-Analytic Thinking.\nCapítulo 6, Numsense! Data Science for the Layman: No Math Added. Ng, A, Soo K."
  },
  {
    "objectID": "pages/week4.html#presentación",
    "href": "pages/week4.html#presentación",
    "title": "Semana 4",
    "section": "Presentación",
    "text": "Presentación\nVer presentación en pantalla completa"
  },
  {
    "objectID": "pages/week4.html#material-adicional",
    "href": "pages/week4.html#material-adicional",
    "title": "Semana 4",
    "section": "Material adicional",
    "text": "Material adicional"
  },
  {
    "objectID": "pages/week5.html#presentación",
    "href": "pages/week5.html#presentación",
    "title": "Semana 5",
    "section": "Presentación",
    "text": "Presentación\nVer presentación en pantalla completa"
  },
  {
    "objectID": "pages/week5.html#material-adicional",
    "href": "pages/week5.html#material-adicional",
    "title": "Semana 5",
    "section": "Material adicional",
    "text": "Material adicional"
  },
  {
    "objectID": "pages/week6.html#presentación",
    "href": "pages/week6.html#presentación",
    "title": "Semana 6",
    "section": "Presentación",
    "text": "Presentación\nVer presentación en pantalla completa"
  },
  {
    "objectID": "pages/week6.html#material-adicional",
    "href": "pages/week6.html#material-adicional",
    "title": "Semana 6",
    "section": "Material adicional",
    "text": "Material adicional"
  },
  {
    "objectID": "pages/week7.html#presentación",
    "href": "pages/week7.html#presentación",
    "title": "Semana 7",
    "section": "Presentación",
    "text": "Presentación\nVer presentación en pantalla completa"
  },
  {
    "objectID": "pages/week7.html#material-adicional",
    "href": "pages/week7.html#material-adicional",
    "title": "Semana 7",
    "section": "Material adicional",
    "text": "Material adicional"
  },
  {
    "objectID": "pages/week8.html#presentación",
    "href": "pages/week8.html#presentación",
    "title": "Semana 8",
    "section": "Presentación",
    "text": "Presentación\nVer presentación en pantalla completa"
  },
  {
    "objectID": "pages/week8.html#material-adicional",
    "href": "pages/week8.html#material-adicional",
    "title": "Semana 8",
    "section": "Material adicional",
    "text": "Material adicional"
  },
  {
    "objectID": "pages/week9.html#presentación",
    "href": "pages/week9.html#presentación",
    "title": "Semana 9",
    "section": "Presentación",
    "text": "Presentación\nVer presentación en pantalla completa"
  },
  {
    "objectID": "pages/week9.html#material-adicional",
    "href": "pages/week9.html#material-adicional",
    "title": "Semana 9",
    "section": "Material adicional",
    "text": "Material adicional"
  },
  {
    "objectID": "slides/lec_week3.html#parámetro-y-espacio-paramétrico",
    "href": "slides/lec_week3.html#parámetro-y-espacio-paramétrico",
    "title": "Introducción a inferencia estadística",
    "section": "Parámetro y espacio paramétrico",
    "text": "Parámetro y espacio paramétrico\n\nParámetro: Es una característica numérica de la distribución de la población, que describe, parcial o completamente, la función de masa de probabilidad de la característica de interés, habitualmente se simboliza por la letra griega \\(\\theta\\).\nEspacio paramétrico: Es el conjunto de posibles valores que puede(n) ser considerado(s) para el(los) parámetro(s). Se simboliza por la letra griega mayúscula \\(\\Theta\\)."
  },
  {
    "objectID": "slides/lec_week3.html#método-de-máxima-verosimilitud",
    "href": "slides/lec_week3.html#método-de-máxima-verosimilitud",
    "title": "Introducción a inferencia estadística",
    "section": "Método de máxima verosimilitud",
    "text": "Método de máxima verosimilitud\nEl método de máxima verosimilitud consiste en encontrar el valor(es) del parámetro(s) que maximiza la función de masa (densidad) de probabilidad conjunta de la muestra, llamada verosimilitud.\n\n\nFunción de verosimilitud: Sean \\(X_1,\\cdots,X_n\\) una muestra aleatoria con función de masa(densidad) de probabilidad \\(f(X;\\theta)\\) y sea \\(L(\\theta,;X_1,\\cdots,X_n)\\) la verosimilitud de la muestra como función de \\(\\theta\\), la cual se representa por:\n\n\n\n\\[L(\\theta;x)=L(\\theta,;X_1,\\cdots,X_n)=f(x_1;\\theta)\\times f(x_2;\\theta)\\times \\cdots f(x_n;\\theta)\\]\n\n\nEl método de máxima verosimilitud busca \\(\\widehat{\\theta}(x_1,\\cdots,x_n)\\) función que depende sólo de la muestra que maximiza \\(L(\\theta;x)\\). Para obtener estimadores máximo verosímiles se utilizan las herramientas de cálculo matemático, además para simplificar los cálculos se utiliza el logaritmo de la verosimilitud, llamada función de logverosimilitud, representado por:\n\\[l(\\theta;x)=\\ln (L(\\theta;x))\\]"
  },
  {
    "objectID": "slides/lec_week3.html#método-de-mínimos-cuadrados",
    "href": "slides/lec_week3.html#método-de-mínimos-cuadrados",
    "title": "Introducción a inferencia estadística",
    "section": "Método de mínimos cuadrados",
    "text": "Método de mínimos cuadrados\nSupongamos que tenemos \\(\\mathbb{E}(Y)=\\alpha X + \\beta\\), donde \\(\\alpha,\\beta\\) y \\(X\\) son tal como en una regresión lineal simple. Sea \\((x_1,Y_1),\\dots,(x_n,Y_n)\\) una muestra aleatoria de \\(Y\\). Los estimadores mínimos cuadrados de los parámetros \\(\\alpha,\\beta\\) son los valores de \\(\\alpha\\) y \\(\\beta\\) que minimizan:\n\\[\\sum_{i=1}^{N} [Y_i - (\\alpha x_i +\\beta)]^2\\]\nPara poder obtener las estimaciones para \\(\\alpha\\) y \\(\\beta\\), procedemos de la siguiente manera:\nSea \\(S(\\alpha,\\beta)=\\sum_{i=1}^{N} [Y_i - (\\alpha x_i +\\beta)]^2\\). Para minimizar \\(S(\\alpha,\\beta)\\), debemos resolver las ecuaciones:\n\\[\\dfrac{\\partial S}{\\partial \\alpha}=0 \\hspace{20pt}\\text{y}\\hspace{20pt}\\dfrac{\\partial S}{\\partial \\beta}=0\\]"
  },
  {
    "objectID": "slides/lec_week3.html#section",
    "href": "slides/lec_week3.html#section",
    "title": "Introducción a inferencia estadística",
    "section": "",
    "text": "Derivando, obtenemos:\n\\[\\dfrac{\\partial S}{\\partial \\alpha}=\\sum_{i=1}^{n}2[Y_i - (\\alpha x_i + \\beta)](-x_i)=-2\\sum_{i=1}^{n}[x_i Y_i - \\alpha x_{i}^{2} - \\beta x_i]\\]\ny,\n\\[\\dfrac{\\partial S}{\\partial \\alpha}=\\sum_{i=1}^{n}2[Y_i - (\\alpha x_i + \\beta)](-1)=-2\\sum_{i=1}^{n}[Y_i - \\alpha x_{i} - \\beta]\\]"
  },
  {
    "objectID": "slides/lec_week3.html#hidemissingtitlestrue",
    "href": "slides/lec_week3.html#hidemissingtitlestrue",
    "title": "Introducción a inferencia estadística",
    "section": "{.hideMissingTitles=true}",
    "text": "{.hideMissingTitles=true}\nDerivando, obtenemos:\n\\[\\dfrac{\\partial S}{\\partial \\alpha}=\\sum_{i=1}^{n}2[Y_i - (\\alpha x_i + \\beta)](-x_i)=-2\\sum_{i=1}^{n}[x_i Y_i - \\alpha x_{i}^{2} - \\beta x_i]\\]\ny,\n\\[\\dfrac{\\partial S}{\\partial \\alpha}=\\sum_{i=1}^{n}2[Y_i - (\\alpha x_i + \\beta)](-1)=-2\\sum_{i=1}^{n}[Y_i - \\alpha x_{i} - \\beta]\\]"
  },
  {
    "objectID": "slides/lec_week3.html#método-de-mínimos-cuadrados-desarrollo",
    "href": "slides/lec_week3.html#método-de-mínimos-cuadrados-desarrollo",
    "title": "Introducción a inferencia estadística",
    "section": "Método de mínimos cuadrados: desarrollo",
    "text": "Método de mínimos cuadrados: desarrollo\nDerivando, obtenemos:\n\\[\\dfrac{\\partial S}{\\partial \\alpha}=\\sum_{i=1}^{n}2[Y_i - (\\alpha x_i + \\beta)](-x_i)=-2\\sum_{i=1}^{n}[x_i Y_i - \\alpha x_{i}^{2} - \\beta x_i]\\]\ny,\n\\[\\dfrac{\\partial S}{\\partial \\beta}=\\sum_{i=1}^{n}2[Y_i - (\\alpha x_i + \\beta)](-1)=-2\\sum_{i=1}^{n}[Y_i - \\alpha x_{i} - \\beta]\\]\nLuego, igualando a cero, se tiene que:\n\\[\\alpha\\sum_{i=1}^{n} x_{i}^{2} + \\beta \\sum_{i=1}^{n} x_i = \\sum_{i=1}^{n} x_i Y_i \\qquad \\text{y,} \\qquad \\alpha \\sum_{i=1}^{n} x_i + n\\beta=\\sum_{i=1}^{n} Y_i\\]"
  },
  {
    "objectID": "slides/lec_week3.html#método-de-mínimos-cuadrados-desarrollo-1",
    "href": "slides/lec_week3.html#método-de-mínimos-cuadrados-desarrollo-1",
    "title": "Introducción a inferencia estadística",
    "section": "Método de mínimos cuadrados: desarrollo",
    "text": "Método de mínimos cuadrados: desarrollo\nTenemos dos ecuaciones lineales y dos incógnitas, por lo que podemos obtener soluciones para \\(\\alpha\\) y \\(\\beta\\), así:\n\\[\\hat{\\alpha}=\\dfrac{\\sum_{i=1}^{n} Y_i (x_i - \\overline{x})}{\\sum_{i=1}^{n} (x_i-\\overline{x})^2}\\hspace{20pt}\\text{donde}\\hspace{20pt}\\overline{x}=\\dfrac{1}{n}\\sum_{i=1}^{n}x_i\\]\n\\[\\hat{\\beta}=\\overline{Y}-\\hat{\\alpha}\\overline{x}\\hspace{20pt}\\text{donde}\\hspace{20pt}\\overline{Y}=\\dfrac{1}{n}\\sum_{i=1}^{n}Y_i\\]\nEstas soluciones siempre se pueden obtener y son únicas si \\(\\sum_{i=1}^{n}(x_i-\\overline{x})^2\\neq 0\\).\nSin embargo, esta condición se satisface cuando no todos los \\(x_i\\) son iguales. En cuanto a la estimación de \\(\\sigma^2\\), esta no puede obtenida mediante este método."
  },
  {
    "objectID": "slides/lec_week3.html#propiedades-de-los-estimadores",
    "href": "slides/lec_week3.html#propiedades-de-los-estimadores",
    "title": "Inferencia estadística",
    "section": "Propiedades de los estimadores:",
    "text": "Propiedades de los estimadores:\nConsideramos una muestra aleatoria, \\(X_1,X_2,\\cdots,X_n\\) y \\(T=T(X_1,X_2,\\cdots,X_n)\\) una función de la muestra, entonces \\(T\\) es llamada un estadística. Cuando una estadística \\(T\\), se utiliza con fines de estimación recibe el nombre de estimador. En general, se desea que los estimadores tengan algunas propiedades especiales.\n\nEstimadores Insesgados: Sea \\(T\\) un estimador (estadística) de un parámetro \\(\\theta\\), se dice que \\(T\\) es un estimador insesgado (o libre de sesgo), si \\(E[T]=\\theta\\), para todos los posibles valores de \\(\\theta\\).\n\n\nEn otras palabras, lo que se desea es que el estimador \\(T\\), en promedio (promediando sobre todas las posibles muestras), sea igual a \\(\\theta\\), “lo que se desea estimar”, bajo la hipótesis que la distribución de probabilidad de la población propuesta es correcta."
  },
  {
    "objectID": "slides/lec_week3.html#propiedades-de-los-estimadores-1",
    "href": "slides/lec_week3.html#propiedades-de-los-estimadores-1",
    "title": "Inferencia estadística",
    "section": "Propiedades de los estimadores:",
    "text": "Propiedades de los estimadores:\nError Cuadrático Medio:\nSea \\(T\\) un estimador de un parámetro \\(\\theta\\), se define el error cuadrático medio de \\(T\\), como el valor esperado del cuadrado de la diferencia entre \\(T\\) y \\(\\theta\\), y se anota \\(ECM(t)\\), esto es:\n\\[ECM(T)=E[(T-\\theta)^2]\\]\nSi de desarrolla la expresión, podemos reescribir lo anterior de la forma:\n\\[ECM(T)=V[T]+(E[T]-\\theta)^2\\]\nEl error cuadrático medio de un estimador \\(T\\), es la suma de dos cantidades no negativas: una es la varianza del estimador, mientras que la otra es el sesgo al cuadrado.\nUn criterio para seleccionar un estimador, es que posea el ECM más pequeño entre los posibles estimadores de \\(\\theta\\)."
  },
  {
    "objectID": "slides/lec_week3.html#propiedades-de-los-estimadores-2",
    "href": "slides/lec_week3.html#propiedades-de-los-estimadores-2",
    "title": "Inferencia estadística",
    "section": "Propiedades de los estimadores: 2",
    "text": "Propiedades de los estimadores: 2\nError Cuadrático Medio:\nSea \\(T\\) un estimador de un parámetro \\(\\theta\\), se define el error cuadrático medio de \\(T\\), como el valor esperado del cuadrado de la diferencia entre \\(T\\) y \\(\\theta\\), y se anota \\(ECM(t)\\), esto es:\n\\[ECM(T)=E[(T-\\theta)^2]\\]\nSi de desarrolla la expresión, podemos reescribir lo anterior de la forma:\n\\[ECM(T)=V[T]+(E[T]-\\theta)^2\\]\nEl error cuadrático medio de un estimador \\(T\\), es la suma de dos cantidades no negativas: una es la varianza del estimador, mientras que la otra es el sesgo al cuadrado.\nUn criterio para seleccionar un estimador, es que posea el ECM más pequeño entre los posibles estimadores de \\(\\theta\\)."
  },
  {
    "objectID": "slides/lec_week3.html#estimadores-insesgados",
    "href": "slides/lec_week3.html#estimadores-insesgados",
    "title": "Introducción a inferencia estadística",
    "section": "Estimadores insesgados",
    "text": "Estimadores insesgados\nConsideramos una muestra aleatoria, \\(X_1,X_2,\\cdots,X_n\\) y \\(T=T(X_1,X_2,\\cdots,X_n)\\) una función de la muestra, entonces \\(T\\) es llamada un estadística. Cuando una estadística \\(T\\), se utiliza con fines de estimación recibe el nombre de estimador. En general, se desea que los estimadores tengan algunas propiedades especiales.\n\nEstimadores Insesgados: Sea \\(T\\) un estimador (estadística) de un parámetro \\(\\theta\\), se dice que \\(T\\) es un estimador insesgado (o libre de sesgo), si \\(E[T]=\\theta\\), para todos los posibles valores de \\(\\theta\\).\n\n\nEn otras palabras, lo que se desea es que el estimador \\(T\\), en promedio (promediando sobre todas las posibles muestras), sea igual a \\(\\theta\\), “lo que se desea estimar”, bajo la hipótesis que la distribución de probabilidad de la población propuesta es correcta."
  },
  {
    "objectID": "slides/lec_week3.html#error-cuadrático-medio",
    "href": "slides/lec_week3.html#error-cuadrático-medio",
    "title": "Introducción a inferencia estadística",
    "section": "Error cuadrático medio",
    "text": "Error cuadrático medio\nSea \\(T\\) un estimador de un parámetro \\(\\theta\\), se define el error cuadrático medio de \\(T\\), como el valor esperado del cuadrado de la diferencia entre \\(T\\) y \\(\\theta\\), y se anota \\(ECM(T)\\), esto es:\n\\[ECM(T)=E[(T-\\theta)^2]\\]\nSi de desarrolla la expresión, podemos reescribir lo anterior de la forma:\n\\[ECM(T)=V[T]+(E[T]-\\theta)^2\\]\nEl error cuadrático medio de un estimador \\(T\\), es la suma de dos cantidades no negativas: una es la varianza del estimador, mientras que la otra es el sesgo al cuadrado.\nUn criterio para seleccionar un estimador, es que posea el ECM más pequeño entre los posibles estimadores de \\(\\theta\\)."
  },
  {
    "objectID": "slides/lec_week3.html#eficiencia-relativa",
    "href": "slides/lec_week3.html#eficiencia-relativa",
    "title": "Introducción a inferencia estadística",
    "section": "Eficiencia relativa",
    "text": "Eficiencia relativa\nSean \\(T_1\\) y \\(T_2\\) dos estimadores de \\(\\theta\\). Se define la eficiencia relativa entre \\(T_1\\) y \\(T_2\\) como:\n\\[Ef(T_1;T_2)=\\dfrac{ECM(T_1)}{ECM(T_2)}\\]\nSi la eficiencia relativa es menor que uno, se concluye que el estimador \\(T_1\\) es más eficiente que el estimador \\(T_2\\), en caso contrario, se concluye que el estimador \\(T_1\\) es más eficiente que el estimador \\(T_2\\)."
  },
  {
    "objectID": "slides/lec_week3.html#consistencia",
    "href": "slides/lec_week3.html#consistencia",
    "title": "Introducción a inferencia estadística",
    "section": "Consistencia",
    "text": "Consistencia\nLa consistencia mide la capacidad del estimador de acercarse cada vez más al verdadero valor del parámetro, a medida que el tamaño de muestra crece.\n\\[ T_n \\overset{p}{\\to} \\theta\\]\n\nConsistencia en media cuadrática:\n\n\nUn estimador \\(T\\), de un parámetro desconocido \\(\\theta\\), se dice consistente en media cuadrática, si se cumple:\n\\[\\lim_{n\\rightarrow\\infty} ECM(T_n)=0\\]"
  },
  {
    "objectID": "slides/lec_week3.html#definición-intervalo-de-confianza",
    "href": "slides/lec_week3.html#definición-intervalo-de-confianza",
    "title": "Introducción a inferencia estadística",
    "section": "Definición intervalo de confianza",
    "text": "Definición intervalo de confianza\nSea \\(X_1,X_2,\\cdots,X_n\\) una muestra aleatoria desde \\(f(x;\\theta)\\), donde \\(f(x;\\theta)\\) es una función de masa (densidad) de probabilidades dependiendo de un parámetro desconocido \\(\\theta\\). Sean \\(T_1\\) y \\(T_2\\) dos estadísticos tales que \\(T_1(x)<T_2(x)\\) para casi todo \\(x\\) y\n\\[\\mathbb{P}(T_1\\leq\\theta \\leq T_2)=\\gamma,\\]\ndonde \\(\\gamma\\) no depende de \\(\\theta\\). Se dice que \\([T_1,T_2]\\) es un intervalo de confianza para \\(\\theta\\) con \\(100\\gamma \\%\\) de confianza.\n\n\\(T_1\\) y \\(T_2\\) reciben el nombre de cota inferior y superior de confianza, respectivamente.\n\\(\\gamma\\) recibe el nombre de coeficiente de confianza.\n\n\\([T_1,T_2]\\) es un intervalo aleatorio, ya que sus extremos son variables aleatorias."
  },
  {
    "objectID": "slides/lec_week3.html#a",
    "href": "slides/lec_week3.html#a",
    "title": "Introducción a inferencia estadística",
    "section": "a",
    "text": "a\nDe la probabilidad del pivote, podemos despejar nuestro parámetro de interés \\(\\mu\\) obteniendo:\n\\[\\mathbb{P}\\left( \\overline{X}-Z_{1-\\alpha/2} \\dfrac{\\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\overline{X}-Z_{\\alpha/2}\\dfrac{\\sigma}{\\sqrt{n}}\\right) =1- \\alpha\\]\nPero como \\(Z_{\\alpha/2}=-Z_{1-\\alpha/2}\\)\n\\[\\mathbb{P}\\left( \\overline{X}-Z_{1-\\alpha/2} \\dfrac{\\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\overline{X}+Z_{1-\\alpha/2}\\dfrac{\\sigma}{\\sqrt{n}}\\right) =1-\\alpha\\]\nCon lo anterior se concluye que el intervalo de \\((1-\\alpha)\\%\\) de confianza para la media poblacional está dado por:\n\\[IC(\\mu):=\\left[\\overline{X}\\mp Z_{1-\\alpha/2}\\dfrac{\\sigma}{\\sqrt{n}}\\right]\\]"
  },
  {
    "objectID": "slides/lec_week3.html#cantidad-pivotal",
    "href": "slides/lec_week3.html#cantidad-pivotal",
    "title": "Introducción a inferencia estadística",
    "section": "Cantidad pivotal",
    "text": "Cantidad pivotal\nExisten técnicas para construir intervalos (regiones) de confianza, y una de ellas es la del pivote.\n\nSea \\(X_1,X_2,\\cdots,X_n\\) una muestra aleatoria \\(n\\) desde \\(f(x;\\theta)\\) y \\(Q=Q(X_1,X_2,\\cdots,X_n)\\). Si la distribución de \\(Q\\) es independiente de \\(\\theta\\), se dice que Q es una cantidad pivotal.\n\n\nEjemplo\nSea \\(X_1,X_2,\\cdots,X_n\\) una muestra aleatoria \\(n\\) desde una familia normal \\(F_{N}(\\mu,\\sigma^2)\\) con media \\(\\mu\\) y varianza conocida \\(\\sigma^2\\), luego:\n\\[Q=\\overline{X}-\\mu \\rightarrow Q \\approx N\\left(0,\\dfrac{\\sigma^2}{n}\\right)\\]"
  },
  {
    "objectID": "slides/lec_week3.html#intervalo-de-confianza-para-la-media-poblacional",
    "href": "slides/lec_week3.html#intervalo-de-confianza-para-la-media-poblacional",
    "title": "Introducción a inferencia estadística",
    "section": "Intervalo de confianza para la media poblacional",
    "text": "Intervalo de confianza para la media poblacional\nSea \\(X_1,X_2,\\cdots,X_n\\) una muestra aleatoria \\(n\\) de una familia normal \\(F_{N}(\\mu,\\sigma^2)\\), como \\(\\overline{X}\\) es el mejor estimador de \\(\\mu\\), entonces si se conoce \\(\\sigma^2\\), se tiene que:\n\\[Z=\\dfrac{(\\overline{X}-\\mu)\\sqrt{n}}{\\sigma} \\approx N(0,1) \\Rightarrow Z \\text{ pivote}\\]\nLuego dado \\(\\gamma\\), se requiere determinar los valores más apropiados de \\(q_1\\) y \\(q_2\\) que cumplan con:\n\\[\\mathbb{P}\\left(q_1 \\leq \\dfrac{(\\overline{X}-\\mu)\\sqrt{n}}{\\sigma} \\leq q_2\\right)=\\gamma\\]\n\nSe desea minimizar la longitud del intervalo de confianza, los valores \\(q_1\\) y \\(q_2\\) deben ser aquellos que produzcan igualdad de probabilidades en las colas."
  },
  {
    "objectID": "slides/lec_week3.html#desarrollo-intervalo-de-confianza",
    "href": "slides/lec_week3.html#desarrollo-intervalo-de-confianza",
    "title": "Introducción a inferencia estadística",
    "section": "Desarrollo intervalo de confianza",
    "text": "Desarrollo intervalo de confianza\nEsto es:\n\\[q_2=Z_{\\dfrac{1+\\gamma}{2}} \\hspace{30pt} q_1=-q_2\\]\nLuego, si tomamos \\(\\alpha=1-\\gamma\\), se tiene:\n\\[\\mathbb{P}\\left( Z_{\\alpha /2} \\leq \\dfrac{(\\overline{X}-\\mu)\\sqrt{n}}{\\sigma} \\leq Z_{1-\\alpha/2} \\right)=1-\\alpha\\]"
  },
  {
    "objectID": "slides/lec_week3.html#b",
    "href": "slides/lec_week3.html#b",
    "title": "Introducción a inferencia estadística",
    "section": "b",
    "text": "b\nSi se tiene una muestra aleatoria \\(n\\) \\(X_1,X_2,\\cdots,X_n\\) tal que \\(X_i \\approx N(\\mu,\\sigma^2)\\), con varianza poblacional \\(\\sigma^2\\) desconocida, como sabemos que \\(S^2\\) es el mejor estimador de \\(\\sigma^2\\), luego se tiene:\n\\[T=\\dfrac{(\\overline{X}-\\mu)\\sqrt{n}}{s} \\approx \\mathcal{T}(n-1) \\rightarrow T \\text{ pivote}\\]\nEn donde \\(\\mathcal{T}\\) es la distribución t-student con \\((n-1)\\) grados de libertad. Análogamente, podemos construir el Intervalo de confianza para \\(\\mu\\) utilizando esta distribución, obteniéndose:\n\\[IC(\\mu):=\\left[\\overline{X}\\mp t_{1-\\alpha/2}(n-1)\\dfrac{s}{\\sqrt{n}}\\right]\\]"
  },
  {
    "objectID": "slides/lec_week3.html#i.c.-para-la-media-con-varianza-población-conocida",
    "href": "slides/lec_week3.html#i.c.-para-la-media-con-varianza-población-conocida",
    "title": "Introducción a inferencia estadística",
    "section": "I.C. para la media con varianza población conocida",
    "text": "I.C. para la media con varianza población conocida\nDe la probabilidad del pivote, podemos despejar nuestro parámetro de interés \\(\\mu\\) obteniendo:\n\\[\\mathbb{P}\\left( \\overline{X}-Z_{1-\\alpha/2} \\dfrac{\\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\overline{X}-Z_{\\alpha/2}\\dfrac{\\sigma}{\\sqrt{n}}\\right) =1- \\alpha\\]\nPero como \\(Z_{\\alpha/2}=-Z_{1-\\alpha/2}\\)\n\\[\\mathbb{P}\\left( \\overline{X}-Z_{1-\\alpha/2} \\dfrac{\\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\overline{X}+Z_{1-\\alpha/2}\\dfrac{\\sigma}{\\sqrt{n}}\\right) =1-\\alpha\\]\nCon lo anterior se concluye que el intervalo de \\((1-\\alpha)\\%\\) de confianza para la media poblacional está dado por:\n\\[IC(\\mu):=\\left[\\overline{X}\\mp Z_{1-\\alpha/2}\\dfrac{\\sigma}{\\sqrt{n}}\\right]\\]"
  },
  {
    "objectID": "slides/lec_week3.html#i.c.-para-la-media-con-varianza-población-desconocida",
    "href": "slides/lec_week3.html#i.c.-para-la-media-con-varianza-población-desconocida",
    "title": "Introducción a inferencia estadística",
    "section": "I.C. para la media con varianza población desconocida",
    "text": "I.C. para la media con varianza población desconocida\nSi se tiene una muestra aleatoria de tamaño \\(n\\), \\(X_1,X_2,\\cdots,X_n\\) tal que \\(X_i \\approx N(\\mu,\\sigma^2)\\), con varianza poblacional \\(\\sigma^2\\) desconocida, como sabemos que \\(S^2\\) es el mejor estimador de \\(\\sigma^2\\), se tiene:\n\\[T=\\dfrac{(\\overline{X}-\\mu)\\sqrt{n}}{s} \\approx \\mathcal{T}(n-1) \\Rightarrow T \\text{ pivote}\\]\nEn donde \\(\\mathcal{T}\\) es la distribución t-student con \\((n-1)\\) grados de libertad. Análogamente, podemos construir el intervalo de confianza para \\(\\mu\\) utilizando esta distribución, obteniéndose:\n\\[IC(\\mu):=\\left[\\overline{X}\\mp t_{1-\\alpha/2}(n-1)\\dfrac{s}{\\sqrt{n}}\\right]\\]"
  },
  {
    "objectID": "slides/lec_week3.html#i.c.-para-la-media-con-tamaño-de-muestra-grande",
    "href": "slides/lec_week3.html#i.c.-para-la-media-con-tamaño-de-muestra-grande",
    "title": "Introducción a inferencia estadística",
    "section": "I.C. para la media con tamaño de muestra grande",
    "text": "I.C. para la media con tamaño de muestra grande\nSi el tamaño de muestra es muy grande (mayor que 50), utilizando el teorema de límite central, el intervalo de confianza toma la siguiente forma:\n\\[IC(\\mu):=\\left[\\overline{X}\\mp Z_{1-\\alpha/2}\\dfrac{s}{\\sqrt{n}}\\right]\\]\nNotamos que es importante distinguir cuando la varianza poblacional es conocida o desconocida. Si a partir de la muestra aleatoria se determina una varianza, ésta es la muestral, por lo tanto, lo correcto es utilizar un intervalo de confianza considerando la distribución t-student, caso contrario si la muestra es superior a 50, entonces empleamos el teorema de límite central para aproximar por distribución normal."
  },
  {
    "objectID": "slides/lec_week3.html#intervalos-de-confianza-para-una-proporción",
    "href": "slides/lec_week3.html#intervalos-de-confianza-para-una-proporción",
    "title": "Introducción a inferencia estadística",
    "section": "Intervalos de confianza para una proporción",
    "text": "Intervalos de confianza para una proporción\nSea \\(X_1,X_2,\\cdots,X_n\\) una muestra aleatoria de tamaño \\(n\\) de una familia binomial \\(\\mathcal{B} (1,p)\\). El estimador de \\(p\\) sobre la base de la muestra es \\(\\widehat{P}=\\overline{X}\\). La distribución de \\(\\widehat{P}=\\overline{X}\\), para muestras grandes, se puede aproximar mediante una distribución normal de parámetros \\(p\\) y \\(\\dfrac{p(1-p)}{n}\\). Con esto podemos aproximar la siguiente cantidad pivotal:\n\\[Z=\\dfrac{(\\widehat{P}-p)}{\\sqrt{\\dfrac{\\widehat{P}(1-\\widehat{P})}{n}}} \\approx N(0,1) \\Rightarrow Z \\text{ pivote}\\]\nLuego dado \\((1-\\alpha)\\), los valores de \\(q_1\\) y \\(q_2\\) que minimizan la longitud del intervalo serán:\n\\[\\mathbb{P}\\left(  \\widehat{P}-Z_{1-\\alpha /2} \\sqrt{\\dfrac{\\widehat{P}(1-\\widehat{P})}{n}} \\leq p \\leq \\widehat{P}+Z_{1-\\alpha /2} \\sqrt{\\dfrac{\\widehat{P}(1-\\widehat{P}) }{n} } \\right)=\\gamma \\]"
  },
  {
    "objectID": "slides/lec_week3.html#i.c.-final-para-una-proporción",
    "href": "slides/lec_week3.html#i.c.-final-para-una-proporción",
    "title": "Introducción a inferencia estadística",
    "section": "I.C. final para una proporción",
    "text": "I.C. final para una proporción\nLuego, el intervalo de confianza, del \\((100*\\gamma)\\%\\) para la proporción es:\n\\[IC(p):=\\left[ \\widehat{P}\\mp Z_{1-\\alpha/2}\\sqrt{\\dfrac{\\widehat{P}(1-\\widehat{P})}{n}}\\right] \\]\nSe puede apreciar que los intervalos de confianza anteriores están compuestos por un estimador puntual, más o menos una cantidad, ésta cantidad recibe el nombre de error de estimación, que resultará útil para determinar el tamaños de muestra."
  },
  {
    "objectID": "slides/lec_week3.html#intervalos-de-confianza-para-la-varianza-poblacional",
    "href": "slides/lec_week3.html#intervalos-de-confianza-para-la-varianza-poblacional",
    "title": "Introducción a inferencia estadística",
    "section": "Intervalos de confianza para la varianza poblacional",
    "text": "Intervalos de confianza para la varianza poblacional\nSea \\(X_1,X_2,\\dots,X_n\\) una muestra aleatoria de tamaño \\(n\\) desde una familia normal \\(F_N(\\mu,\\sigma^2)\\). Existen dos posibilidades para la estimación de la varianza, cuando la media población es conocida (caso no práctico) y cuando ésta es desconocida. Para ambos casos podemos definir cantidades pivotales:\n\n\\(\\dfrac{n S_{n}^{2}}{\\sigma^2} \\sim \\chi^2(n)\\)\n\n\\(\\dfrac{(n-1) S_{n-1}^{2}}{\\sigma^2} \\sim \\chi^2(n-1)\\)\n\n\nen donde:\n\\[S_{n}^{2}=\\sum_{i=1}^{n} \\dfrac{(X_i - \\mu)^2}{n},\\qquad S_{n-1}^{2}=\\sum_{i=1}^{n} \\dfrac{(X_i - \\overline{X})^2}{n-1}\\]"
  },
  {
    "objectID": "slides/lec_week3.html#i.c.-final-para-la-varianza-poblacional",
    "href": "slides/lec_week3.html#i.c.-final-para-la-varianza-poblacional",
    "title": "Introducción a inferencia estadística",
    "section": "I.C. final para la varianza poblacional",
    "text": "I.C. final para la varianza poblacional\nSiguiendo el mismo procedimiento para la cantidad pivotal, en particular, el caso donde la media poblacional es desconocida. Se tiene:\n\\[\\mathbb{P}\\left[ \\chi_{\\alpha/2}^{2}(n-1) \\leq \\dfrac{(n-1) S_{n-1}^{2}}{\\sigma^2} \\leq \\chi_{1-\\alpha/2}^{2}(n-1) \\right]=1-\\alpha\\]\nLuego, despejando el parámetro de interés \\(\\sigma^2\\), podemos definir un intervalo de \\((1-\\alpha)\\%\\) de confianza para la varianza poblacional:\n\\[IC(\\sigma^2)=\\left[ \\dfrac{(n-1)S_{n-1}^{2}}{\\chi_{1-\\alpha/2}^{2}(n-1)};\\dfrac{(n-1)S_{n-1}^{2}}{\\chi_{\\alpha/2}^{2}(n-1)}\\right]\\]"
  },
  {
    "objectID": "slides/lec_week3.html#test-de-hipótesis-1",
    "href": "slides/lec_week3.html#test-de-hipótesis-1",
    "title": "Introducción a inferencia estadística",
    "section": "Test de hipótesis:",
    "text": "Test de hipótesis:\nPara llegar a tomar decisiones, conviene hacer determinados supuestos o conjeturas acerca de las poblaciones que se estudian. Tales supuestos que pueden ser o no ciertos y se llaman hipótesis estadísticas y, en general, lo son sobre las distribuciones de probabilidad de las poblaciones.\nEn muchos casos se formulan las hipótesis estadísticas con el sólo propósito de rechazarlas o invalidarlas. Cualquier hipótesis que difiera de una hipótesis dada se llama hipótesis alternativa. Denotaremos por \\(H_0\\) a nuestro supuesto o hipótesis nula, y \\(H_1\\) a nuestra hipótesis alternativa."
  },
  {
    "objectID": "slides/lec_week3.html#tipos-de-error",
    "href": "slides/lec_week3.html#tipos-de-error",
    "title": "Introducción a inferencia estadística",
    "section": "Tipos de error",
    "text": "Tipos de error\nAl realizar nuestra prueba de hipótesis estamos sujetos al estado real de la naturaleza, es decir, la veracidad de nuestra conjetura (\\(H_0\\))\n\n\n\n\n\n\n\n\n\n\n\nEstado Real\nen la naturaleza\n\n\n\n\n\n\n\\(H_0\\) es Verdadera\n\\(H_0\\) es Falsa\n\n\nDecisión\nNo se rechaza \\(H_0\\)\nDecisión Correcta\nError Tipo II\n\n\n\nSe rechaza \\(H_0\\)\nError Tipo I\nDecisión Correcta\n\n\n\nPodemos cometer dos tipos de errores, tipo I y tipo II.\n\nError tipo I: Se comete al rechazar la hipótesis nula, cuando corresponde aceptarla por ser ésta verdadera. Lo denotamos por \\(\\alpha\\) y es llamado nivel de significación.\nError tipo II: Se comete al no rechazar la hipótesis nula, cuando corresponde rechazarla por ser esta falsa. Lo denotamos por \\(\\beta\\).\n\n\nEl error tipo I es fundamental debido a que es el error que el experimentador controla y pueda manejar."
  },
  {
    "objectID": "slides/lec_week3.html#prueba-para-la-media-poblacional",
    "href": "slides/lec_week3.html#prueba-para-la-media-poblacional",
    "title": "Introducción a inferencia estadística",
    "section": "Prueba para la media poblacional",
    "text": "Prueba para la media poblacional\nSea \\(X_1,X_2,\\dots,X_n\\) una muestra aleatoria de una distribución normal con media \\(\\mu\\) desconocida. En este caso el interés recae en probar uno de los siguientes conjuntos de hipotesis con respecto a \\(\\mu\\).\n\\[H_0:\\mu = \\mu_0 \\qquad H_0:\\mu= \\mu_0 \\qquad H_0:\\mu = \\mu_0 \\\\H_1:\\mu \\neq \\mu_0 \\qquad H_1:\\mu > \\mu_0 \\qquad H_1:\\mu < \\mu_0\\]\nSupongamos primero que la varianza poblacional \\(\\sigma^2\\) es conocida. Utilizando la estadística de prueba \\(\\overline{X}\\), bajo \\(H_0\\) se tiene que \\(\\overline{X}\\sim N\\left(\\mu_0,{\\sigma^2 \\over n}\\right)\\). La región crítica de tamaño \\(\\alpha\\) para la hipótesis bilateral es de la forma:\n\\[\\text{Rechazar }H_0 \\text{ si}\\begin{cases} \\overline{X}\\geq \\overline{x}_{1-\\alpha /2}\\\\ \\overline{X}\\leq \\overline{x}_{\\alpha /2} \\end{cases}\\]"
  },
  {
    "objectID": "slides/lec_week3.html#prueba-para-la-media-poblacional-desarrollo",
    "href": "slides/lec_week3.html#prueba-para-la-media-poblacional-desarrollo",
    "title": "Introducción a inferencia estadística",
    "section": "Prueba para la media poblacional: desarrollo",
    "text": "Prueba para la media poblacional: desarrollo\nEn donde \\(\\overline{x}_{1-\\alpha /2}\\) y, \\(\\overline{x}_{\\alpha /2}\\) son los valores cuantiles críticos de \\(\\overline{X}\\) de manera tal que:\n\\[\\mathbb{P}(\\overline{X}\\geq  \\overline{x}_{1-\\alpha /2})= \\alpha /2 \\hspace{20pt}\\text{y}\\hspace{20pt} \\mathbb{P}(\\overline{X}\\geq  \\overline{x}_{\\alpha /2})= \\alpha /2\\]\nDado que bajo \\(H_0, \\overline{X}\\sim N(\\mu_0,{\\sigma^2 \\over n})\\), entonces de forma equivalente:\n\\[\\mathbb{P}\\left( Z \\geq \\underbrace{\\dfrac{\\overline{x}_{1-\\alpha /2}-\\mu_0}{\\sigma / \\sqrt{n}}}_{z_{1-\\alpha /2}}\\right)=\\alpha /2\\hspace{20pt}\\text{y}\\hspace{20pt}\\mathbb{P}\\left( Z \\leq \\underbrace{\\dfrac{\\overline{x}_{\\alpha /2}-\\mu_0}{\\sigma / \\sqrt{n}}}_{z_{\\alpha /2}}\\right)=\\alpha /2\\]\nPor lo que, \\(H_0\\) debe rechazarse cuando un valor de \\(\\overline{x}\\) de la media muestral \\(\\overline{X}\\) es tal que:\n\\[\\overline{x} \\geq \\dfrac{\\sigma z_{1-\\alpha /2}}{\\sqrt{n}}+\\mu_0\\hspace{20pt}\\text{o}\\hspace{20pt}\\overline{x} \\leq \\dfrac{\\sigma z_{\\alpha /2}}{\\sqrt{n}}+\\mu_0\\]"
  },
  {
    "objectID": "slides/lec_week3.html#prueba-para-la-media-poblacional-regiones-de-rechazo",
    "href": "slides/lec_week3.html#prueba-para-la-media-poblacional-regiones-de-rechazo",
    "title": "Introducción a inferencia estadística",
    "section": "Prueba para la media poblacional: regiones de rechazo",
    "text": "Prueba para la media poblacional: regiones de rechazo\nDe manera equivalente, se rechazará \\(H_0\\) cuando,\n\\[z\\geq z_{1-\\alpha /2}\\hspace{20pt}\\text{o}\\hspace{20pt}z\\leq z_{\\alpha /2}\\]\nDonde \\(z=\\dfrac{\\overline{x}-\\mu_0}{\\sigma / \\sqrt{n}}\\) es el valor de la correspondiente normal estándar al valor \\(\\overline{x}\\) de \\(\\overline{X}\\).\nPara la hipótesis alternativa unilateral, \\(H_1: \\mu > \\mu_0\\), la región crítica de tamaño \\(\\alpha\\) es el extremo derecho de la distribución de muestreo de \\(\\overline{X}\\), ésta es de la forma:\n\\[\\text{Rechazar } H_0  \\text{ si } \\overline{X} \\geq \\overline{x}_{1-\\alpha}\\]\nDe forma similar, para la hipótesis alternativa unilateral \\(H_1:\\mu < \\mu_0\\), la región crítica es de la forma:\n\\[\\text{Rechazar } H_0 \\text{ si } \\overline{X} \\leq \\overline{x}_{1-\\alpha}\\]"
  },
  {
    "objectID": "slides/lec_week3.html#observaciones",
    "href": "slides/lec_week3.html#observaciones",
    "title": "Introducción a inferencia estadística",
    "section": "Observaciones",
    "text": "Observaciones\nNotar que lo anterior, sólo fue posible debido a que sabíamos \\(\\sigma^2\\). En caso de no conocerlo, si utilizamos la misma estadística de prueba \\(\\overline{X}\\), se tiene que:\n\\[T=\\dfrac{\\overline{X}-\\mu_0}{S / \\sqrt{n}} \\sim t(n-1)\\]\nPor lo que siguiendo el mismo procedimiento que antes, podemos llegar a regiones críticas similares."
  },
  {
    "objectID": "slides/lec_week3.html#pruebas-para-la-varianza-poblacional",
    "href": "slides/lec_week3.html#pruebas-para-la-varianza-poblacional",
    "title": "Introducción a inferencia estadística",
    "section": "Pruebas para la varianza poblacional",
    "text": "Pruebas para la varianza poblacional\nSea \\(X_1,\\dots,X_n\\) una muestra aleatoria de una distribución normal con media \\(\\mu\\) desconocida y varianza \\(\\sigma^2\\). Se considera el siguiente test de hipótesis:\n\\[H_0:\\sigma^2=\\sigma_{0}^{2}\\]\ncontra una de las siguientes alternativas:\n\\[H_1:\\sigma^2\\neq \\sigma_{0}^{2},\\hspace{5pt}H_1:\\sigma^2> \\sigma_{0}^{2},\\hspace{5pt}H_1:\\sigma^2< \\sigma_{0}^{2}\\]\ndonde \\(\\sigma_{0}^{2}\\) es el valor propuesto para \\(\\sigma^2\\). La estadística de interés es la varianza muestral \\(S^2\\). La hipótesis nula será rechazada si la realización de \\(s^2\\) calculada a partir de la muestra, es suficientemente diferente, mayor que o menos que \\(\\sigma_{0}^{2}\\), dependiendo de la hipótesis alternativa. Bajo \\(H_0\\):\n\\[\\dfrac{(n-1)s^2}{\\sigma_{0}^{2}}\\sim  \\chi^2(n-1)\\]"
  },
  {
    "objectID": "slides/lec_week3.html#pruebas-para-la-proporción-poblacional",
    "href": "slides/lec_week3.html#pruebas-para-la-proporción-poblacional",
    "title": "Introducción a inferencia estadística",
    "section": "Pruebas para la proporción poblacional",
    "text": "Pruebas para la proporción poblacional\nSea \\(X_1,\\dots,X_n\\) una muestra aleatoria de una distribución Bernoulli \\(Ber(1,p)\\). Consideramos el siguiente test de hipótesis:\n\\[H_0:p=p_0\\]\ncontra una de las siguientes alternativas:\n\\[H_1:p\\neq p_0,\\hspace{5pt}H_1:p> p_0,\\hspace{5pt}H_1:p<p_0\\]\ndonde \\(p_0\\) es el valor propuesto para \\(p\\). La estadística de interés bajo \\(H_0\\) es:\n\\[E=\\dfrac{\\hat{p}-p_0}{\\sqrt{\\dfrac{p_0(1-p_0)}{n}}}\\sim N(0,1)\\]\nPara \\(n>>50\\) y \\(\\hat{p}=\\sum_{i=1}^{n}X_i/n\\)"
  },
  {
    "objectID": "slides/lec_week3.html#t",
    "href": "slides/lec_week3.html#t",
    "title": "Introducción a inferencia estadística",
    "section": "t",
    "text": "t\n ## Pruebas para la varianza poblacional\nSea \\(X_1,\\dots,X_n\\) una muestra aleatoria de una distribución normal con media \\(\\mu\\) desconocida y varianza \\(\\sigma^2\\). Se considera el siguiente test de hipótesis:\n\\[H_0:\\sigma^2=\\sigma_{0}^{2}\\]\ncontra una de las siguientes alternativas:\n\\[H_1:\\sigma^2\\neq \\sigma_{0}^{2},\\hspace{5pt}H_1:\\sigma^2> \\sigma_{0}^{2},\\hspace{5pt}H_1:\\sigma^2< \\sigma_{0}^{2}\\]\ndonde \\(\\sigma_{0}^{2}\\) es el valor propuesto para \\(\\sigma^2\\). La estadística de interés es la varianza muestral \\(S^2\\). La hipótesis nula será rechazada si la realización de \\(s^2\\) calculada a partir de la muestra, es suficientemente diferente, mayor que o menos que \\(\\sigma_{0}^{2}\\), dependiendo de la hipótesis alternativa. Bajo \\(H_0\\):\n\\[\\dfrac{(n-1)s^2}{\\sigma_{0}^{2}}\\sim  \\chi^2(n-1)\\]"
  },
  {
    "objectID": "slides/lec_week3.html#tabla-resumen-i.c.-para-la-media-con-varianza-conocida",
    "href": "slides/lec_week3.html#tabla-resumen-i.c.-para-la-media-con-varianza-conocida",
    "title": "Introducción a inferencia estadística",
    "section": "Tabla resumen I.C. para la media con varianza conocida",
    "text": "Tabla resumen I.C. para la media con varianza conocida\nLo anterior puede ser resumido en:"
  },
  {
    "objectID": "slides/lec_week3.html#tabla-resumen-i.c.-para-la-media-con-varianza-desconocida",
    "href": "slides/lec_week3.html#tabla-resumen-i.c.-para-la-media-con-varianza-desconocida",
    "title": "Introducción a inferencia estadística",
    "section": "Tabla resumen I.C. para la media con varianza desconocida",
    "text": "Tabla resumen I.C. para la media con varianza desconocida\nLo anterior puede ser resumido en:"
  },
  {
    "objectID": "slides/lec_week3.html#tabla-resumen-pruebas-para-la-media-con-varianza-conocida",
    "href": "slides/lec_week3.html#tabla-resumen-pruebas-para-la-media-con-varianza-conocida",
    "title": "Introducción a inferencia estadística",
    "section": "Tabla resumen pruebas para la media con varianza conocida",
    "text": "Tabla resumen pruebas para la media con varianza conocida\nLo anterior puede ser resumido en:"
  },
  {
    "objectID": "slides/lec_week3.html#tabla-resumen-pruebas-para-la-media-con-varianza-desconocida",
    "href": "slides/lec_week3.html#tabla-resumen-pruebas-para-la-media-con-varianza-desconocida",
    "title": "Introducción a inferencia estadística",
    "section": "Tabla resumen pruebas para la media con varianza desconocida",
    "text": "Tabla resumen pruebas para la media con varianza desconocida\nLo anterior puede ser resumido en:"
  },
  {
    "objectID": "slides/lec_week3.html#tabla-resumen-pruebas-para-la-varianza",
    "href": "slides/lec_week3.html#tabla-resumen-pruebas-para-la-varianza",
    "title": "Introducción a inferencia estadística",
    "section": "Tabla resumen pruebas para la varianza",
    "text": "Tabla resumen pruebas para la varianza\nAsí, conforme la misma construcción realizada anteriormente, es posible encontrar las criterios de rechazo, en resumen:"
  },
  {
    "objectID": "slides/lec_week3.html#tabla-resumen-pruebas-para-la-proporción",
    "href": "slides/lec_week3.html#tabla-resumen-pruebas-para-la-proporción",
    "title": "Introducción a inferencia estadística",
    "section": "Tabla resumen pruebas para la proporción",
    "text": "Tabla resumen pruebas para la proporción\nAsí, conforme la misma construcción realizada anteriormente, es posible enconrtar los criterios de rechazo, en resumen:"
  },
  {
    "objectID": "slides/lec_week3.html#definición-formal",
    "href": "slides/lec_week3.html#definición-formal",
    "title": "Introducción a inferencia estadística",
    "section": "Definición formal",
    "text": "Definición formal\nUn modelo de regresión básico donde sólo hay una variable predictora y la función de regresión es lineal se define como:\n\\[Y_i=\\beta_0+\\beta_1 X_i + \\varepsilon_i\\]\ndonde,\n\n\\(Y_i\\) es el valor de la varible respuesta en la i-ésima observación\n\\(\\beta_0\\) y \\(\\beta_1\\) son parámetros\n\\(X_i\\) es una constante conocida: el valor de la variable predictora en la i-ésima observación.\n\\(\\varepsilon_i\\) es un término de error aleatorio con meadia \\(\\mathbb{E}(\\varepsilon_i)=0\\) y varianza \\(\\mathbb{V}(\\varepsilon_i)=\\sigma^2\\)\n\\(\\varepsilon_i\\) y \\(\\varepsilon_j\\) no están correlacionados, por lo que su covarianza es cero.\n\n\nEsto modelo se le conoce como modelo de regresión lineal simple."
  },
  {
    "objectID": "slides/lec_week3.html#características-importantes-del-modelo-de-regresión-lineal-simple",
    "href": "slides/lec_week3.html#características-importantes-del-modelo-de-regresión-lineal-simple",
    "title": "Introducción a inferencia estadística",
    "section": "Características importantes del modelo de regresión lineal simple",
    "text": "Características importantes del modelo de regresión lineal simple\nLa respuesta \\(Y_i\\) en la i-ésimo ensayo es la suma de dos componentes: - El término constante \\(\\beta_0+\\beta_1 X_i\\) y, - El término aleatorio \\(\\varepsilon_i\\). Por lo que \\(Y_i\\) es una variable aleatoria\nDebido a que \\(\\mathbb{E}(\\varepsilon_i)=0\\), sigue que:\n\\[\\mathbb{E}(Y_i)=\\mathbb{E}(\\beta_0+\\beta_1 X_i + \\varepsilon_i)=\\beta_0+\\beta_1 X_i + \\mathbb{E}(\\varepsilon_i)= \\beta_0+\\beta_1 X_i\\]\nAsí, la respuesta \\(Y_i\\), cuando el nivel de \\(X\\) en el i-ésimo ensayo es \\(X_i\\), viene desde una distribución de probabilidad cuya media está dada por:\n\\[\\mathbb{E}(Y_i)=\\beta_0+\\beta_1 X_i\\]\nSabremos que la función de regresión para este modelo es \\(\\mathbb{E}(Y)=\\beta_0+\\beta_1 X\\)\nDebido a que la función de regresión relaciona la media de la distribución de probabilidad de \\(Y\\) para un \\(X\\) dado para el nivel de \\(X\\)."
  },
  {
    "objectID": "slides/lec_week3.html#características-importantes-del-modelo-de-regresión-lineal-simple-continuación",
    "href": "slides/lec_week3.html#características-importantes-del-modelo-de-regresión-lineal-simple-continuación",
    "title": "Introducción a inferencia estadística",
    "section": "Características importantes del modelo de regresión lineal simple: continuación",
    "text": "Características importantes del modelo de regresión lineal simple: continuación\nLa respuesta \\(Y_i\\) en el i-ésimo ensayo excede o queda bajo el valor de la función de regresión por la cantidad del término \\(\\varepsilon_i\\). Además, Los errores \\(\\varepsilon_i\\) se asumen que tienen varianza constante \\(\\sigma^2\\), por lo que la variable respuesta \\(Y_i\\) tiene la misma varianza constante.\nAsí, el modelo de regresión lineal simple asume que la distribución de probabilidad de \\(Y\\) tiene la misma varianza \\(\\sigma^2\\), independiente del nivel de la variable predictora \\(X\\).\nLos errores se asumen independientes. Debido a que los términos \\(\\varepsilon_i\\) y \\(\\varepsilon_j\\) no están correlacionados, también no lo estarán las respuestas \\(Y_i\\) e \\(Y_j\\).\nEn resumen, el modelo de regresión lineal simple implica que la respuesta \\(Y_i\\) viene desde una distribución de probabilidad cuyas medias son \\(\\mathbb{E}(Y_i)=\\beta_0+\\beta_1 X_i\\) y cuyas varianzas son \\(\\sigma^2\\), para todos los niveles de \\(X\\). Además, dos respuestas distintas \\(Y_i\\) e \\(Y_j\\) no están correlacionadas."
  },
  {
    "objectID": "slides/lec_week3.html#interpretación-de-los-parámetros-de-regresión",
    "href": "slides/lec_week3.html#interpretación-de-los-parámetros-de-regresión",
    "title": "Introducción a inferencia estadística",
    "section": "Interpretación de los parámetros de regresión",
    "text": "Interpretación de los parámetros de regresión\nLos parámetros de regresión \\(\\beta_0\\) y \\(\\beta_1\\) en un modelo de regresión lineal simple son llamados coeficientes de regresión, siendo \\(\\beta_1\\) la pendiente y \\(\\beta_0\\) el intercepto. El primero indica el cambio en la media de la distribución de probabilidad de \\(Y\\) por el incremento unitario en \\(X\\).\nCuando el alcance del modelo incluye \\(X=0\\), \\(\\beta_0\\) entrega la media de la distribución de probabilidad de \\(Y\\) en \\(X=0\\). Cuando el alcance del modelo no incluye \\(X=0\\), \\(\\beta_0\\) no tienen ninguna interpretación particular como termino separado en la regresión."
  },
  {
    "objectID": "slides/lec_week3.html#teorema-de-gauss-markov",
    "href": "slides/lec_week3.html#teorema-de-gauss-markov",
    "title": "Introducción a inferencia estadística",
    "section": "Teorema de Gauss-Markov",
    "text": "Teorema de Gauss-Markov\n\nBajo las condiciones de un modelo de regresión lineal simple, los estimadores de mínimos cuadrados \\(b_0\\) y \\(b_1\\) son estimadores insesgados y tienen mínima varianza entre los estimadores insesgados lineales.\n\nEste teorema establece que \\(b_0\\) y \\(b_1\\) son estimadores insesgados, por lo que:\n\\[\\mathbb{E}(b_0)=\\beta_0 \\hspace{40pt} \\mathbb{E}(b_1)=\\beta_1\\]\nPor lo que ninguno de estos estimadores tiende a sobrestimar o subestimar sistemáticamente. Segundo, el teorema establece que los estimadores \\(b_0\\) y \\(b_1\\) son más precisos (esto es, su distribución muestral es menos variable) que cualquier otro estimador perteneciente a la clase de estimadores insesgados que son funciones lineales de las observaciones \\(Y_1,\\dots,Y_n\\).\nLos estimadores \\(b_0\\) y \\(b_1\\) son funciones lineal de \\(Y_i\\)."
  },
  {
    "objectID": "slides/lec_week3.html#propiedades-del-ajuste-de-regresión",
    "href": "slides/lec_week3.html#propiedades-del-ajuste-de-regresión",
    "title": "Introducción a inferencia estadística",
    "section": "Propiedades del ajuste de regresión",
    "text": "Propiedades del ajuste de regresión\nEl ajuste de regresión lineal al usar el método de mínimos cuadrados tiene un número de propiedades que valen la pena mencionar. Estas propiedades de los estimadores de mínimos cuadrados de una función de regresión no aplican para todos los modelos de regresión.\n\nLa suma de los residuos es cero: \\(\\sum_{i=1}^{n} e_i = 0\\)\nLa suma de los valores observados \\(Y_i\\) es igual a la suma de los valores ajustados \\(\\widehat{Y}_i\\):\n\n\n\\[\\sum_{i=1}^{n} Y_i = \\sum_{i=1}^{n} \\widehat{Y}_i\\]\n\n\nDe esto último, se desprende que la media de los valores ajustados \\(\\widehat{Y}_i\\) es la misma que la media de los valores observados \\(Y_i\\)."
  },
  {
    "objectID": "slides/lec_week3.html#propiedades-del-ajuste-de-regresión-continuación",
    "href": "slides/lec_week3.html#propiedades-del-ajuste-de-regresión-continuación",
    "title": "Introducción a inferencia estadística",
    "section": "Propiedades del ajuste de regresión: continuación",
    "text": "Propiedades del ajuste de regresión: continuación\n\nLa suma de los residuos ponderados es cero cuando el i-ésimo residuo es ponderado con el nivel de la variable predictora i-ésima, esto es:\n\n\n\\[\\sum_{i=1}^{n} X_i e_i = 0\\]\n\nUna consecuencia de las propiedades 1 y 3, es que la suma de los pesos ponderados es cero cuando el i-ésimo residuo es ponderado con el valor ajustado de la i-ésima variable respuesta, esto es:\n\n\n\n\\[\\sum_{i=1}^{n} \\widehat{Y}_i e_i = 0\\]\n\nLa recta de regresión siempre pasa por el punto \\((\\overline{X},\\overline{Y})\\)."
  },
  {
    "objectID": "slides/lec_week4.html#inferencia-sobre-la-pendiente",
    "href": "slides/lec_week4.html#inferencia-sobre-la-pendiente",
    "title": "Regresión lineal",
    "section": "Inferencia sobre la pendiente",
    "text": "Inferencia sobre la pendiente\nFrecuentemente es de particular interés la inferencia sobre el parámetro de la pendiente de regresión, pues nos entrega una noción de cambio medio por unidad en la variable regresora. Un tipo de test relevante en este contexto es:\n\\[H_0: \\beta_1=0 \\hspace{20pt} H_1: \\beta_1\\ne 0\\]\nEste test de hipótesis es relevante debido a que cuando \\(\\beta_1=0\\), no existe una asociación lineal entre las variables \\(X\\) e \\(Y\\).\nEn el caso de que el término de error en el modelo de regresión sea normal, la condición de que \\(\\beta_1=0\\) implica aún más cosas. Debido a que en este modelo todas las distribución de probabilidades de \\(Y\\) son normales con varianza constante, y que las medias son iguales cuando \\(\\beta_1=0\\), sigue que las distribuciones de probabilidad de \\(Y\\) son idénticas cuando \\(\\beta_1=0\\).\n\nAsí, \\(\\beta_1=0\\) para el modelo de regresión lineal normal implica que no sólo no existe relación lineal entre \\(X\\) e \\(Y\\), pero además no existe ningún tipo de relación entre \\(Y\\) y \\(X\\), dado que las distribuciones de probabilidad de \\(Y\\) son idénticas para todos los niveles de \\(X\\)."
  },
  {
    "objectID": "slides/lec_week4.html#distribución-muestral-de-b_1",
    "href": "slides/lec_week4.html#distribución-muestral-de-b_1",
    "title": "Regresión lineal",
    "section": "Distribución muestral de \\(b_1\\)",
    "text": "Distribución muestral de \\(b_1\\)\nPor lo visto antes, sabemos que el estimador puntual de \\(b_1\\) está dado por:\n\\[b_1=\\dfrac{\\sum (X_i-\\overline{X})(Y_i - \\overline{Y})}{\\sum (X_i-\\overline{X})^2}\\]\nLa distribución muestral de \\(b_1\\) hace referencia a los diferentes valores de \\(b_1\\) que serían obtenidos con un muestreo repetido cuando los niveles de la variable predictora \\(X\\) se mantiene constante entre las diferentes muestras. Para el modelo de regresión normal, la distribución muestral de \\(b_1\\) es normal con media y varianza dada por:\n\\[\\mathbb{E}(b_1)=\\beta_1\\qquad \\qquad\\mathbb{V}(b_1)=\\dfrac{\\sigma^2}{\\sum (X_i-\\overline{X})^2}\\]\nPara mostrar esto, debemos identificar que \\(b_1\\) es una combinación lineal de las observaciones \\(Y_i\\)."
  },
  {
    "objectID": "slides/lec_week4.html#b_1-como-combinación-lineal-de-y_i",
    "href": "slides/lec_week4.html#b_1-como-combinación-lineal-de-y_i",
    "title": "Regresión lineal",
    "section": "\\(b_1\\) como combinación lineal de \\(Y_i\\)",
    "text": "\\(b_1\\) como combinación lineal de \\(Y_i\\)\nSe puede mostrar que \\(b_1\\) puede ser reescrito como:\n\\[b_1=\\sum k_i Y_i\\]\ndonde,\n\\[k_i=\\dfrac{X_i-\\overline{X}}{\\sum (X_i-\\overline{X})^2}\\]\nNotamos que los \\(k_i\\) son funciones de \\(X_i\\) y por lo tanto son cantidades fijas, ya que los \\(X_i\\) son conocidos. Los coeficientes \\(k_i\\) tienen propiedades interesantes que usaremos más adelante: \\[\\begin{align*}\n\\sum k_i&=0 \\\\\n\\sum k_i X_i &= 1 \\\\\n\\sum k_{i}^{2}&=\\dfrac{1}{\\sum (X_i-\\overline{X})^2}\n\\end{align*}\\]\nLeer detalles de este cálculo página 42, Applied Lineal Statistical Models 5th Edition, Kutner et al."
  },
  {
    "objectID": "slides/lec_week4.html#normalidad-media-y-varianza",
    "href": "slides/lec_week4.html#normalidad-media-y-varianza",
    "title": "Regresión lineal",
    "section": "Normalidad, media y varianza",
    "text": "Normalidad, media y varianza\nDebido a que el término \\(b_1\\) es una combinación lineal de \\(Y_i\\), y este último son variables aleatoria normales independientes, sigue que \\(b_1\\) también lo es.\nLa insesgadez del estimador puntual de \\(b_1\\) es debido al teorema de Gauss-Markov, sigue que:\n\\[\\begin{align*}\n\\mathbb{E}(b_1)&=\\mathbb{E}\\left(\\sum k_i Y_i\\right)=\\sum k_i \\mathbb{E}(Y_i)= \\sum k_i(\\beta_0+\\beta_1 X_i)\\\\\n&= \\beta_0 \\sum k_i + \\beta_1 \\sum k_i X_i = \\beta_1\n\\end{align*}\\] En cuanto a la varianza de \\(b_1\\), sólo necesitamos recordar que \\(Y_i\\) son variables aleatorias independientes, cada una con varianza \\(\\sigma^2\\) y que \\(k_i\\) son constantes. Por lo que: \\[\\begin{align*}\n\\mathbb{V}(b_1)&=\\mathbb{V}\\left(\\sum k_i Y_i\\right)=\\sum k_{i}^{2} \\mathbb{V}(Y_i)\\\\\n&=\\sum k_{i}^{2} \\sigma^2=\\sigma^2 \\sum k_{i}^{2}\\\\\n&= \\dfrac{\\sigma^2}{\\sum (X_i -\\overline{X})^2}\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/lec_week4.html#varianza-estimada",
    "href": "slides/lec_week4.html#varianza-estimada",
    "title": "Regresión lineal",
    "section": "Varianza estimada",
    "text": "Varianza estimada\nPodemos estimar la varianza de la distribución muestral de \\(b_1\\):\n\\[\\mathbb{V}(b_1)=\\dfrac{\\sigma^2}{\\sum (X_i - \\overline{X})^2}\\]\nReemplazando el parámetro \\(\\sigma^2\\) con el ECM, el estimador insesgado de \\(\\sigma^2\\):\n\\[\\widehat{\\mathbb{V}(b_1)}=\\dfrac{MSE}{\\sum (X_i - \\overline{X})^2}\\]\nEsta estimación puntual es un estimador insesgada de \\(\\mathbb{V}(b_1)\\). Tomando la raíz cuadrado podemos obtener la estimación puntual para la desviación estándar."
  },
  {
    "objectID": "slides/lec_week4.html#distribución-muestral-útil",
    "href": "slides/lec_week4.html#distribución-muestral-útil",
    "title": "Regresión lineal",
    "section": "Distribución muestral útil",
    "text": "Distribución muestral útil\nCon vistas en obtener intervalos de confianza para los parámetros de regresión, necesitamos obtener las distribuciones muestrales de cantidades pivotales, entre ellas la cantidad:\n\\[(b_1-\\beta_1)/\\sqrt{\\widehat{\\mathbb{V}(b_1)}}\\]\nDebido a que \\(b_1\\) está distribuido normalmente, sabemos que la estandarización:\n\\[\\dfrac{(b_1-\\beta_1)}{\\sqrt{\\mathbb{V}(b_1)}}\\]\nes una variable aleatoria normal estándar. En la práctica, no se tiene acceso a la varianza teórica por lo que esta cantidad debe ser estimada por \\(\\widehat{\\mathbb{V}(b_1)}\\) por que estamos particularmente interesados en la distribución de \\((b_1-\\beta_1)/\\sqrt{\\widehat{\\mathbb{V}(b_1)}}\\)"
  },
  {
    "objectID": "slides/lec_week4.html#distribución-muestral-útil-continuación",
    "href": "slides/lec_week4.html#distribución-muestral-útil-continuación",
    "title": "Regresión lineal",
    "section": "Distribución muestral útil: continuación",
    "text": "Distribución muestral útil: continuación\nCuando una estadístico está estandarizado pero el denominador es una estimación de la desviación estándar en vez de su valor real, se le llama estadístico estudentizado. Un teorema importante en estadística establece que el estadístico:\n\\[\\dfrac{(b_1-\\beta_1)}{\\sqrt{\\widehat{\\mathbb{V}(b_1)}}}\\sim t(n-2)\\]\nPara el modelo de regresión que estamos estudiando. Esto viene del hecho que \\(SSE/\\sigma^2 \\sim \\chi^2(n-2)\\) y es independiente de \\(b_0\\) y \\(b_1\\)."
  },
  {
    "objectID": "slides/lec_week4.html#intervalo-de-confianza-para-la-pendiente",
    "href": "slides/lec_week4.html#intervalo-de-confianza-para-la-pendiente",
    "title": "Regresión lineal",
    "section": "Intervalo de confianza para la pendiente",
    "text": "Intervalo de confianza para la pendiente\nDebido a que esta cantidad sigue una distribución t-student, podemos establecer que:\n\\[\\mathbb{P}(t(\\alpha/2,n-2)\\leq (b_1-\\beta_1)/\\sqrt{\\widehat{\\mathbb{V}(b_1)}} \\leq  t(1-\\alpha/2,n-2))=1-\\alpha\\]\nLuego, operando de igual manera que en la construcción de intervalos de confianza usual (vía pivote). Podemos llegar a un intervalo de confianza para \\(\\beta_1\\):\n\\[\\left[ b_1 \\pm t(1-\\alpha/2, n-2) \\sqrt{\\widehat{\\mathbb{V}(b_1)}}\\right]\\]"
  },
  {
    "objectID": "slides/lec_week4.html#test-de-hipótesis-para-la-pendiente",
    "href": "slides/lec_week4.html#test-de-hipótesis-para-la-pendiente",
    "title": "Regresión lineal",
    "section": "Test de hipótesis para la pendiente",
    "text": "Test de hipótesis para la pendiente\nDebido a que:\n\\[\\dfrac{(b_1-\\beta_1)}{\\sqrt{\\widehat{\\mathbb{V}(b_1)}}}\\sim t(n-2)\\]\nToda la teoría de test de hipótesis usuales es válida (tests unilaterales y bilaterales).\nTenemos particular interés en un test del tipo:\n\\[H_0: \\beta_1 = 0 \\hspace{20pt} H_1:\\beta_1 \\neq 0\\]\nPues con ello probamos si existe una asociación lineal entre las variables del modelo bajo un cierto nivel de confianza."
  },
  {
    "objectID": "slides/lec_week4.html#inferencia-sobre-el-intercepto",
    "href": "slides/lec_week4.html#inferencia-sobre-el-intercepto",
    "title": "Regresión lineal",
    "section": "Inferencia sobre el intercepto",
    "text": "Inferencia sobre el intercepto\nComo lo mencionamos antes, rara vez tendremos interés en hacer inferencia sobre el parámetro \\(\\beta_0\\), y estos son sólo válidos cuando el rango de la variable predictora incluye \\(X=0\\).\nComo hemos visto antes la estimación puntal del intercepto está dado por:\n\\[b_0=\\overline{Y}-b_1\\overline{X}\\]\nPara el modelo de regresión en estudio, la distribución muestral de \\(b_0\\) es normal, con media y varianza:\n\\[\\mathbb{E}(b_0)=\\beta_0\\qquad \\qquad \\mathbb{V}(b_0)=\\sigma^2\\left[ \\dfrac{1}{n}+\\dfrac{\\overline{X}^2}{\\sum (X_i-\\overline{X})^2}\\right]\\]\nLa normalidad es obtenida debido a que \\(b_0\\) al igual que \\(b_1\\), es una combinación lineal de observaciones \\(Y_i\\). Al igual que antes, una estimador de la varianza viene dado al reemplazar \\(\\sigma^2\\) por su estimación puntual (ECM). El estimador de la desviación estándar es obtenido aplicando raíz cuadrada."
  },
  {
    "objectID": "slides/lec_week4.html#intervalo-de-confianza-para-el-intercepto",
    "href": "slides/lec_week4.html#intervalo-de-confianza-para-el-intercepto",
    "title": "Regresión lineal",
    "section": "Intervalo de confianza para el intercepto",
    "text": "Intervalo de confianza para el intercepto\nAl igual que antes, se tiene que:\n\\[\\dfrac{b_0-\\beta_0}{\\sqrt{\\widehat{\\mathbb{V}(b_0)}}}\\sim t(n-2)\\]\npara este modelo de regresión. Así, los intervalos de confianza pueden ser construidos al igual que para \\(\\beta_1\\). Esto es:\n\\[\\left[ b_0 \\pm t(1-\\alpha/2, n-1)\\sqrt{\\widehat{\\mathbb{V}(b_0)}}\\right]\\]"
  },
  {
    "objectID": "slides/lec_week4.html#análisis-de-varianza-para-análisis-de-regresión",
    "href": "slides/lec_week4.html#análisis-de-varianza-para-análisis-de-regresión",
    "title": "Regresión lineal",
    "section": "Análisis de Varianza para análisis de regresión",
    "text": "Análisis de Varianza para análisis de regresión\nCon lo anterior, ya hemos visto gran parte de la teoría de un modelo de regresión básico. En lo que sigue, estudiaremos el análisis de regresión desde la perspectiva de análisis de varianza.\nNociones básicas: El enfoque desde el análisis de varianza se base en particionar la suma de cuadrado y grados de libertad asociados con la variable respuesta \\(Y\\). Identificaremos 3 términos que usaremos frecuentemente:\n\nSuma de cuadrados total (SSTO): \\(\\sum (Y_i - \\overline{Y})^2\\)\nSuma de los cuadrados del error (SSE): \\(\\sum (Y_i - \\hat{Y}_i)^2\\)\nSuma de los cuadrados de la regresión (SSR): \\(\\sum (\\hat{Y}_i-\\overline{Y})^2\\)\n\n\nen donde se tiene la relación:\n\\[SSTO=SSE+SSR\\]"
  },
  {
    "objectID": "slides/lec_week4.html#desglose-de-los-grados-de-libertad",
    "href": "slides/lec_week4.html#desglose-de-los-grados-de-libertad",
    "title": "Regresión lineal",
    "section": "Desglose de los grados de libertad",
    "text": "Desglose de los grados de libertad\nAl igual que para la varianza, podemos desglosar los grados de libertad. Es claro ver que:\n\nSSTO tiene asociado \\(n-1\\) grados de libertad, debido a que estimamos la media poblacional.\nSSE tiene asociado \\(n-2\\) grados de libertad, debido a que para obtener \\(\\hat{Y}_i\\) debemos estimar \\(\\beta_0\\) y \\(\\beta_1\\)\nSSR tiene asociado \\(1\\) grado de libertad debido a que los valores ajustados son calculados a partir de la recta de regresión, por lo que \\(2\\) grados de libertad están a asociado a esta, pero uno de ello es perdido debido a la estimación \\(\\overline{Y}\\).\n\n\nAsí, se tiene que:\n\\[n-1=1+(n-2)\\]"
  },
  {
    "objectID": "slides/lec_week4.html#cuadrados-medios",
    "href": "slides/lec_week4.html#cuadrados-medios",
    "title": "Regresión lineal",
    "section": "Cuadrados medios",
    "text": "Cuadrados medios\nLlamamos cuadrados medios a las sumas cuadradas divididas por sus grados de libertad respectivos. Por lo que tenemos:\n\nError cuadrático medio: \\(\\dfrac{SSE}{n-2}\\)\nCuadrado medio de regresión: \\(\\dfrac{SSR}{1}\\)\n\n\nEn este caso, los cuadrados medios no son aditivos"
  },
  {
    "objectID": "slides/lec_week4.html#tabla-anova",
    "href": "slides/lec_week4.html#tabla-anova",
    "title": "Regresión lineal",
    "section": "Tabla ANOVA",
    "text": "Tabla ANOVA\nLo que hemos visto anteriormente, puede ser resumido en la tabla ANOVA usual, en donde se incorporó además la esperanza de los cuadrados medios.\n\n\n\n\n\n\n\n\n\n\nF.V.\nSS\ng.l.\nMS\n\\(\\mathbf{\\mathbb{E}(MS)}\\)\n\n\n\n\nRegresión\n\\(SSR = \\sum (\\hat{Y}_i-\\overline{Y})^2\\)\n\\(1\\)\n\\(MSR=SSR\\)\n\\(\\sigma^2+\\beta_{1}^{2}\\sum (X_i-\\overline{X})^2\\)\n\n\nError\n\\(SSE = \\sum (Y_i - \\hat{Y}_i)^2\\)\n\\(n-2\\)\n\\(MSE=\\dfrac{SSE}{n-2}\\)\n\\(\\sigma^2\\)\n\n\nTotal\n\\(SSTO=\\sum (Y_i - \\overline{Y})^2\\)\n\\(n-1\\)"
  },
  {
    "objectID": "slides/lec_week4.html#test-f",
    "href": "slides/lec_week4.html#test-f",
    "title": "Regresión lineal",
    "section": "Test F",
    "text": "Test F\nEl enfoque de análisis de varianza nos permite realizar fácilmente test para modelos de regresión (y otros modelos lineales). Por ejemplo, consideremos:\n\\[H_0: \\beta_1 = 0 \\hspace{20pt} H_1:\\beta_1 \\neq 0\\]\nEstadístico de prueba\nBajo este enfoque consideramos el estadístico \\(F^*\\), definido como:\n\\[F^*=\\dfrac{MSR}{MSE}\\]\nDistribución muestral de \\(F^*\\)\nEs posible mostrar que bajo \\(H_0\\), \\(F^*\\) sigue una distribución \\(F(1,n-2)\\)"
  },
  {
    "objectID": "slides/lec_week4.html#introducción",
    "href": "slides/lec_week4.html#introducción",
    "title": "Regresión lineal",
    "section": "Introducción",
    "text": "Introducción\nCon lo anterior, ya hemos visto gran parte de la teoría de un modelo de regresión básico. En lo que sigue, estudiaremos el análisis de regresión desde la perspectiva de análisis de varianza.\nNociones básicas: El enfoque desde el análisis de varianza se base en particionar la suma de cuadrado y grados de libertad asociados con la variable respuesta \\(Y\\). Identificaremos 3 términos que usaremos frecuentemente:\n\nSuma de cuadrados total (SSTO): \\(\\sum (Y_i - \\overline{Y})^2\\)\nSuma de los cuadrados del error (SSE): \\(\\sum (Y_i - \\hat{Y}_i)^2\\)\nSuma de los cuadrados de la regresión (SSR): \\(\\sum (\\hat{Y}_i-\\overline{Y})^2\\)\n\n\nen donde se tiene la relación:\n\\[SSTO=SSE+SSR\\]"
  },
  {
    "objectID": "slides/lec_week4.html#test-f-continuación",
    "href": "slides/lec_week4.html#test-f-continuación",
    "title": "Regresión lineal",
    "section": "Test F: continuación",
    "text": "Test F: continuación\nRegla de decisión\nDebido a que \\(F^*\\) sigue una distribución \\(F(1,n-2)\\) bajo \\(H_0\\), la regla de decisión será:\n\nSi \\(F^* \\leq F(1-\\alpha; 1,n-2)\\), optamos por \\(H_0\\)\nSi \\(F^* > F(1-\\alpha; 1,n-2)\\), optamos por \\(H_1\\)"
  },
  {
    "objectID": "slides/lec_week4.html#coeficiente-de-determinación",
    "href": "slides/lec_week4.html#coeficiente-de-determinación",
    "title": "Regresión lineal",
    "section": "Coeficiente de determinación",
    "text": "Coeficiente de determinación\nEl coeficiente de determinación lo definimos como:\n\\[R^2=\\dfrac{SSR}{SSTO}=1-\\dfrac{SSE}{SSTO}\\]\ny lo interpretamos como la proporción de la variabilidad que es explicada por el ajuste de regresión lineal.\nEste coeficiente se mueve entre 0 y 1, siendo 1 un ajuste perfecto. Un buen ajuste de regresión suele estar entre 0.7 - 0.9, pero esto puede variar dependiendo del contexto del problema."
  },
  {
    "objectID": "slides/lec_week4.html#limitaciones-del-coeficiente-de-determinación",
    "href": "slides/lec_week4.html#limitaciones-del-coeficiente-de-determinación",
    "title": "Regresión lineal",
    "section": "Limitaciones del coeficiente de determinación",
    "text": "Limitaciones del coeficiente de determinación\n\nUn coeficiente de determinación alto no indica que se puedan hacer predicciones buenas\nUn coeficiente de determinación alto no indica que el ajuste es necesariamente bueno\nUn coeficiente de determinación cercano a cero no indica que \\(X\\) e \\(Y\\) no estén relacionados."
  },
  {
    "objectID": "slides/lec_week4.html#coeficiente-de-correlación",
    "href": "slides/lec_week4.html#coeficiente-de-correlación",
    "title": "Regresión lineal",
    "section": "Coeficiente de correlación",
    "text": "Coeficiente de correlación\nEste coeficiente puede ser definido como la raíz del coeficiente de determinación.\n\\[r=\\pm \\sqrt{R^2}\\]\ny lo interpretamos como el coeficiente de correlación de Pearson."
  },
  {
    "objectID": "slides/lec_week4.html#aplicación-computacional",
    "href": "slides/lec_week4.html#aplicación-computacional",
    "title": "Regresión lineal",
    "section": "Aplicación computacional",
    "text": "Aplicación computacional\n\nrequire(tidyverse)\nrequire(MASS)\nrequire(car)\nrequire(mosaic)\nset.seed(163)\ndata(UScereal)\nplot<-ggplot(UScereal,aes(x=fibre,y=calories)) + geom_point() +\n  geom_smooth(method=lm,se=FALSE,color=\"red\")"
  },
  {
    "objectID": "slides/lec_week4.html#aplicación-computacional-continuación",
    "href": "slides/lec_week4.html#aplicación-computacional-continuación",
    "title": "Regresión lineal",
    "section": "Aplicación computacional: continuación",
    "text": "Aplicación computacional: continuación\n\nmodel <- lm(calories~fibre,data=UScereal)\nsummary(model)\n\n\nCall:\nlm(formula = calories ~ fibre, data = UScereal)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-144.73  -28.07  -17.48   15.51  258.48 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  134.117      8.522  15.738   <2e-16 ***\nfibre          3.950      1.181   3.344   0.0014 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 57.97 on 63 degrees of freedom\nMultiple R-squared:  0.1507,    Adjusted R-squared:  0.1372 \nF-statistic: 11.18 on 1 and 63 DF,  p-value: 0.001396"
  },
  {
    "objectID": "slides/lec_week4.html#aplicación-computacional-continuación-1",
    "href": "slides/lec_week4.html#aplicación-computacional-continuación-1",
    "title": "Regresión lineal",
    "section": "Aplicación computacional: continuación",
    "text": "Aplicación computacional: continuación\n\nconfint(model)\n\n                 2.5 %    97.5 %\n(Intercept) 117.087793 151.14595\nfibre         1.589422   6.31138\n\nanova(model)\n\nAnalysis of Variance Table\n\nResponse: calories\n          Df Sum Sq Mean Sq F value   Pr(>F)   \nfibre      1  37572   37572   11.18 0.001396 **\nResiduals 63 211723    3361                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/lec_week4.html#diagnóstico",
    "href": "slides/lec_week4.html#diagnóstico",
    "title": "Regresión lineal",
    "section": "Diagnóstico",
    "text": "Diagnóstico\nCuando realizamos un modelo de regresión, como por ejemplo el modelo de regresión lineal simple antes visto, frecuentemente no podemos estar seguros por adelantado si el modelo es apropiado para aplicación que se le desea dar. Muchas de las características del modelo, tales como la linealidad de la función de regresión o normalidad de los errores podría no ser apropiada, por lo que toma relevancia saber si el modelo puede ser aplicado.\nEn lo que sigue estudiaremos métodos gráficos y test formales, para saber si un modelo es apropiado usarlo. Nos concentramos en el modelo de regresión lineal simple, pero los mismos principios son válidos para todos los modelos estadísticos que veremos."
  },
  {
    "objectID": "slides/lec_week4.html#diagnóstico-para-las-variables-predictoras",
    "href": "slides/lec_week4.html#diagnóstico-para-las-variables-predictoras",
    "title": "Regresión lineal",
    "section": "Diagnóstico para las variables predictoras",
    "text": "Diagnóstico para las variables predictoras\nPrimero debemos analizar las variables predictora para detectar la presencia de datos anómalos o outliers, que puedan influenciar la viabilidad del modelo.\n\nLa presencia de outliers, puede provocar residuos grandes en magnitud, influenciando enormemente el ajuste de regresión."
  },
  {
    "objectID": "slides/lec_week4.html#diagnóstico-para-residuos",
    "href": "slides/lec_week4.html#diagnóstico-para-residuos",
    "title": "Regresión lineal",
    "section": "Diagnóstico para residuos",
    "text": "Diagnóstico para residuos\nEn general, los gráficos de diagnósticos utilizando directamente la variable respuesta \\(Y\\) no son muy útiles en el análisis de regresión debido a que el valor de las observaciones en la variable respuesta son una función del nivel de la variable predictora. Por lo que usualmente, se analizan indirectamente mediante la inspección de los residuos.\nLos residuos \\(e_i\\) son la diferencia entre el valor observado \\(Y_i\\) y el valor ajustado \\(\\hat{Y}_i\\):\n\\[e_i=Y_i-\\hat{Y}_i\\]\nEstos pueden ser considerados como el error observado, a diferencia de valor real del error \\(\\varepsilon_i\\) en el modelo de regresión:\n\\[\\varepsilon_i=Y_i - \\mathbb{E}(Y_i)\\]\nPara el modelo de regresión lineal simple, los errores \\(\\varepsilon_i\\) se asumen variables aleatorias normales independientes, con media 0 y varianza constante \\(\\sigma^2\\). Si el modelo es apropiado para los datos disponibles, el residuo observado \\(e_i\\) deben reflejar las propiedades que se asumieron para \\(\\varepsilon_i\\).\nEsta es la idea básica del análisis de residuos, una herramienta útil para evaluar la viabilidades de los modelos."
  },
  {
    "objectID": "slides/lec_week4.html#propiedades-de-los-residuos-media",
    "href": "slides/lec_week4.html#propiedades-de-los-residuos-media",
    "title": "Regresión lineal",
    "section": "Propiedades de los residuos: media",
    "text": "Propiedades de los residuos: media\nLa media de los \\(n\\) residuos \\(e_i\\) para el modelo de regresión lineal simple es:\n\\[\\overline{e}=\\dfrac{\\sum e_i}{n}=0\\]\ndonde \\(\\overline{e}\\) denota la media de los residuos. Así, debido a que \\(\\overline{e}\\) es siempre 0, este no provee información sobre si los errores reales \\(\\varepsilon_i\\) tienen valor esperado \\(\\mathbb{E}(\\varepsilon_i)=0\\)."
  },
  {
    "objectID": "slides/lec_week4.html#propiedades-de-los-residuos-varianza",
    "href": "slides/lec_week4.html#propiedades-de-los-residuos-varianza",
    "title": "Regresión lineal",
    "section": "Propiedades de los residuos: varianza",
    "text": "Propiedades de los residuos: varianza\nLa varianza de los \\(n\\) residuos \\(e_i\\) está definida como:\n\\[s^2=\\dfrac{\\sum (e_i - \\overline{e})^2}{n-2}=\\dfrac{\\sum e_{i}^{2}}{n-2}=\\dfrac{SSE}{n-2}=MSE\\]\nSi el modelo es apropiado, el error cuadrático medio es un estimador insesgado de la varianza del error \\(\\sigma^2\\)."
  },
  {
    "objectID": "slides/lec_week4.html#propiedades-de-los-residuos-no-independencia",
    "href": "slides/lec_week4.html#propiedades-de-los-residuos-no-independencia",
    "title": "Regresión lineal",
    "section": "Propiedades de los residuos: no independencia",
    "text": "Propiedades de los residuos: no independencia\nLos residuos \\(e_i\\) no son variables aleatorias independientes debido a que involucran los valores ajustados \\(\\hat{Y}_i\\), los cuales están basado en la misma función de regresión ajustada. Como resultado de lo anterior, los residuos para el modelo de regresión están sujetos a dos restricciones:\n\nLa suma de \\(e_i\\) debe ser 0\nla suma de \\(X_i e_i\\) debe ser 0\n\n\nCuando el tamaño de muestra es grande en comparación con el número de parámetros en el modelo de regresión, la efecto de dependencia entre los residuos \\(e_i\\) no tiene mayor importancia y puede ser ignorado."
  },
  {
    "objectID": "slides/lec_week4.html#propiedades-de-los-residuos-residuos-semi-studentizados",
    "href": "slides/lec_week4.html#propiedades-de-los-residuos-residuos-semi-studentizados",
    "title": "Regresión lineal",
    "section": "Propiedades de los residuos: residuos semi-studentizados",
    "text": "Propiedades de los residuos: residuos semi-studentizados\nFrecuentemente, sirve estandarizar los residuos para realizar el análisis. debido a que la desviación estándar de los términos de error \\(\\varepsilon_i\\) es \\(\\sigma\\), el cual puede ser estimado mediante \\(\\sqrt{MSE}\\), por lo que es natural considerar la estandarización:\n\\[e_{i}^{*}=\\dfrac{e_i-\\overline{e}}{\\sqrt{MSE}}=\\dfrac{e_i}{\\sqrt{MSE}}\\]\nSi \\(\\sqrt{MSE}\\) fuese una estimación de la desviación estándar de los residuos \\(e_i\\), llamaríamos \\(e_{i}^{*}\\) residuos studentizados. Sin embargo, la desviación estándar de \\(e_i\\) es compleja y varía para los diferentes residuos \\(e_i\\), y \\(\\sqrt{MSE}\\) es sólo una aproximación de la desviación estándar de \\(e_i\\).\nPor lo que llamamos el estadístico \\(e_{i}^{*}\\) un residuo semi-studentizado. Estos tipo de residuos nos sirven para identificar la presencia de datos anómalos."
  },
  {
    "objectID": "slides/lec_week4.html#diferencias-con-el-modelo-estudiado",
    "href": "slides/lec_week4.html#diferencias-con-el-modelo-estudiado",
    "title": "Regresión lineal",
    "section": "Diferencias con el modelo estudiado",
    "text": "Diferencias con el modelo estudiado\nUsualmente, estaremos en busca de 6 formas en la cuales un modelo de regresión lineal simple con errores normales no es adecuado.\n\nLa función de regresión no es lineal\nLos errores no tienen varianza constante\nLos errores no son independientes\nEl modelo ajusta todas las observaciones exceptuando algunas\nLos errores no se distribuyen de manera normal\nUnas o varias variables predictoras fueron omitidas del modelo"
  },
  {
    "objectID": "slides/lec_week4.html#diagnóstico-de-los-residuos",
    "href": "slides/lec_week4.html#diagnóstico-de-los-residuos",
    "title": "Regresión lineal",
    "section": "Diagnóstico de los residuos",
    "text": "Diagnóstico de los residuos\nUtilizaremos varios gráficos para identificar si ocurre alguna de las 6 situaciones antes planteadas. Los siguientes gráficos son usualmente usados para este fin\n\nGráficos de los residuos vs la variable predictora\nGráfico del valor absoluto o el cuadrado de los residuos vs la variable predictora\nGráfico de los residuos vs valores ajustados\nGráfico de los residuos vs tiempo u otra secuencia\nGráfico de los residuos vs variables predictoras omitidas\nBox-Plot de los residuos\nGráfico de probabilidad normal de los residuos"
  },
  {
    "objectID": "slides/lec_week4.html#test-relacionados-con-los-residuos",
    "href": "slides/lec_week4.html#test-relacionados-con-los-residuos",
    "title": "Regresión lineal",
    "section": "Test relacionados con los residuos",
    "text": "Test relacionados con los residuos\nEl análisis de residuos mediante gráficos es inherentemente subjetivo. Aún así, este análisis subjetivo de una variedad de gráficos de residuos frecuentemente revela dificultades en la implementación del modelo más claramente que un test formal.\n\nTest de aleatoriedad: Durbin-Watson Test\nTest para la consistencia de varianza: Brown-Forsythe test y Breusch-Pagan test\nTest de normalidad: Test Chi-cuadrado, Kolmogorov-Smirnov, Lilliefors test."
  },
  {
    "objectID": "slides/lec_week4.html#medidas-correctivas",
    "href": "slides/lec_week4.html#medidas-correctivas",
    "title": "Regresión lineal",
    "section": "Medidas correctivas",
    "text": "Medidas correctivas\nSi el modelo de regresión lineal simple no es apropiado para el conjunto de datos que se está analizando, se tienen dos opciones:\n\nAbandonar el modelo de regresión lineal simple y desarrollar otro modelo\nAplicar alguna transformación a los datos tal que el modelo de regresión lineal simple sea apropiado para los datos transformados."
  },
  {
    "objectID": "slides/lec_week4.html#modelo-de-regresión-lineal-general-forma-equivalente",
    "href": "slides/lec_week4.html#modelo-de-regresión-lineal-general-forma-equivalente",
    "title": "Regresión lineal",
    "section": "Modelo de regresión lineal general: forma equivalente",
    "text": "Modelo de regresión lineal general: forma equivalente\nSi consideramos \\(X_{i0}=1\\), el modelo de regresión anterior puede reescrito como:\n\\[Y_i=\\beta_0 X_{i0}+\\beta_1 X_{i1}+\\beta_2 X_{i2} + \\cdots + \\beta_{p-1}X_{i,p-1}+\\varepsilon_i\\]\npor lo que,\n\\[Y_i=\\sum_{k=0}^{p-1} \\beta_k X_{ik}+\\varepsilon_i\\]\nLa respuesta media para este modelo de regresión está dado por:\n\\[\\mathbb{E}(Y)=\\beta_0+\\beta_1 X_1 + \\beta_2 X_2 + \\cdots+\\beta_{p-1} X_{p-1}\\]\ndebido a que \\(\\mathbb{E}(\\varepsilon_i)=0\\)\nAsí, el modelo de regresión lineal general con errores normales implica que las observaciones \\(Y_i\\) son variables aleatorias normales, con media \\(\\mathbb{E}(Y_i)\\) dado por la expresión anterior y con varianza constante \\(\\sigma^2\\)."
  },
  {
    "objectID": "slides/lec_week4.html#variables-predictoras-cualitativas",
    "href": "slides/lec_week4.html#variables-predictoras-cualitativas",
    "title": "Regresión lineal",
    "section": "Variables predictoras cualitativas",
    "text": "Variables predictoras cualitativas\nEl modelo de regresión lineal general abarca no sólo variables predictoras cuantitativas, sino también variables cualitativas. Estas se conocen como variables indicadoras que toman los valores 0 y 1 para identificar las clases de la variable cualitativa.\nEjemplo\nConsideramos el siguiente análisis de regresión para predecir el largo de la estadía en un hospital \\((Y)\\) basado en la edad \\((X_1)\\) y género \\((X_2)\\) del paciente. Definimos \\(X_2\\) como:\n\\[X_2=\\begin{cases}1 \\hspace{20pt} \\text{si el paciente es mujer}\\\\  0 \\hspace{20pt} \\text{si el paciente es hombre}\\end{cases}\\]\nEl modelo de regresión lineal de primer order estará dado por:\n\\[Y_i=\\beta_0+\\beta_1 X_{i1} + \\beta_2 X_{i2}+\\varepsilon_i\\]\ndonde:\n\\[\\begin{align*}\nX_{i1}&= \\text{ Edad del paciente}\\\\\nX_{i2}&=\\begin{cases}1 \\hspace{20pt} \\text{si el paciente es mujer}\\\\  0 \\hspace{20pt} \\text{si el paciente es hombre}\\end{cases}\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/lec_week4.html#ejemplo-1",
    "href": "slides/lec_week4.html#ejemplo-1",
    "title": "Regresión lineal",
    "section": "Ejemplo",
    "text": "Ejemplo\nEn este caso, la función de respuesta estará dada por:\n\\[\\mathbb{E}(Y)=\\beta_0+\\beta_1 X_1 +\\beta_2 X_2\\]\nPara los pacientes hombres, \\(X_2=0\\) y la respuesta media será:\n\\[\\mathbb{E}(Y)=\\beta_0+\\beta_1 X_1\\]\nY para los pacientes mujeres, \\(X_2=1\\) y la respuesta media será:\n\\[\\mathbb{E}(Y)=(\\beta_0+\\beta_2)+\\beta_1 X_1\\]\n\nEn general, representamos una variable cualitativa con \\(c\\) clases mediante \\(c-1\\) variables indicadoras."
  },
  {
    "objectID": "slides/lec_week4.html#ejemplo-continuación",
    "href": "slides/lec_week4.html#ejemplo-continuación",
    "title": "Regresión lineal",
    "section": "Ejemplo: continuación",
    "text": "Ejemplo: continuación\nPor ejemplo, si en el ejemplo anterior se agrega una variable cualitativa que representa el estado de discapacidad. Podemos agregar dos variable indicadoras \\(X_3\\) y \\(X_4\\) como:\n\\[X_3=\\begin{cases}1 \\hspace{20pt} \\text{si el paciente no es discapacitado}\\\\  0 \\hspace{20pt} \\text{en otro caso}\\end{cases}\\]\ny,\n\\[X_4=\\begin{cases}1 \\hspace{20pt} \\text{si el paciente es discapacitado}\\\\  0 \\hspace{20pt} \\text{en otro caso}\\end{cases}\\]\nAsí, el modelo quedaría como:\n\\[Y_i=\\beta_0+\\beta_1 X_{i1} + \\beta_2 X_{i2}+ \\beta_3 X_{i3} + \\beta_4 X_{i4} + \\varepsilon_i\\]\ndonde las variables predictoras están definidas como antes."
  },
  {
    "objectID": "slides/lec_week4.html#regresión-polinómica",
    "href": "slides/lec_week4.html#regresión-polinómica",
    "title": "Regresión lineal",
    "section": "Regresión polinómica",
    "text": "Regresión polinómica\nLas regresiones polinómicas son casos especiales del modelo de regresión lineal general. Estos contienen términos cuadrados o de grados mayores de las variables predictoras, provocando que la función de respuesta sea curvilínea. Un ejemplo de una regresión polinómica sería:\n\\[Y_i=\\beta_0 + \\beta_1 X_i + \\beta_2 X_{i}^{2}+\\varepsilon_i\\]"
  },
  {
    "objectID": "slides/lec_week4.html#variables-transformadas",
    "href": "slides/lec_week4.html#variables-transformadas",
    "title": "Regresión lineal",
    "section": "Variables transformadas",
    "text": "Variables transformadas\nLos modelos con variables transformadas involucran funciones respuesta complejas y curvilíneas, aún así son casos especiales de un modelo de regresión lineal general.\nConsideremos el siguiente modelo:\n\\[\\log Y_i = \\beta_0 + \\beta_1 X_{i1} +\\beta_2 X_{i2}+\\beta_3 X_{i3} +\\varepsilon_i\\]\nAcá, la superficie de respuesta (desde el punto de vista geométrico) es compleja, aún así puede ser tratada como un modelo de regresión lineal general. Si consideramos \\(Y_{i}^{'}=\\log Y_i\\), podemos reescribir el modelo de regresión anterior como:\n\\[Y_{i}^{'}=\\beta_0 + \\beta_1 X_{i1} +\\beta_2 X_{i2}+\\beta_3 X_{i3} +\\varepsilon_i\\]\nEl cual tiene la forma del modelo general. La variable respuesta es el logaritmo de \\(Y\\)."
  },
  {
    "objectID": "slides/lec_week4.html#variables-transformadas-continuación",
    "href": "slides/lec_week4.html#variables-transformadas-continuación",
    "title": "Regresión lineal",
    "section": "Variables transformadas: continuación",
    "text": "Variables transformadas: continuación\nMuchos modelos pueden ser transformados al modelo de regresión lineal general, por ejemplo el modelo:\n\\[Y_i=\\dfrac{1}{\\beta_0 + \\beta_1 X_{i1} +\\beta_2 X_{i2}+\\varepsilon_i}\\]\nPuede ser transformado al modelo de regresión lineal general al considerar \\(Y_{i}^{'}=1/Y_i\\). Así, se puede reescribir como:\n\\[Y_{i}^{'}=\\beta_0 + \\beta_1 X_{i1} +\\beta_2 X_{i2}+\\varepsilon_i\\]"
  },
  {
    "objectID": "slides/lec_week4.html#efectos-de-interacción",
    "href": "slides/lec_week4.html#efectos-de-interacción",
    "title": "Regresión lineal",
    "section": "Efectos de interacción",
    "text": "Efectos de interacción\nCuando los efectos de la variables predictoras en la variable respuesta no son aditivos, el efecto de un predictor depende del nivel en otra variable predictora. El modelo de regresión lineal general abarca modelos con efectos no aditivos o que interactúan entre sí. Un ejemplo de un modelo de regresión lineal no aditivo con dos variables predictoras \\(X_1\\) y \\(X_2\\) es:\n\\[Y_i=\\beta_0 + \\beta_1 X_{i1}+\\beta_2 X_{i2} + \\beta_3 X_{i1} X_{2i} +\\varepsilon_i\\]\nAcá, la función de respuesta es compleja debido a la término de interacción \\(\\beta_3 X_{i1} X_{2i}\\). Aún así, el modelo anterior, es un caso especial de un modelo de regresión lineal general. Sea \\(X_{i3}=X_{i1}X_{i2}\\), podemos reescribir el modelos anterior como:\n\\[Y_i=\\beta_0 + \\beta_1 X_{i1}+\\beta_2 X_{i2} + \\beta_3 X_{i3} +\\varepsilon_i\\]\nEn el cual es claro ver que tiene la forma general buscada."
  },
  {
    "objectID": "slides/lec_week4.html#combinación-de-casos",
    "href": "slides/lec_week4.html#combinación-de-casos",
    "title": "Regresión lineal",
    "section": "Combinación de casos",
    "text": "Combinación de casos\nUn modelo de regresión puede combinar muchos de los elementos que hemos mencionado, y aún así ser tratado como un modelo de regresión lineal general. Consideremos el siguiente modelo de regresión que contiene términos lineal y cuadráticos para cada una de las variables predictoras, y un término de interacción.\n\\[Y_i=\\beta_0 + \\beta_1 X_{i1}+\\beta_{2}X_{i1}^{2} + \\beta_3 X_{i2} + \\beta_4 X_{i2}^{2}+\\beta_5 X_{i1}X_{i2}+\\varepsilon_i\\]\nSi definimos,\n\\[Z_{i1}=X_{i1} \\hspace{15pt} Z_{i2}=X_{i1}^{2} \\hspace{15pt} Z_{i3}=X_{i2} \\hspace{15pt} Z_{i4}=X_{i2}^{2} \\hspace{15pt} Z_{i5}=X_{i1}X_{i2}\\]\nPodemos representar el modelo como:\n\\[Y_i=\\beta_0 + \\beta_1 Z_{i1}+\\beta_2 Z_{i2}+\\beta_3 Z_{i3} + \\beta_4 Z_{i4} + \\beta_5 Z_{i5} +\\varepsilon\\]"
  },
  {
    "objectID": "slides/lec_week4.html#interpretación",
    "href": "slides/lec_week4.html#interpretación",
    "title": "Regresión lineal",
    "section": "Interpretación",
    "text": "Interpretación\nDebe estar claro, por lo ejemplos, que el modelo de regresión lineal general no está restringido a una respuesta lineal. El término modelo lineal hace referencia al hecho que el modelo en estudio es lineal en los parámetros; no hace referencia a la forma de la superficie de respuesta.\nDecimos que un modelo de regresión es lineal en los parámetros cuando puede ser escrito de la forma:\n\\[Y_i=c_{i0}\\beta_0 + c_{i1}\\beta_1 + c_{i2}\\beta_2 + \\dots + c_{i,p-1}\\beta_{p-1} +\\varepsilon_i\\]\ndonde los términos \\(c_{i0},c_{i1},\\)etc, son coeficientes que acompañan a las variables predictoras. Un ejemplo de un modelo de regresión lineal no lineal sería:\n\\[Y_i=\\beta_0 \\exp (\\beta_1 X_i) + \\varepsilon\\]\nEste último modelo no puede ser expresado en la forma de un modelo de regresión lineal."
  },
  {
    "objectID": "slides/lec_week4.html#bibliografía-recomendada-para-modelos-lineales",
    "href": "slides/lec_week4.html#bibliografía-recomendada-para-modelos-lineales",
    "title": "Regresión lineal",
    "section": "Bibliografía recomendada para modelos lineales",
    "text": "Bibliografía recomendada para modelos lineales\nPara profundizar en la teoría de modelos lineales (y sus aplicaciones) se recomienda el libro: Applied lineal statistical models. Kutner Michael H., Nachtsheim Christopher J. , Neter John ,Li William. 5th Edition, 2004."
  },
  {
    "objectID": "pages/week5.html",
    "href": "pages/week5.html",
    "title": "Semana 5",
    "section": "",
    "text": "Guía ejemplo - Certamen #1\nBase de datos: Advertising\nGuía Certamen #1\nBase de datos: Marketing\nBase de datos: Real Estate"
  },
  {
    "objectID": "documents/G1/IND163C_2022_02_G1.html",
    "href": "documents/G1/IND163C_2022_02_G1.html",
    "title": "Ejemplo resulto - Certamen #1",
    "section": "",
    "text": "Pregunta tipo prueba\nPublicidad El conjunto de datos Advertising consiste en las ventas (sales) en miles de unidades de un producto en 200 mercados diferentes, junto con los presupuestos en dólares de publicidad en cada uno de estos mercados para tres medios diferentes: televisión (TV), radio y periódicos (newspaper).\n\nRealice un análisis exploratorio del conjunto de datos Advertising.\n\n\nlibrary(tidyverse) \nlibrary(modelr)\nlibrary(broom)\nlibrary(readr)\nAdvertising <- read_csv(\"Advertising.csv\") %>% select(-X1)\n\n⊕Cargamos algunos paquetes que serán útiles para el análisis, luego leémos el conjunto de datos y descartamos la primera columna (por ser una columna que no nos entrega información). Adicionalmente, imprimimos parte de los datos para verificar que están siendo correctamente ingresados.\n\nhead(Advertising)\n\n# A tibble: 6 x 4\n     TV radio newspaper sales\n  <dbl> <dbl>     <dbl> <dbl>\n1 230.   37.8      69.2  22.1\n2  44.5  39.3      45.1  10.4\n3  17.2  45.9      69.3   9.3\n4 152.   41.3      58.5  18.5\n5 181.   10.8      58.4  12.9\n6   8.7  48.9      75     7.2\n\n\nPara realizar el análisis exploratorio de datos (EDA por sus siglas en inglés), existen varias formas de abarcar el problema. Una manera sencilla aunque sólo preliminar para realizar un EDA sistemático a un conjunto de datos, es usar el paquete DataExplorer\n\nlibrary(DataExplorer)\nplot_intro(Advertising)\n\n\n\n\nExploración del tipo de variables y datos faltantes\n\n\n\n\nLa función plot_intro() nos entrega el tipo de variables en las columnas y el porcentaje de datos faltantes. Alternativamente, se puede obtener la misma información en formato de tabla usando introduce().\nEn el caso que existan datos faltantes a lo largo del conjunto de datos en distintas variables, es posible obtener el detalle del porcentaje de estos utilizando la función plot_missing(). El paquete DataExplorer entrega además sugerencias sobre la calidad de las variables conforme el nivel de datos faltantes presentes, sin embargo, la eliminación de columnas debe ser estudiada cuidadosamente y siempre dependerá del contexto del problema.\nEn el caso de que existan datos discretos, es posible visualizar la distribución de frecuencias para todas estas variables utilizando la función plot_bar(). De manera similar, es posible obtener los histogramas para las variables continuas utilizando la función plot_histogram():\n\nplot_histogram(Advertising)\n\n\n\n\nHistograma para las variables continuas\n\n\n\nplot_density(Advertising)\n\n\n\n\nHistograma para las variables continuas\n\n\n\n\nLa primera función nos entrega los histogramas hechos sistemáticamente usando el paquete ggplot2, mientras que el segundo realiza una estimación de densidad por kernel, que vendría siendo algo así como una versión suavizada del histograma. Esta técnica tomará relevancia más adelante en el curso.\nPara comparar visualmente la distribución de las variables en estudio con distribuciones teóricas conocidas, es posible utilizar QQ-plot mediante la función plot_qq().\n\nplot_qq(Advertising)\n\n\n\n\nQQ plot de las variables\n\n\n\n\nPor defecto, el comando plot_qq() compara con una distribución normal, por lo que es una buena herramienta visual para el análisis de residuos bajo un modelo lineal. Adicionalmente, es posible agrupar las variables continuas graficadas por factores o variables categóricas mediante el argumento plot_qq(... , by=\"\").\nPara realizar un análisis correlacional de las variables en estudio, es posible utilizar la función plot_correlation()\n\nplot_correlation(Advertising)\n\n\n\n\nAnálisis correlacional\n\n\n\n\nPara realizar un análisis de componentes principales (que veremos más adelante en detalle) se puede utilizar la función plot_prcomp(). Omitiremos estos gráficos por el momento.\n⊕Hay que ser particularmente cuidadoso en la interpretación de este análisis, pues se debe tener claro que tipo de correlación se está calculando (y graficando). Este comando utiliza la función cor(). Es posible, realizar este análisis para los dos tipos de variables: discretos y continuos. Se recomienda tratar los datos faltantes antes de realizar este proceso.\nComo recordarán de cursos anteriores, uno de los mejores gráficos disponibles es el boxplot que puede ser calculado fácilmente utilizando la función plot_boxplot(..., by=\"\") si deseamos agrupar por alguna variable categórica. Para ver cada uno de manera univariada usamos la siguiente función.\n\np <- ggplot(Advertising, aes(TV)) + geom_boxplot()\np \n\n\n\n\n⊕Este tipo de gráficos toma más relevancia cuando podemos analizar una misma variable agrupada por una categórica, como veremos más adelante.\nTambién es posible obtener los gráficos de dispersión de cada una de las variables en estudio mediante la función plot_scatterplot(... ,by=\"\") agrupada por una variable categórica.\nFinalmente, cabe mencionar que los estadísticos descriptivos (media, varianza, cuartiles, etc) también son parte del EDA. Todo el proceso anterior puede ser en su totalidad automatizado con el comando create_report(), este creo un archivo .html con las funciones que puede hacer con el conjunto de datos, sin embargo, hay que tomar atención a lo que hace en cada uno de los pasos pues es sólo un proceso sistematizado con parámetros por defecto. Se recomienda realizar cada paso por separado.\nEn este ejemplo introductorio, al no tener datos faltantes y categóricos, el EDA es bastante sencillo y se reduce a la creación de gráficos básicos y estadística descriptiva.\n\nRealice un ajuste lineal simple para las ventas (sales) medidas en miles de unidades vs cada uno de los tres medios utilizados. Explicite los ajustes realizados.\n\n\np1<- ggplot(data = Advertising, mapping = aes(x = TV, y = sales)) + \n  geom_point() + geom_smooth(method = \"lm\", se = FALSE)\np2<- ggplot(data = Advertising, mapping = aes(x = radio, y = sales)) + \n  geom_point() + geom_smooth(method = \"lm\", se = FALSE)\np3<- ggplot(data = Advertising, mapping = aes(x = newspaper, y = sales)) + \n  geom_point() + geom_smooth(method = \"lm\", se = FALSE)\n\nLa creación de gráficos utilizando ggplot2 funciona de manera modular, primero se establece el conjunto de datos a utilizar, y se especifica que variables serán los ejes (ggplot()). Luego, se grafican los puntos (geom_point()) y finalmente la recta geom_smooth(), respectivamente.\n\nlibrary(gridExtra)\ngrid.arrange(p1, p2, p3, nrow = 1)\n\n\n\n\nAjuste lineal simple para las ventas en función de los tres medios utilizados\n\n\n\n\nLos modelos de regresión ajustados tienen la forma:\n\\[Y=\\beta_0+\\beta_1 X +\\epsilon\\]\ndonde:\n\n\\(Y\\) representa las ventas en miles de unidades\n\\(X\\) representa el presupuesto en cada uno de los medios, respectivamente.\n\\(\\beta_0\\) es el intercepto\n\\(\\beta_1\\) es la pendiente, que representa la relación lineal\n\\(\\epsilon\\) es el término de error aleatorio con media cero.\n\nPara analizar el detalle de nuestro ajusto lineal, guardamos los modelos lineales en tres objetivos distintos:\n⊕El comando lm viene de linear models y existen versiones más generales y específicas dentro de R\n\nmodelo_1<-lm(sales ~ TV, data=Advertising)\nmodelo_2<-lm(sales ~ radio, data=Advertising)\nmodelo_3<-lm(sales ~ newspaper, data=Advertising)\n\nLa función lm() utiliza por defecto el método de mínimos cuadrados para estimar los coeficientes de regresión, pero es posible definir otras metodologías utilizando la función glm().\nLos ajustos especificos obtenidos (con sus estimaciones de los parámetros) los podemos obtener simplemente haciendo un summary() a los modelos calculados.\n\n# Sales vs TV \nsummary(modelo_1)\n\n\nCall:\nlm(formula = sales ~ TV, data = Advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.3860 -1.9545 -0.1913  2.0671  7.2124 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 7.032594   0.457843   15.36   <2e-16 ***\nTV          0.047537   0.002691   17.67   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.259 on 198 degrees of freedom\nMultiple R-squared:  0.6119,    Adjusted R-squared:  0.6099 \nF-statistic: 312.1 on 1 and 198 DF,  p-value: < 2.2e-16\n\n# Sales vs radio\nsummary(modelo_2)\n\n\nCall:\nlm(formula = sales ~ radio, data = Advertising)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7305  -2.1324   0.7707   2.7775   8.1810 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  9.31164    0.56290  16.542   <2e-16 ***\nradio        0.20250    0.02041   9.921   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.275 on 198 degrees of freedom\nMultiple R-squared:  0.332, Adjusted R-squared:  0.3287 \nF-statistic: 98.42 on 1 and 198 DF,  p-value: < 2.2e-16\n\n# Sales vs newspaper\nsummary(modelo_3)\n\n\nCall:\nlm(formula = sales ~ newspaper, data = Advertising)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2272  -3.3873  -0.8392   3.5059  12.7751 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 12.35141    0.62142   19.88  < 2e-16 ***\nnewspaper    0.05469    0.01658    3.30  0.00115 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.092 on 198 degrees of freedom\nMultiple R-squared:  0.05212,   Adjusted R-squared:  0.04733 \nF-statistic: 10.89 on 1 and 198 DF,  p-value: 0.001148\n\n\n\n¿Qué modelo ajustado es mejor? Comente e interprete los resultados de este modelo.\n\nEs claro notar que el modelo que mejor se ajusta es el primero, las ventas (sales) vs TV, lo cual era esperable desde la figura exploratoria. El modelo ajustado corresponde a:\n\\[Y=7.032594+0.047537 X + \\epsilon\\]\nEn otras palabras, nuestra estimación del intercepto es 7.032594, por lo que cuando el presupuesto para el medio televisivo es cero, esperaremos ventas de 7032 unidades, y por cada $1000 dólares adicionales en el presupuesto esperaremos un incremento promedio en las ventas de 47 unidades. Además, es claro notar que ambos coeficientes son** estadísticamente significativos**, y podemos calcular sus intervalos de confianza como:\n\nconfint(modelo_1)\n\n                 2.5 %     97.5 %\n(Intercept) 6.12971927 7.93546783\nTV          0.04223072 0.05284256\n\n\nDebido a que el cero no está incluido en el intervalo de confianza para el coeficiente de pendiente, podemos concluir que por cada $1000 dólares adicionales de presupuesto en el medio televisivo, esperaremos un incremento promedio en las ventas entre 42 y 52 unidades.\nPara justificar en detalle, podemos el R^2 del modelo_1 es el mayor entre los realizados, y podemos realizar una tabla anova para verificar que mediante el test F, los coeficientes no son nulos:\n\nanova(modelo_1)\n\nAnalysis of Variance Table\n\nResponse: sales\n           Df Sum Sq Mean Sq F value    Pr(>F)    \nTV          1 3314.6  3314.6  312.14 < 2.2e-16 ***\nResiduals 198 2102.5    10.6                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nPor lo que, de ser viable nuestro modelo realizado, esto es, que cumpla con los supuestos de una regresión lineal, sería el modelo más adecuado entre los realizados. Para ello, primero visualizamos -nuevamente-:\n\nggplot(Advertising, aes(TV, sales)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  geom_smooth(se = FALSE, color = \"red\")\n\n\n\n\nAjuste lineal simple para ventas vs TV\n\n\n\n\nPara ver el análisis de residuos, es posible utilizar el comando plot(modelo_1) el cual entregará 4 gráficos en formato básico. En lo que sigue, los creamos uno por uno utilizando el paquete ggplot2.\n\nggplot(modelo_1, aes(.fitted, .resid)) +\n  geom_ref_line(h = 0) +\n  geom_point() +\n  geom_smooth(se = FALSE) +\n  ggtitle(\"Residuos vs Ajuste\")\n\n\n\n\nGráfico de residuos vs ajuste\n\n\n\n\nUna forma de visualizar más claramente los residuos, es estandarizándolos y reescalándolos, respectivamente.\n\nmodelo_1_res <- augment(modelo_1, Advertising)\np4 <- ggplot(modelo_1_res, aes(.fitted, .std.resid)) +\n  geom_ref_line(h = 0) +\n  geom_point() +\n  geom_smooth(se = FALSE) +\n  ggtitle(\"Residuos Estadarizados vs Ajuste\")\n\np5 <- ggplot(modelo_1_res, aes(.fitted, sqrt(.std.resid))) +\n  geom_ref_line(h = 0) +\n  geom_point() +\n  geom_smooth(se = FALSE) +\n  ggtitle(\"Reescalamiento\")\n\ngridExtra::grid.arrange(p4, p5, nrow = 1)\n\n\n\n\nGráfico de residuos estandarizados y reescalados vs ajuste\n\n\n\n\nEn el primer gráfico podemos identificar fácilmente cuando un residuo se desvía por varias desviaciones estándar, en donde usualmente estamos en busca de los residuos que difieren por más de 3 desviaciones estándar. El segundo gráfico muestra si los residuos están dispersos equitativamente a lo largo del rango de los predictores. Luego, como hemos asumido normalidad en los errores, debemos realizar un QQ plot\n\nqq_plot <- qqnorm(modelo_1_res$.resid)\nqq_plot <- qqline(modelo_1_res$.resid)\n\n\n\n\nQQ plot de los residuos\n\n\n\n\nComo lo cuantiles esperados se asemejan a los teóricos, podemos asumir normalidad.\nSiguiendo, si deseamos encontrar datos u observaciones anómales podemos calculos las distancias de cook de las observaciones y graficar los apalancamientos.\n\npar(mfrow=c(1, 2))\n\nplot(modelo_1, which = 4, id.n = 5)\nplot(modelo_1, which = 5, id.n = 5)\n\n\n\n\nDistancias de Cook y apalancamientos\n\n\n\n\nEn el gráfico anterior, buscamos las observaciones que tienen mayor distancia de cook, y estas serán sospechosas de ser outliers, siendo esta sospecha reforzada si su apalancamiento está muy a la derecha en el gráfico respectivo. Para extraer las n observaciones con mayor distancia de cook, podemos escribir:\n\nmodelo_1_res %>%\n  top_n(3, wt = .cooksd)\n\n# A tibble: 3 x 10\n     TV radio newspaper sales .fitted .resid   .hat .sigma .cooksd .std.resid\n  <dbl> <dbl>     <dbl> <dbl>   <dbl>  <dbl>  <dbl>  <dbl>   <dbl>      <dbl>\n1  263.   3.5      19.5  12      19.5  -7.53 0.0142   3.22  0.0389      -2.33\n2  291.   4.1       8.5  12.8    20.9  -8.05 0.0191   3.22  0.0605      -2.49\n3  277.   2.3      23.7  11.8    20.2  -8.39 0.0165   3.21  0.0563      -2.59\n\n\nConforme lo anterior, se cumplen todos los supuestos del modelo de regresión lineal simple, por lo que la regresión ajustada es la mejor entre las realizadas.\n\nAjustar mediante un ajuste de regresión múltiple las ventas en miles de unidades (sales), sin incorporar interacciones. Interprete los resultados.\n\nDe manera similar, podemos realizar un ajuste de regresión múltiple utilizando los presupuestos en los distintos medios de manera conjunta:\n\nmodelo_4<-lm(sales ~ TV + radio + newspaper, data= Advertising)\nsummary(modelo_4)\n\n\nCall:\nlm(formula = sales ~ TV + radio + newspaper, data = Advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8277 -0.8908  0.2418  1.1893  2.8292 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.938889   0.311908   9.422   <2e-16 ***\nTV           0.045765   0.001395  32.809   <2e-16 ***\nradio        0.188530   0.008611  21.893   <2e-16 ***\nnewspaper   -0.001037   0.005871  -0.177     0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.686 on 196 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: < 2.2e-16\n\n\nLa interpretación de los coeficientes de regresión es similar a caso de la regresión simple. Primero, notamos que los coeficientes asociados a los presupuestos en televisión y radio son significativos bajo un test de hipótesis t (p-valor \\(< 0.05\\)), mientras que el coeficiente asociado al presupuesto en periódicos no lo es. Por lo que, bajo un modelo de regresión múltiple, cambios en el presupuesto en periódicos no pareciera tener una relación con los cambios en las ventas. Sin embargo, en el caso del presupuesto televisivo, si este aumenta en $1000 dólares y se mantienen los otros predictores constantes, esperaríamos un incremento de 45 unidades en las ventas, en promedio. Análogamente, para un aumento de igual monto en el presupuesto radial, se esperaría un aumento de 188 unidades en promedio.\n\nInvestigue la viabilidad del modelo de regresión múltiple y compare los resultados con el mejor modelo de regresión lineal simple. Obtenga intervalos de confianza para los parámetros de la regresión.\n\nDe manera similar al caso de regresión lineal simple, podemos calcular intervalos de confianza para los parámetros de regresión como:\n\nconfint(modelo_4)\n\n                  2.5 %     97.5 %\n(Intercept)  2.32376228 3.55401646\nTV           0.04301371 0.04851558\nradio        0.17154745 0.20551259\nnewspaper   -0.01261595 0.01054097\n\n\nLuego, podemos hacemos un análisis de residuos:\n\nmodelo_1_res <- modelo_1_res %>%\n  mutate(Model = \"Modelo de regresión lineal simple\")\n\nmodelo_4_res <- augment(modelo_4, Advertising) %>%\n  mutate(Model = \"Modelo de regresión lineal múltiple\") %>%\n  rbind(modelo_1_res)\n\nggplot(modelo_4_res, aes(.fitted, .resid)) +\n  geom_ref_line(h = 0) +\n  geom_point() +\n  geom_smooth(se = FALSE) +\n  facet_wrap(~ Model) +\n  ggtitle(\"Residuos vs Ajuste\")\n\n\n\n\nDistancias de Cook y apalancamientos\n\n\n\n\nComo vemos, la variabilidad de los residuos pareciera ser más constante en el modelo de regresión lineal simple, por lo que sugiere que los supuestos sobre la varianza se cumple. Comparamos los QQ-plot:\n\npar(mfrow=c(1, 2))\n\n# Izquierda: Modelo de regresión lineal simple\nqqnorm(modelo_1_res$.resid); qqline(modelo_1_res$.resid)\n\n# Derecha: Modelo de regresión lineal múltiple\nqqnorm(modelo_4_res$.resid); qqline(modelo_4_res$.resid)\n\n\n\n\nDistancias de Cook y apalancamientos\n\n\n\n\nSin embargo, en el caso de los supuestos distribucionales, lo contrario pareciera suceder. El modelo de regresión lineal múltiple pareciera tener colas de distribución más pesadas que una distribución normal, por lo que el supuesto de normalidad de los errores podría no estar cumpliéndose.\nLuego, podemos comparar las medidas de desempeño de ambos modelos de la forma:\n⊕El paquete stargazer es útil para exportar los resultados de modelos estadísticos de manera tabulada a LaTeX y otros formatos.\n\nlibrary(stargazer)\nstargazer(modelo_1,modelo_4, type=\"latex\", header = FALSE)\n\nComo vemos, el modelo de regresión múltiple aumenta considerablemente nuestros \\(R^2\\) y \\(R^2\\) ajustado, de 0.612 a 0.897 y 0.61 a 0.896, sugiriendo que el modelo de regresión múltiple es más adecuado para modelar la venta de productos. Adicionalmente, nuestro estadístico F es mayor en el caso múltiple, sugiriendo un mayor ajuste de curva. ⊕Estos criterios los veremos detalladamente más adelante en el curso Complementariamente, es posible calcular los medidas de AIC (criterio de información de Akaike) y BIC (criterio de información Bayesiano), en las que el modelo de regresión múltiple también supera al modelo de regresión simple, al tener menor valor en estos indicadores.\n\n¿Cómo se podría justificar -dentro del contexto del problema- una incorporación de interacción en el modelo de regresión múltiple? Proponga una modelo de regresión múltiple con interacción adecuado, analice y compare con los modelos anteriores.\n\nEs claro que en el modelo de regresión múltiple, los incrementos en las ventas se han interpretado manteniendo los otros presupuestos constantes, y que además, estos son independientes. Sin embargo, esto podría ser erróneo, pues es posible que aumentando el presupuesto de publicidad en radio, se aumente la efectividad de la publicidad en televisión, por lo que el coeficiente asociado a la variable TV se verá aumentado conforme la variable radio aumenta. Bajo este escenario, es posible que al tener un monto fijo de presupuesto, repartirlo en ambos medios (tv y radio) sea más efectivo que simplemente asignarlo a publicidad televisiva (como el modelo de regresión múltiple sin interacción sugiere). Así, una segunda iteración del modelo propuesto sería incorporar una interacción entre los dos medios de publicidad mencionados y además, descartar el medio de publicidad en periódicos pues este no fue significativo anteriormente.\n\nmodelo_5<-lm(sales~ TV + radio + TV * radio, data= Advertising)\nsummary(modelo_5)\n\n\nCall:\nlm(formula = sales ~ TV + radio + TV * radio, data = Advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.3366 -0.4028  0.1831  0.5948  1.5246 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 6.750e+00  2.479e-01  27.233   <2e-16 ***\nTV          1.910e-02  1.504e-03  12.699   <2e-16 ***\nradio       2.886e-02  8.905e-03   3.241   0.0014 ** \nTV:radio    1.086e-03  5.242e-05  20.727   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9435 on 196 degrees of freedom\nMultiple R-squared:  0.9678,    Adjusted R-squared:  0.9673 \nF-statistic:  1963 on 3 and 196 DF,  p-value: < 2.2e-16\n\n\n⊕Alternativamente, se puede escribir solo TV * radio, y R interpretará el modelo de la misma manera\nNotamos que todos nuestros coeficientes, incluida la interacción son estadísticamente significativos. Por lo que, tras un aumento de $1000 dólares en el presupuesto de televisión esperaremos, en promedio, un\n\\[(\\beta_1+ \\beta_3 \\times radio) \\times 1000 = 19 + 1\\times radio\\]\ny análogamente, ante un equitativo en el presupuesto de radio, se esperará:\n\\[(\\beta_2+ \\beta_3 \\times TV) \\times 1000 = 28 + 1\\times radio\\]\nLuego, comparamos nuestro nuevo modelo con los dos modelos anterior:\n\nlibrary(stargazer)\nstargazer(modelo_1,modelo_4, modelo_5, type=\"latex\", header = FALSE)\n\nEs claro notar que la incorporación de la interacción en nuestro modelo de regresión múltiple mejoró aún más nuestro ajuste de curva, bajo la perspectiva de los mismos indicadores utilizas para comparar los primeros dos modelos. Finalmente, realizamos un análisis de residuos comparando los modelos realizados:\n\nmodelo_5_res <- augment(modelo_5, Advertising) %>%\n  mutate(Model = \"Model de regresión lineal múltiple con interacción\") %>%\n  rbind(modelo_4_res)\n\nggplot(modelo_5_res, aes(.fitted, .resid)) +\n  geom_ref_line(h = 0) +\n  geom_point() +\n  geom_smooth(se = FALSE) +\n  facet_wrap(~ Model) +\n  ggtitle(\"Residuos vs Ajuste\")\n\n\n\n\nDistancias de Cook y apalancamientos\n\n\n\n\nEl modelo con interacción provee una varianza constante que los otros dos modelos, sin embargo, parecieran haber datos anómalos. Un manera alternativa de analizar visualmente la distribución de los residuos, es utilizando histogramas apropiadamente (en vez de QQ-plot):\n\nggplot(modelo_5_res, aes(.resid)) +\n  geom_histogram(binwidth = .25) +\n  facet_wrap(~ Model, scales = \"free_x\") +\n  ggtitle(\"Histograma de residuos\")\n\n\n\n\nDistancias de Cook y apalancamientos\n\n\n\n\nEs posible que si analizamos para distintas magnitudes de ventas veamos mayor grado de normalidad en los residuos, digamos que si ventas sales mayores a 10, obtenemos:\n\nmodelo_5_res %>%\n  filter(sales > 10) %>%\n  ggplot(aes(.resid)) +\n  geom_histogram(binwidth = .25) +\n  facet_wrap(~ Model, scales = \"free_x\") +\n  ggtitle(\"Histograma de residuos\")\n\n\n\n\nDistancias de Cook y apalancamientos\n\n\n\n\nEs claro ver la normalidad en el modelo de regresión lineal con interacción es bastante viable. En cuanto a las observaciones anómalas, las diagnosticamos como:\n\npar(mfrow=c(1, 2))\n\nplot(modelo_5, which = 4, id.n = 5)\nplot(modelo_5, which = 5, id.n = 5)\n\n\n\n\nDistancias de Cook y apalancamientos\n\n\n\n\nEn el gráfico de la distancia de Cook, se ve claramente que las observaciones 6, 9, 109, 131 y 156 parecieran ser outliers. Por lo que vemos estas observaciones.\n⊕La coma final, ordena a R que nos entregue todas las columnas.\n\nAdvertising[c(6,9,109,131,156),]\n\n# A tibble: 5 x 4\n     TV radio newspaper sales\n  <dbl> <dbl>     <dbl> <dbl>\n1   8.7  48.9      75     7.2\n2   8.6   2.1       1     4.8\n3  13.1   0.4      25.6   5.3\n4   0.7  39.6       8.7   1.6\n5   4.1  11.6       5.7   3.2\n\n\nNotamos que en todas estas observaciones se tienen pocas ventas, lo que reafirma que nuestro modelo no se desempeña bien para niveles bajos de ventas."
  },
  {
    "objectID": "slides/lec_week6.html#ejemplo",
    "href": "slides/lec_week6.html#ejemplo",
    "title": "Introducción a machine learning",
    "section": "Ejemplo",
    "text": "Ejemplo\nTomemos como ejemplo el reconocer dígitos escritos a mano."
  },
  {
    "objectID": "slides/lec_week6.html#ejemplo-continuación",
    "href": "slides/lec_week6.html#ejemplo-continuación",
    "title": "Introducción a machine learning",
    "section": "Ejemplo: continuación",
    "text": "Ejemplo: continuación\nEstos dígitos corresponden a imágenes de 28x28 pixeles, por lo que pueden ser representados en un vector \\(\\mathbf{x}\\) que contiene 784 números reales.\nEl objetivo es construir una máquina que tome el vector \\(\\mathbf{x}\\) como entrada y produzca la identidad del dígito \\(0,\\dots,9\\) como salida.\nEste problema es claramente no-trivial debido a la gran variedad de escrituras. Podría abordarse utilizando reglas heurísticas para distinguir los dígitos en función de las formas de los trazos, pero en la práctica, tal enfoque conduce a una proliferación de reglas y de excepciones a las reglas, etc., e invariablemente da malos resultados."
  },
  {
    "objectID": "slides/lec_week6.html#introducción-continuación",
    "href": "slides/lec_week6.html#introducción-continuación",
    "title": "Introducción a machine learning",
    "section": "Introducción: continuación",
    "text": "Introducción: continuación\nMejores resultados pueden ser obtenidos adoptando un enfoque de machine learning, en donde un conjunto grande de datos de \\(N\\) dígitos \\(\\{x_1 ,\\ldots, x_n\\}\\) llamados conjunto de entrenamiento (training set) se utiliza para ajustar los parámetros de un modelo adaptativo.\nLas categorías de los dígitos en el conjunto de entrenamiento se conocen de antemano, normalmente inspeccionándolos individualmente y etiquetándolos a mano.\nPodemos expresar la categoría de un dígito usando un vector objetivo (target vector) \\(\\mathbf{t}\\), que representa la identidad del dígito correspondiente. Notar que hay un vector objetivo \\(\\mathbf{t}\\) para cada dígito de la imagen \\(\\mathbf{x}\\)."
  },
  {
    "objectID": "slides/lec_week6.html#introducción-continuación-1",
    "href": "slides/lec_week6.html#introducción-continuación-1",
    "title": "Introducción a machine learning",
    "section": "Introducción: continuación",
    "text": "Introducción: continuación\nEl resultado tras aplicar el algoritmo de machine learning puede ser expresado como una función \\(\\mathbf{y}(\\mathbf{x})\\), que toma una nueva imagen del dígito \\(\\mathbf{x}\\) como entrada y que genera como salida un vector \\(\\mathbf{y}\\), codificada de la misma manera que los vector objetivos.\nLa forma exacta de la función \\(\\mathbf{y}(\\mathbf{x})\\) es determinada durante la fase de entrenamiento, también conocida como la fase de aprendizaje, en base al conjunto de entrenamiento.\nUna vez que el modelo es entrenado, este puede ser usado para identificar nuevas imágenes de dígitos, que les llamamos conjunto de prueba (test set).\nLa habilidad de categorizar correctamente nuevos ejemplos que difieren de los utilizados en la fase de aprendizaje es conocido como generalización."
  },
  {
    "objectID": "slides/lec_week6.html#introducción-continuación-2",
    "href": "slides/lec_week6.html#introducción-continuación-2",
    "title": "Introducción a machine learning",
    "section": "Introducción: continuación",
    "text": "Introducción: continuación\nEn la mayoría de las aplicaciones reales, las variables de entrada son típicamente preprocesadas para transformarlas a un nuevo espacio de variables donde, se espera que la problemática de reconocer patrones sea más fácil de resolver.\nPor ejemplo, en el reconocimiento de dígitos escritos a mano, las imágenes de los dígitos generalmente se transforman y escalan tal que cada dígito esté contenido dentro de un cuadro de tamaño fijo. Esto reduce en gran medida la variabilidad dentro de cada clase de dígito, debido a que la localización y la escala de todos los dígitos serán las mismas, por lo que la identificación de patrones se facilitará.\nLa etapa de de pre-procesamiento es usualmente conocida como extracción de características (feature extraction).\nNotar que los nuevos datos, incluidos en el conjunto de entrenamiento, deben ser preprocesados de igual manera que los del conjunto de entrenamiento."
  },
  {
    "objectID": "slides/lec_week6.html#introducción-continuación-5",
    "href": "slides/lec_week6.html#introducción-continuación-5",
    "title": "Introducción a machine learning",
    "section": "Introducción: continuación",
    "text": "Introducción: continuación\nOtra técnica utilizada en machine learning es el aprendizaje reforzado (reinforcement learning), que se ocupa del problema de encontrar acciones adecuadas para tomar en una situación específica con el fin de maximizar una recompensa.\nEn este caso, el algoritmo de aprendizaje no recibe ejemplos de resultados óptimos (como se tienen en el aprendizaje supervisado), sino que debe descubrirlos mediante un proceso de prueba y error."
  },
  {
    "objectID": "slides/lec_week6.html#introducción-continuación-4",
    "href": "slides/lec_week6.html#introducción-continuación-4",
    "title": "Introducción a machine learning",
    "section": "Introducción: continuación",
    "text": "Introducción: continuación\nLas aplicaciones en donde la entrada son los datos de entrenamiento (training set) sin sus correspondientes vectores objetivos son conocidas como problemas de aprendizaje no supervisado (unsupervised learning problems). Varios pueden ser los objetivos en este tipo de problemas:\n\nDescubrir grupos de elementos similares dentro de los datos, en este caso le llamamos agrupamiento (clustering)\nEstimar la distribución de los datos dentro del espacio de los datos, a esto le llamamos estimación de densidad\nProyectar los datos desde un espacio multidimensional a uno de 2 o 3 dimensiones, para así poder visualizarlo, a esto le llamamos visualización."
  },
  {
    "objectID": "slides/lec_week6.html#introducción-continuación-3",
    "href": "slides/lec_week6.html#introducción-continuación-3",
    "title": "Introducción a machine learning",
    "section": "Introducción: continuación",
    "text": "Introducción: continuación\nLa etapa de preprocesamiento también puede ser utilizada para acelerar el cálculo del algoritmo utilizado. Se debe tener especial cuidado en esta etapa debido a que usualmente, cierta información es descartada, y si esta es importante para la solución del problema, la precisión general del sistema confeccionado puede verse afectada.\nLas aplicaciones en donde la entrada son los datos de entrenamiento (training set) en conjunto con sus correspondientes vectores objetivo son conocidas como problemas de aprendizaje supervisado (supervised learning problems).\nLos casos en donde el objetivo es asignar a cada vector de entrada una categoría, se conocen como problemas de clasificación.\nSi se desean salidas que consisten en una o más variables continuas, entonces le llamamos regresión."
  },
  {
    "objectID": "slides/lec_week6.html#ejemplo-1",
    "href": "slides/lec_week6.html#ejemplo-1",
    "title": "Introducción a machine learning",
    "section": "Ejemplo",
    "text": "Ejemplo\nSe desea mejorar las ventas de un producto en particular. El siguiente conjunto de datos contiene datos de las ventas de aquel producto en 200 mercados diferentes, junto con el presupuesto de publicidad para el producto en cada uno de los mercados para 3 medios de publicidad: TV, radio y diario.\n\n\n  X    TV Radio Newspaper Sales\n1 1 230.1  37.8      69.2  22.1\n2 2  44.5  39.3      45.1  10.4\n3 3  17.2  45.9      69.3   9.3\n4 4 151.5  41.3      58.5  18.5\n5 5 180.8  10.8      58.4  12.9\n6 6   8.7  48.9      75.0   7.2"
  },
  {
    "objectID": "slides/lec_week6.html#ejemplo-continuación-1",
    "href": "slides/lec_week6.html#ejemplo-continuación-1",
    "title": "Introducción a machine learning",
    "section": "Ejemplo: continuación",
    "text": "Ejemplo: continuación"
  },
  {
    "objectID": "slides/lec_week6.html#ejemplo-continuación-2",
    "href": "slides/lec_week6.html#ejemplo-continuación-2",
    "title": "Introducción a machine learning",
    "section": "Ejemplo: continuación",
    "text": "Ejemplo: continuación"
  },
  {
    "objectID": "slides/lec_week6.html#ejemplo-continuación-3",
    "href": "slides/lec_week6.html#ejemplo-continuación-3",
    "title": "Introducción a machine learning",
    "section": "Ejemplo: continuación",
    "text": "Ejemplo: continuación\nEn este ejemplo, los presupuestos son las variables de entrada (input) mientras que las ventas es la variable de salida (output). Usualmente denotaremos a las variables de entrada por la letra \\(X\\), así \\(X_1\\) es el presupuesto en televisión, \\(X_2\\) en Radio y \\(X_3\\) en periódicos.\nEstas variables de entregada también se le conocen como predictores, variables independientes, features o simplemente variables.\nLa variable respuesta Sales es usualmente llamada respuesta o variable dependiente, y se denota por la letra \\(Y\\)."
  },
  {
    "objectID": "slides/lec_week6.html#ejemplo-continuación-4",
    "href": "slides/lec_week6.html#ejemplo-continuación-4",
    "title": "Introducción a machine learning",
    "section": "Ejemplo: continuación",
    "text": "Ejemplo: continuación\nEn general, supongamos que observamos una variable respuesta cuantitativa \\(Y\\) y \\(p\\) diferentes predictores \\(X_1,\\dots,X_p\\). Asumiremos que existe algún tipo de relación entre \\(Y\\) y \\(X=(X_1,X_2,\\dots,X_p)\\) que puede ser escrito de forma general como\n\\[Y=f(X)+\\varepsilon\\]\nDonde \\(f\\) es una función fija de \\(X_1,\\dots,X_p\\) y \\(\\varepsilon\\) es un error aleatorio, que es independiente de \\(X\\) y tiene media cero. En lo anterior, \\(f\\) representa la información sistemática que \\(X\\) provee sobre \\(Y\\)."
  },
  {
    "objectID": "slides/lec_week6.html#aprendizaje-estadístico-1",
    "href": "slides/lec_week6.html#aprendizaje-estadístico-1",
    "title": "Regresión lineal",
    "section": "Aprendizaje estadístico",
    "text": "Aprendizaje estadístico\nEl aprendizaje estadístico refiere al conjunto de herramientas y enfoques para estimar \\(f\\).\n¿Para qué estimar \\(f\\)?\nPredicción\nEn muchas situaciones, un conjunto de variables de entrada \\(X\\) son fácilmente obtenibles, pero las salidas \\(Y\\) tienen difícil acceso. Bajo esta configuración, debido a que el promedio de los errores tiene media cero, podemos predecir \\(Y\\) usando:\n\\[\n\\hat{Y}=\\hat{f}(X)\n\\]\ndonde \\(\\hat{f}\\) representa nuestra estimación para \\(f\\) e \\(\\hat{Y}\\) representa la predicción obtenida para \\(Y\\). En este contexto, \\(\\hat{f}\\) es usualmente tratada como una caja negra, en el sentido que no estamos usualmente preocupados con la forma exacta de \\(\\hat{f}\\), si es que esta entrega predicciones precisas de \\(Y\\)."
  },
  {
    "objectID": "slides/lec_week6.html#predicción",
    "href": "slides/lec_week6.html#predicción",
    "title": "Introducción a machine learning",
    "section": "Predicción",
    "text": "Predicción\nEn muchas situaciones, un conjunto de variables de entrada \\(X\\) son fácilmente obtenibles, pero las salidas \\(Y\\) tienen difícil acceso. Bajo esta configuración, debido a que el promedio de los errores tiene media cero, podemos predecir \\(Y\\) usando:\n\\[\n\\hat{Y}=\\hat{f}(X)\n\\]\ndonde \\(\\hat{f}\\) representa nuestra estimación para \\(f\\) e \\(\\hat{Y}\\) representa la predicción obtenida para \\(Y\\). En este contexto, \\(\\hat{f}\\) es usualmente tratada como una caja negra, en el sentido que no estamos usualmente preocupados con la forma exacta de \\(\\hat{f}\\), si es que esta entrega predicciones precisas de \\(Y\\)."
  },
  {
    "objectID": "slides/lec_week6.html#ejemplo-2",
    "href": "slides/lec_week6.html#ejemplo-2",
    "title": "Introducción a machine learning",
    "section": "Ejemplo",
    "text": "Ejemplo\n\nlibrary(plot3D)\nlibrary(tidyverse)\nIncome2<- read.csv(\"./db/Income2.csv\")\n# Ajuste\nfit_2_3_loess <- loess(Income ~ Education + Seniority, data = Income2) \n# Predicción de valores\nx.pred <- seq(min(Income2$Education), max(Income2$Education), length.out = 30)\ny.pred <- seq(min(Income2$Seniority), max(Income2$Seniority), length.out = 30)\nxy     <- expand.grid(Education = x.pred, Seniority = y.pred)\nz.pred <- matrix(predict(fit_2_3_loess, newdata = xy), nrow = 30, ncol = 30)"
  },
  {
    "objectID": "slides/lec_week6.html#ejemplo-continuación-5",
    "href": "slides/lec_week6.html#ejemplo-continuación-5",
    "title": "Introducción a machine learning",
    "section": "Ejemplo: continuación",
    "text": "Ejemplo: continuación\n\nIncome2 %>% \n  scatter3D(\n    type = \"p\",\n    x = Income2$Education, \n    y = Income2$Seniority, \n    z = Income2$Income,\n    colvar = NA, pch = 19, col = \"gold\", cex = 1.75,\n    phi = 25, theta = 45, expand = 0.6,\n    xlab = \"Years of Education\", ylab = \"Seniority\", zlab = \"Income\",\n    panel.first = scatter3D(x = Income2$Education,y = Income2$Seniority,\n    z = Income2$Income,colvar = NA, col = \"black\", add = T,\n    surf = list(x = x.pred, y = y.pred, z = z.pred, \n    fit = predict(fit_2_3_loess), facets = T, col = \"skyblue\",\n    border = \"royalblue\", alpha = 0.45)))"
  },
  {
    "objectID": "slides/lec_week6.html#ejemplo-continuación-6",
    "href": "slides/lec_week6.html#ejemplo-continuación-6",
    "title": "Introducción a machine learning",
    "section": "Ejemplo: continuación",
    "text": "Ejemplo: continuación"
  },
  {
    "objectID": "slides/lec_week6.html#ejemplo-continuación-7",
    "href": "slides/lec_week6.html#ejemplo-continuación-7",
    "title": "Introducción a machine learning",
    "section": "Ejemplo: continuación",
    "text": "Ejemplo: continuación\nConsideremos que un estimador \\(\\hat{f}\\) y un conjunto de variables \\(X\\) entregan la predicción \\(\\hat{Y}=\\hat{f}(X)\\) . Asumiendo que \\(\\hat{f}\\) y \\(X\\) son fijos, entonces se tiene:\n\\[\\begin{align*}\n\\mathbb{E}(Y-\\hat{Y})^2 &= \\mathbb{E}(f(X)+\\varepsilon - \\hat{f}(X))^2 \\\\\n&= \\underbrace{[f(X) - \\hat{f}(X)]^2}_\\text{Reducible} + \\underbrace{\\mathbb{V}(\\varepsilon)}_\\text{Irreducible}\n\\end{align*}\\]\nNosotros nos concentraremos en técnicas para estimar \\(f\\) con el fin de poder minimizar el error reducible."
  },
  {
    "objectID": "slides/lec_week6.html#inferencia",
    "href": "slides/lec_week6.html#inferencia",
    "title": "Introducción a machine learning",
    "section": "Inferencia",
    "text": "Inferencia\nUsualmente estamos interesados en entender la forma en que \\(Y\\) se ve afectada conforme \\(X_1,\\dots,X_p\\) cambia. En este tipo de situaciones, deseamos estimar \\(f\\), pero nuestro objetivo no es necesariamente hacer predicciones para \\(Y\\). En cambio, se quiere entender la relación entre \\(X\\) e \\(Y\\), por lo que ya no podemos tratar \\(\\hat{f}\\) como una caja negra, debido a que para poder explicar el fenómeno debemos tener una forma exacta. Usualmente nos preguntamos:\n\n¿Qué predictores están asociados con la respuesta?\n¿Cuál es la relación entre la respuesta y cada predictor?\n¿La relación entre \\(Y\\) y cada predictor ser explicada adecuadamente usando una ecuación lineal o la relación es más complicada?"
  },
  {
    "objectID": "slides/lec_week6.html#cómo-estimamos-f",
    "href": "slides/lec_week6.html#cómo-estimamos-f",
    "title": "Introducción a machine learning",
    "section": "¿Cómo estimamos \\(f\\)?",
    "text": "¿Cómo estimamos \\(f\\)?\nA lo largo del curso, veremos enfoques lineales y no lineales para estimar \\(f\\). Estos métodos usualmente comparten ciertas características.\nEn general, la mayoría de las técnicas de aprendizaje estadístico pueden ser categorizadas como paramétricas o no-paramétricas."
  },
  {
    "objectID": "slides/lec_week6.html#métodos-paramétricos",
    "href": "slides/lec_week6.html#métodos-paramétricos",
    "title": "Introducción a machine learning",
    "section": "Métodos paramétricos",
    "text": "Métodos paramétricos\nEste enfoque tiene dos pasos y se base en modelos que reducen el problema de estimar \\(f\\) a estimar un conjunto de parámetros.\n\nPros\n\nEs mucho más fácil que ajustar una función arbitraria cualquiera\n\n\n\nContras\n\nEl modelo usualmente no seguirá la forma real de \\(f\\)\nSi el ajuste está muy lejano a la forma real, la estimación será mala\nSe puede caer en sobreajuste."
  },
  {
    "objectID": "slides/lec_week6.html#métodos-paramétricos-continuación",
    "href": "slides/lec_week6.html#métodos-paramétricos-continuación",
    "title": "Introducción a machine learning",
    "section": "Métodos paramétricos: continuación",
    "text": "Métodos paramétricos: continuación\n¿Cuales serían los pasos de un enfoque paramétrico?\n\nAsumir la forma de \\(f\\)\nRealizar un proceso que ajuste el conjunto de datos (training set) para el modelo."
  },
  {
    "objectID": "slides/lec_week6.html#métodos-no-paramétricos",
    "href": "slides/lec_week6.html#métodos-no-paramétricos",
    "title": "Introducción a machine learning",
    "section": "Métodos no paramétricos",
    "text": "Métodos no paramétricos\nEl enfoque no paramétrico se caracteriza por no asumir la forma de \\(f\\), pero en lugar de eso intenta obtener una estimación de \\(f\\) que sea lo más cercano al conjunto de datos sin llegar a un sobreajuste.\n\nPros\n\nAl no asumir nada sobre \\(f\\), estos métodos permiten un vasto rango de formas que se ajustan con precisión a \\(f\\)\n\n\n\nContras\n\nUn gran número de datos es necesario para estimar de forma precisa \\(f\\), mucho más que bajo un enfoque paramétrico."
  },
  {
    "objectID": "slides/lec_week6.html#compensación-entre-precisión-vs-interpretabilidad",
    "href": "slides/lec_week6.html#compensación-entre-precisión-vs-interpretabilidad",
    "title": "Introducción a machine learning",
    "section": "Compensación entre precisión vs interpretabilidad",
    "text": "Compensación entre precisión vs interpretabilidad\nComo sabemos hay métodos de aprendizaje estadístico que son menos flexibles que otros, por ejemplo la regresión lineal. Sin embargo, existen razones para escoger estas metodologías en vez de una más flexible.\n\nSi la inferencia es nuestro principal objetivo, los modelos más restrictivos son recomendados debido a que la relación entre \\(X\\) e \\(Y\\) es fácilmente interpretable.\nMétodos más flexibles usualmente llegar a estimación más complejas que dificultan el análisis de alguna relación individual entre un predictor y la variable respuesta.\nIncluso cuando la predicción es el único objetivo, modelos más restrictivos pueden entregar mayor precisión que la mayoría de los métodos más flexible, debido a que estos últimos pueden sobreajustar."
  },
  {
    "objectID": "slides/lec_week6.html#diagrama-de-modelos",
    "href": "slides/lec_week6.html#diagrama-de-modelos",
    "title": "Introducción a machine learning",
    "section": "Diagrama de modelos",
    "text": "Diagrama de modelos"
  },
  {
    "objectID": "slides/lec_week6.html#teorema-del-no-free-lunch",
    "href": "slides/lec_week6.html#teorema-del-no-free-lunch",
    "title": "Introducción a machine learning",
    "section": "Teorema del No-Free-Lunch",
    "text": "Teorema del No-Free-Lunch\n¿Por qué no simplemente elegimos el mejor método para todos los problemas?\nEl teorema de No-Free-Lunch establece que todos los algoritmos de optimización se desempeñan igualmente bien cuando su desempeño es promediado sobre todas las funciones objetivos posibles."
  },
  {
    "objectID": "slides/lec_week6.html#compromiso-sesgo-varianza",
    "href": "slides/lec_week6.html#compromiso-sesgo-varianza",
    "title": "Introducción a machine learning",
    "section": "Compromiso sesgo-varianza",
    "text": "Compromiso sesgo-varianza\nUna de las herramientas que tenemos para cuantificar que tan bueno es nuestro ajuste es el Error cuadrático medio, lo notamos por sus siglas en inglés MSE. Para un valor \\(x_0\\) dado, es posible mostrar que el error cuadrático medio se puede descomponer de la forma\n\\[\n\\mathbb{E}\\left(y_0 - \\hat{f}(x_0)\\right)^2=\\mathbb{V}(\\hat{f}(x_0))+[Bias(\\hat{f}(x_0))]^2+\\mathbb{V}(\\varepsilon)\n\\]\nEn donde el lado izquierdo representa el error cuadrado medio esperado cuando se estima \\(f\\) y se evalúan en el punto \\(x_0\\).\nDe la ecuación anterior se desprende que para minimizar el error cuadrático medio se debe seleccionar una metodología que simultáneamente logre una varianza baja y un bajo sesgo."
  },
  {
    "objectID": "slides/lec_week6.html#compromiso-sesgo-varianza-continuación",
    "href": "slides/lec_week6.html#compromiso-sesgo-varianza-continuación",
    "title": "Introducción a machine learning",
    "section": "Compromiso sesgo-varianza: continuación",
    "text": "Compromiso sesgo-varianza: continuación\nA esta relación le llamamos un compromiso, debido a que es fácil obtener un método con extremadamente bajo sesgo pero varianza alta o un modelo con baja varianza pero alto sesgo.\nComo regla general, si se utilizan metodologías más flexibles, la varianza crecerá y el sesgo disminuirá."
  }
]