<!DOCTYPE html>
<html lang="es"><head>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/tabby.min.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.0.36">

  <meta name="author" content="Eloy Alvarado Narváez">
  <title>IND163C - Análisis de Negocios (Business Analytics) - Métodos supervisados: continuación</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
      }
    pre.numberSource { margin-left: 3em;  padding-left: 4px; }
    div.sourceCode
      { color: #24292e;  }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #24292e; } /* Normal */
    code span.al { color: #ff5555; font-weight: bold; } /* Alert */
    code span.an { color: #6a737d; } /* Annotation */
    code span.at { color: #d73a49; } /* Attribute */
    code span.bn { color: #005cc5; } /* BaseN */
    code span.bu { color: #d73a49; } /* BuiltIn */
    code span.cf { color: #d73a49; } /* ControlFlow */
    code span.ch { color: #032f62; } /* Char */
    code span.cn { color: #005cc5; } /* Constant */
    code span.co { color: #6a737d; } /* Comment */
    code span.cv { color: #6a737d; } /* CommentVar */
    code span.do { color: #6a737d; } /* Documentation */
    code span.dt { color: #d73a49; } /* DataType */
    code span.dv { color: #005cc5; } /* DecVal */
    code span.er { color: #ff5555; text-decoration: underline; } /* Error */
    code span.ex { color: #d73a49; font-weight: bold; } /* Extension */
    code span.fl { color: #005cc5; } /* Float */
    code span.fu { color: #6f42c1; } /* Function */
    code span.im { color: #032f62; } /* Import */
    code span.in { color: #6a737d; } /* Information */
    code span.kw { color: #d73a49; } /* Keyword */
    code span.op { color: #24292e; } /* Operator */
    code span.ot { color: #6f42c1; } /* Other */
    code span.pp { color: #d73a49; } /* Preprocessor */
    code span.re { color: #6a737d; } /* RegionMarker */
    code span.sc { color: #005cc5; } /* SpecialChar */
    code span.ss { color: #032f62; } /* SpecialString */
    code span.st { color: #032f62; } /* String */
    code span.va { color: #e36209; } /* Variable */
    code span.vs { color: #032f62; } /* VerbatimString */
    code span.wa { color: #ff5555; } /* Warning */
  </style>
  <link rel="stylesheet" href="../site_libs/revealjs/dist/theme/quarto.css" id="theme">
  <link href="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-captioned.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-captioned) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-captioned.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-captioned .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-captioned .callout-caption  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-captioned.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-captioned.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-caption {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-caption {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-caption {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-captioned .callout-body > .callout-content > :last-child {
    margin-bottom: 0.5rem;
  }

  .callout.callout-captioned .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-captioned) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-caption {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-caption {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-caption {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-caption {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-caption {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="center">
  <h1 class="title">Métodos supervisados: continuación</h1>
  <p class="subtitle">IND 163 - 2022/02</p>
  <p class="author">Eloy Alvarado Narváez</p>
  <p class="institute">Universidad Técnica Federico Santa María</p>
  <p class="date">28/10/22</p>
</section>

<section>
<section id="máquina-de-vectores-de-soporte-svm" class="title-slide slide level1 center">
<h1>Máquina de vectores de soporte (SVM)</h1>
<p>Esta metodología fue desarrollada en lo ’90 por la comunidad de ciencias computacionales. Consiste en la generalización de un clasificador llamado <strong>clasificador de máximo margen</strong>, que destaca por su simpleza, pero que en la práctica es difícil de utilizar debido a que requiere que las clases sean separables por un límite lineal.</p>
</section>
<section id="clasificador-de-máximo-margen" class="slide level2">
<h2>Clasificador de máximo margen</h2>
<p>Antes de definidir el clasificador de máximo margen, debemos introducir dos conceptos primordiales:</p>
<ul>
<li class="fragment"><p>Hiperplano</p></li>
<li class="fragment"><p>Hiperplano de separación óptimo</p></li>
</ul>
</section>
<section id="hiperplano" class="slide level2">
<h2>Hiperplano</h2>
<p>En un espacio <span class="math inline">\(p-\)</span>dimensional, un hiperplano es un subespacio afín plano de dimensión <span class="math inline">\(p-1\)</span>. Por ejemplo, en dos dimensiones, un hiperplano es una linea. En tres dimensiones, un hiperplano es un subespacio 2-dimensional plano.</p>
<p>Matemáticamente, para el caso bidimensional, un hiperplano está definido por la ecuación:</p>
<p><span class="math display">\[
\beta_0+\beta_1 X_1 + \beta_2 X_2 = 0
\]</span></p>
<p>para parámetros <span class="math inline">\(\beta_0,\beta_1\)</span> y <span class="math inline">\(\beta_2\)</span>. Naturalmente, la extensión a <span class="math inline">\(p\)</span> dimensiones es:</p>
<p><span class="math display">\[
\beta_0+\beta_1 X_1 +\beta_2 X_2 + \dots + \beta_p X_p =0
\]</span></p>
<p>que define un hiperplano <span class="math inline">\(p-\)</span>dimensional, en el sentido de que si un punto <span class="math inline">\(X=(X_1,X_2,\dots,X_p)^T\)</span> en un espacio <span class="math inline">\(p-\)</span>dimensional que satisface la ecuación anterior.</p>
</section>
<section id="figura-hiperplano" class="slide level2">
<h2>Figura hiperplano</h2>

<img data-src="images/week9/hyperplane.png" class="r-stretch quarto-figure-center"></section>
<section id="clasificando-usando-un-hiperplano-separable" class="slide level2">
<h2>Clasificando usando un hiperplano separable</h2>
<p>Supongamos que tenemos una matriz de datos <span class="math inline">\(\mathbf{X}\)</span> de tamaño <span class="math inline">\(n\times p\)</span> que consiste en <span class="math inline">\(n\)</span> observaciones de entrenamiento en un espacio <span class="math inline">\(p-\)</span>dimensional,</p>
<p><span class="math display">\[
x_1=\begin{pmatrix}x_{11} \\ \vdots \\ x_{1p}\end{pmatrix} , \dots,x_n=\begin{pmatrix} x_{n1} \\ \vdots \\ x_{np} \end{pmatrix}
\]</span></p>
<p>y que estas observaciones caen dentro de dos clases, esto es, <span class="math inline">\(y_1,\dots,y_n \in \{-1,1\}\)</span> donde <span class="math inline">\(-1\)</span> representa una clase y <span class="math inline">\(1\)</span> la otra clase. También tenemos una observación de prueba, un <span class="math inline">\(p-\)</span>vector de <em>features</em> observadas <span class="math inline">\(x^*=(x_{1}^{*}\, \dots \,x_{p}^{*})^T\)</span></p>
</section>
<section id="clasificando-usando-un-hiperplano-separable-continuación" class="slide level2">
<h2>Clasificando usando un hiperplano separable: continuación</h2>
<p>Nuestro objetivo es desarrollar un clasificador basado en este conjunto de entrenamiento que clasifique correctamente la observación de prueba usando las variables medidas, para esto nosotros ya hemos visto varias metodologías que podríamos usar: LDA, QDA, árboles de decisión y regresión logística.</p>
<p>En lo que sigue veremos una metodología que se basa en el concepto de hiperplano separable.</p>
</section>
<section id="clasificando-usando-un-hiperplano-separable-continuación-1" class="slide level2">
<h2>Clasificando usando un hiperplano separable: continuación</h2>
<p>Supongamos que es posible construir un hiperplano que separe las observaciones de entrenamiento perfectamente de acuerdo a sus clases. Por lo que si utilizamos las clase como antes (<span class="math inline">\(\{-1,1\}\)</span>) se tendrá la propiedad que</p>
<p><span class="math display">\[
\beta_0+\beta_1 x_{i1} +\beta_2 x_{i2}+\dots+\beta_p x_{ip} &gt; 0 \quad \text{si} \quad y_i=1
\]</span></p>
<p>y,</p>
<p><span class="math display">\[
\beta_0+\beta_1 x_{i1} +\beta_2 x_{i2}+\dots+\beta_p x_{ip} &lt; 0 \quad \text{si} \quad y_i=-1
\]</span></p>
<p>equivalentemente, un hiperplano separable tiene la propiedad que:</p>
<p><span class="math display">\[
y_i(\beta_0 +\beta_0+\beta_1 x_{i1} +\beta_2 x_{i2}+\dots+\beta_p x_{ip})&gt;0
\]</span></p>
<p>para todo <span class="math inline">\(i=1,\dots,n\)</span>.</p>
</section>
<section id="clasificando-usando-un-hiperplano-separable-continuación-2" class="slide level2">
<h2>Clasificando usando un hiperplano separable: continuación</h2>

<img data-src="images/week9/hyperplane_sep.png" class="r-stretch quarto-figure-center"></section>
<section id="clasificando-usando-un-hiperplano-separable-continuación-3" class="slide level2">
<h2>Clasificando usando un hiperplano separable: continuación</h2>
<p>Si un hiperplano separable existe, podemos usarlo para construir un clasificador bastante natural: una observación de prueba es asignada una clase dependiendo de que lado del hiperplano está ubicada.</p>
<p>Intuitivamente, podremos estar seguro de nuestra clasificación conforme la magnitud obtenida tras clasificar la observación de prueba.</p>
</section>
<section id="clasificador-de-margen-máximo" class="slide level2">
<h2>Clasificador de margen máximo</h2>
<p>En general, si nuestros datos pueden ser perfectamente separados un hiperplano, entonces existiran un infinito número de aquellos hiperplanos. Para poder construir un clasificador basado en un hiperplano separable, debemos encontrar una forma razonable de decidir cual de estos infinitos hiperplanos separables usar.</p>
<p>Una elección natural es el <strong>hiperplano de máximo margen</strong> (también conocido como <em>hiperplano separable máximo</em>), que es el hiperplano que está más lejos de las observaciones de entrenamiento. Esto es, podemos calcular la distancia (perpendicular) desde cada punto a un hiperplano separable dado; la menor de aquellas distancias es la mínima distancia entre las observaciones y el hiperplano, esta distancia es conocida como <strong>margen</strong>.</p>
<p>El hiperplano de margen máximo es el hiperplano separable en donde el margen es el más grande, esto es, es el hiperplano que tiene la distancia mínima más lejana a las observaciones de entrenamiento.</p>
<p>Luego, podemos clasificar una observación de prueba basado en que lado del hiperplano de margen máximo recae.</p>
</section>
<section id="vector-de-soportes" class="slide level2">
<h2>Vector de soportes</h2>

<img data-src="images/week9/support_vectors.png" class="r-stretch quarto-figure-center"></section>
<section id="construcción-de-un-clasificador-de-margen-máximo" class="slide level2">
<h2>Construcción de un clasificador de margen máximo</h2>
<p>Ahora consideramos la tarea de construir el hiperplano de margen máximo basado en un conjunto de <span class="math inline">\(n\)</span> observaciones de entrenamiento <span class="math inline">\(x_1,\dots, x_n \in \mathbb{R}^{p}\)</span> y clases asociadas <span class="math inline">\(y_1,\dots,y_n \in \{-1,1\}\)</span>. En síntesis, este hiperplano es la solución de un problema de optimización dado por:</p>
<span class="math display">\[\begin{align*}
&amp;\max_{\beta_0,\beta_1,\dots,\beta_p} \quad M\\
&amp;\text{ Sujeto a } \sum_{j=1}^{p} \beta_{j}^{2}=1 \\
&amp;y_i(\beta_0 +\beta_0+\beta_1 x_{i1} +\beta_2 x_{i2}+\dots+\beta_p x_{ip})\geq M \quad \forall i=1,\dots,n
\end{align*}\]</span>
</section>
<section id="construcción-de-un-clasificador-de-margen-máximo-continuación" class="slide level2">
<h2>Construcción de un clasificador de margen máximo: continuación</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a><span class="fu">library</span>(e1071)</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="fu">set.seed</span>(<span class="dv">411</span>)</span>
<span id="cb1-4"><a href="#cb1-4"></a>coord <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(<span class="dv">40</span>), <span class="dv">20</span>, <span class="dv">2</span>)</span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="fu">colnames</span>(coord) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"X1"</span>,<span class="st">"X2"</span>)</span>
<span id="cb1-6"><a href="#cb1-6"></a>y <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="dv">10</span>), <span class="fu">rep</span>(<span class="dv">1</span>,<span class="dv">10</span>))</span>
<span id="cb1-7"><a href="#cb1-7"></a>coord[y <span class="sc">==</span> <span class="dv">1</span>, ] <span class="ot">&lt;-</span> coord[y <span class="sc">==</span> <span class="dv">1</span>, ] <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb1-8"><a href="#cb1-8"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(coord, y)</span>
<span id="cb1-9"><a href="#cb1-9"></a>plot_svm<span class="ot">&lt;-</span><span class="fu">ggplot</span>(<span class="at">data =</span> data, <span class="fu">aes</span>(<span class="at">x =</span> X1, <span class="at">y =</span> X2, <span class="at">color =</span> <span class="fu">as.factor</span>(y))) <span class="sc">+</span></span>
<span id="cb1-10"><a href="#cb1-10"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">6</span>) <span class="sc">+</span></span>
<span id="cb1-11"><a href="#cb1-11"></a>  <span class="fu">theme_bw</span>() <span class="sc">+</span></span>
<span id="cb1-12"><a href="#cb1-12"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"none"</span>)</span>
<span id="cb1-13"><a href="#cb1-13"></a></span>
<span id="cb1-14"><a href="#cb1-14"></a>data<span class="sc">$</span>y <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(data<span class="sc">$</span>y)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="construcción-de-un-clasificador-de-margen-máximo-continuación-1" class="slide level2">
<h2>Construcción de un clasificador de margen máximo: continuación</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1"></a>mod_svm <span class="ot">&lt;-</span> <span class="fu">svm</span>(<span class="at">formula =</span> y <span class="sc">~</span> X1 <span class="sc">+</span> X2, <span class="at">data =</span> data, <span class="at">kernel =</span> <span class="st">"linear"</span>,</span>
<span id="cb2-2"><a href="#cb2-2"></a>                  <span class="at">cost =</span> <span class="dv">10</span>, <span class="at">scale =</span> <span class="cn">FALSE</span>)</span>
<span id="cb2-3"><a href="#cb2-3"></a><span class="fu">summary</span>(mod_svm)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
svm(formula = y ~ X1 + X2, data = data, kernel = "linear", cost = 10, 
    scale = FALSE)


Parameters:
   SVM-Type:  C-classification 
 SVM-Kernel:  linear 
       cost:  10 

Number of Support Vectors:  8

 ( 4 4 )


Number of Classes:  2 

Levels: 
 -1 1</code></pre>
</div>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1"></a>mod_svm<span class="sc">$</span>index</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1]  1  5  6  8 12 13 15 18</code></pre>
</div>
</div>
</section>
<section id="construcción-de-un-clasificador-de-margen-máximo-continuación-2" class="slide level2">
<h2>Construcción de un clasificador de margen máximo: continuación</h2>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1"></a><span class="fu">plot</span>(mod_svm, data)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>

</div>
<img data-src="lec_week9_files/figure-revealjs/unnamed-chunk-6-1.png" width="960" class="r-stretch quarto-figure-center"></section>
<section id="clasificador-de-vectores-de-soporte" class="slide level2">
<h2>Clasificador de vectores de soporte</h2>

<img data-src="images/week9/hyperplane_non_sep.png" class="r-stretch quarto-figure-center"></section>
<section id="clasificador-de-vectores-de-soporte-continuación" class="slide level2">
<h2>Clasificador de vectores de soporte: continuación</h2>
<p>En el caso anterior, un hiperplano separable no existe. En la figura siguiente, la adición de un solo dato provoca un cambio drástico en el margen máximo del hiperplano.</p>

<img data-src="images/week9/hyperplane_non_sep_2.png" class="r-stretch quarto-figure-center"></section>
<section id="clasificador-de-vectores-de-soporte-continuación-1" class="slide level2">
<h2>Clasificador de vectores de soporte: continuación</h2>
<p>Este cambio, reduce la confianza de la asignación de clases. Así, podemos concluir que nuestra metodología es extremadamente sensible a cambio, incluso de sólo una observación.</p>
<p>En estos casos, quizás deberíamos considerar un clasificador basado en un hiperplano que no separe perfectamente las dos clases, con el fin de:</p>
<ul>
<li class="fragment"><p>Tener mayor robustez a las observaciones individuales</p></li>
<li class="fragment"><p>Tener mayor clasificación para la mayoría de las observaciones de entrenamiento</p></li>
</ul>
</section>
<section id="clasificador-de-vectores-de-soporte-continuación-2" class="slide level2">
<h2>Clasificador de vectores de soporte: continuación</h2>
<p>Así, podría ser beneficioso clasificar erróneamente un par de observaciones de entrenamiento para realizar un mejor trabajo clasificando el resto de las observaciones.</p>
<p>El clasificador de vectores de soporte (<em>support vector classifier</em>) o aveces llamado <em>soft margin classifier</em>, hace exactamente lo anterior; en vez de buscar el marger más grande tal que cada observación que clasifique perfectamente, permite que ciertas observaciones estén en la lado incorrecto del margen, o del lado incorrecto del hiperplano.</p>
</section>
<section id="clasificador-de-vectores-de-soporte-continuación-3" class="slide level2">
<h2>Clasificador de vectores de soporte: continuación</h2>
<p>Matemáticamente, corresponde a la solución del siguiente problema de optimización:</p>
<span class="math display">\[\begin{align*}
&amp;\max_{\beta_0,\beta_1,\dots,\beta_p; \epsilon_1,\dots,\epsilon_n} \quad M\\
&amp;\text{ Sujeto a } \sum_{j=1}^{p} \beta_{j}^{2}=1 \\
&amp;y_i(\beta_0 +\beta_0+\beta_1 x_{i1} +\beta_2 x_{i2}+\dots+\beta_p x_{ip})\geq M (1-\epsilon_i) \quad \forall i=1,\dots,n \\
&amp;\epsilon_i \geq 0, \sum_{i=1}^{n} \epsilon_i \leq C
\end{align*}\]</span>
<p>donde <span class="math inline">\(C\)</span> es un parámetro de <em>tunning</em> no negativo. Cuando este parámetro es grande, habrá una gran tolerancia a que las observaciones están al lado incorrecto del margen (y por ende el margen será grande)</p>
</section>
<section id="svm" class="slide level2">
<h2>SVM</h2>
<p>Support Vector Machine (SVM) o Maquina de vectores de soporte es una extensión del clasificador de vectores de soporte que se obtiene tras aumentar el espacio de variables de una manera específica: usando <strong>kernels</strong>.</p>
<p>En el caso del problema de optimización del clasificador de vectores de soporte, la solución involucra sólo <strong>productos internos</strong> de las observaciones (en contraste con las observaciones mismas).</p>
</section>
<section id="svm-continuación" class="slide level2">
<h2>SVM: continuación</h2>
<p>El producto interno de dos <span class="math inline">\(r-\)</span>vectores <span class="math inline">\(a\)</span> y <span class="math inline">\(b\)</span> se define como <span class="math inline">\(\langle a,b\rangle=\sum_{i=1}^{r} a_i b_i\)</span>. Por lo que el producto interno de dos observaciones <span class="math inline">\(x_i,x_{i'}\)</span> está dado por:</p>
<p><span class="math display">\[
\langle x_i, x_{i'}\rangle=\sum_{j=1}^{p}x_{ij}x_{i'j}
\]</span></p>
<p>Más precisamente, se puede mostrar que:</p>
<ul>
<li class="fragment"><p>El clasificador de vectores de soportes lineal se puede representar como</p>
<p><span class="math display">\[
f(x)=\beta_0+\sum_{i=1}^{n}\alpha_i\langle x,x_i\rangle
\]</span></p></li>
</ul>
<div class="fragment">
<p>donde hay <span class="math inline">\(n\)</span> parámetros <span class="math inline">\(\alpha_i, i=1,\dots,n\)</span>, uno para cada observación de entrenamiento.</p>
</div>
</section>
<section id="svm-continuación-1" class="slide level2">
<h2>SVM: continuación</h2>
<ul>
<li class="fragment">Para estimar los parámetros <span class="math inline">\(\alpha_1,\dots,\alpha_n\)</span> y <span class="math inline">\(\beta_0\)</span>, sólo necesitamos los <span class="math inline">\(\begin{pmatrix} n \\ 2 \end{pmatrix}\)</span> productos internos <span class="math inline">\(\langle x_i,x_{i'}\rangle\)</span> entre todos los pares de observaciones de entrenamiento. (esto es <span class="math inline">\(n(n-1)/2\)</span> pares).</li>
</ul>
<div class="fragment">
<p>Supongamos que cada producto interno que hemos definido aparece en la representación del clasificador de vectores de soporte lineal, o en el cálculo del problema de optimización. Reemplazaremos este producto interno con una <strong>generalización</strong> de este, de la forma:</p>
<p><span class="math display">\[
K(x_i,x_{i'})
\]</span></p>
<p>donde <span class="math inline">\(K\)</span> es una función que le llamaremos <strong>kernel</strong>.</p>
</div>
</section>
<section id="svm-continuación-2" class="slide level2">
<h2>SVM: continuación</h2>
<p>Un <strong>kernel</strong> es una función que cuantifica la similitud entre dos observaciones. Por ejemplo, podemos tomar:</p>
<p><span class="math display">\[
K(x_i,x_{i'})=\sum_{j=1}^{p} x_{ij}x_{i'j}
\]</span></p>
<p>que nos entregaría el clasificar de vectores de soporte. Lo anterior se dice que es un kernel lineal porque el lineal para las <em>features</em>. El kernel lineal esencialmente cuantifica la similitud de un par de observaciones usando la correlación de Pearson.</p>
</section>
<section id="svm-continuación-3" class="slide level2">
<h2>SVM: continuación</h2>
<p>Alternativamente, podemos considerar otra forma de kernel, por ejemplo:</p>
<p><span class="math display">\[
K(x_i,x_{i'})=(1+\sum_{j=1}^{p} x_{ij}x_{i'j})^{d}
\]</span></p>
<p>Este kernel es conocido como el <strong>kernel polinomial</strong> de grado <span class="math inline">\(d\)</span>, donde <span class="math inline">\(d\)</span> es un entero positivo. Cuando este parámetro es mayor a 1, el clasificador de vectores de soportes tiende a tener un límite de decisión bastante más flexible.</p>
</section>
<section id="svm-continuación-4" class="slide level2">
<h2>SVM: continuación</h2>
<p>Cuando el clasificador de vectores de soporte es combinado con un kernel no lineal (como el anterior), el clasificador resultante se conoce como <strong>support vector machine</strong>.</p>
<p>Otra opción popular de kernel es el <strong>kernel radial</strong>, que tiene la forma:</p>
<p><span class="math display">\[
K(x_i,x_{i'})=\exp(-\gamma \sum_{j=1}^{p}(x_{ij}-x_{i'j})^2)
\]</span></p>
<p>donde <span class="math inline">\(\gamma\)</span> es una constante positiva.</p>
</section>
<section id="figura-svm" class="slide level2">
<h2>Figura SVM</h2>

<img data-src="images/week9/radial_kernel.png" class="r-stretch quarto-figure-center"></section>
<section id="ejemplo" class="slide level2">
<h2>Ejemplo</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1"></a><span class="fu">set.seed</span> (<span class="dv">411</span>)</span>
<span id="cb7-2"><a href="#cb7-2"></a>x<span class="ot">=</span><span class="fu">matrix</span>(<span class="fu">rnorm</span>(<span class="dv">200</span><span class="sc">*</span><span class="dv">2</span>), <span class="at">ncol=</span><span class="dv">2</span>)</span>
<span id="cb7-3"><a href="#cb7-3"></a>x[<span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>,]<span class="ot">=</span>x[<span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>,]<span class="sc">+</span><span class="dv">2</span></span>
<span id="cb7-4"><a href="#cb7-4"></a>x[<span class="dv">101</span><span class="sc">:</span><span class="dv">150</span>,]<span class="ot">=</span>x[<span class="dv">101</span><span class="sc">:</span><span class="dv">150</span>,]<span class="sc">-</span><span class="dv">2</span></span>
<span id="cb7-5"><a href="#cb7-5"></a>y<span class="ot">=</span><span class="fu">c</span>(<span class="fu">rep</span>(<span class="dv">1</span>,<span class="dv">150</span>),<span class="fu">rep</span>(<span class="dv">2</span>,<span class="dv">50</span>))</span>
<span id="cb7-6"><a href="#cb7-6"></a>dat<span class="ot">=</span><span class="fu">data.frame</span>(<span class="at">x=</span>x,<span class="at">y=</span><span class="fu">as.factor</span>(y))</span>
<span id="cb7-7"><a href="#cb7-7"></a>plot_radial<span class="ot">&lt;-</span><span class="fu">ggplot</span>(dat) <span class="sc">+</span></span>
<span id="cb7-8"><a href="#cb7-8"></a>  <span class="fu">aes</span>(<span class="at">x =</span> x<span class="fl">.1</span>, <span class="at">y =</span> x<span class="fl">.2</span>, <span class="at">colour =</span> y) <span class="sc">+</span></span>
<span id="cb7-9"><a href="#cb7-9"></a>  <span class="fu">geom_point</span>(<span class="at">shape =</span> <span class="st">"circle"</span>, <span class="at">size =</span> <span class="fl">1.5</span>) <span class="sc">+</span></span>
<span id="cb7-10"><a href="#cb7-10"></a>  <span class="fu">scale_color_hue</span>(<span class="at">direction =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb7-11"><a href="#cb7-11"></a>  <span class="fu">theme_bw</span>()</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="ejemplo-continuación" class="slide level2">
<h2>Ejemplo: continuación</h2>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1"></a>plot_radial</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>

</div>
<img data-src="lec_week9_files/figure-revealjs/unnamed-chunk-10-1.png" width="960" class="r-stretch quarto-figure-center"></section>
<section id="ejemplo-continuación-1" class="slide level2">
<h2>Ejemplo: continuación</h2>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1"></a>train<span class="ot">=</span><span class="fu">sample</span>(<span class="dv">200</span>,<span class="dv">100</span>)</span>
<span id="cb9-2"><a href="#cb9-2"></a>svmfit<span class="ot">=</span><span class="fu">svm</span>(y<span class="sc">~</span>., <span class="at">data=</span>dat[train,], <span class="at">kernel=</span><span class="st">"radial"</span>, <span class="at">gamma=</span><span class="dv">1</span>,<span class="at">cost =</span><span class="dv">1</span>)</span>
<span id="cb9-3"><a href="#cb9-3"></a><span class="fu">plot</span>(svmfit , dat[train ,])</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>

</div>
<img data-src="lec_week9_files/figure-revealjs/unnamed-chunk-12-1.png" width="960" class="r-stretch quarto-figure-center"></section>
<section id="ejemplo-continuación-2" class="slide level2">
<h2>Ejemplo: continuación</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1"></a><span class="fu">summary</span>(svmfit)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
svm(formula = y ~ ., data = dat[train, ], kernel = "radial", gamma = 1, 
    cost = 1)


Parameters:
   SVM-Type:  C-classification 
 SVM-Kernel:  radial 
       cost:  1 

Number of Support Vectors:  35

 ( 16 19 )


Number of Classes:  2 

Levels: 
 1 2</code></pre>
</div>
</div>
</section>
<section id="ejemplo-continuación-3" class="slide level2">
<h2>Ejemplo: continuación</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb12-2"><a href="#cb12-2"></a><span class="fu">confusionMatrix</span>(<span class="fu">table</span>(<span class="at">true=</span>dat[<span class="sc">-</span>train,<span class="st">"y"</span>],<span class="at">pred=</span><span class="fu">predict</span>(svmfit, <span class="at">newdata=</span>dat[<span class="sc">-</span>train,])))</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Confusion Matrix and Statistics

    pred
true  1  2
   1 68  7
   2  9 16
                                          
               Accuracy : 0.84            
                 95% CI : (0.7532, 0.9057)
    No Information Rate : 0.77            
    P-Value [Acc &gt; NIR] : 0.05701         
                                          
                  Kappa : 0.5616          
                                          
 Mcnemar's Test P-Value : 0.80259         
                                          
            Sensitivity : 0.8831          
            Specificity : 0.6957          
         Pos Pred Value : 0.9067          
         Neg Pred Value : 0.6400          
             Prevalence : 0.7700          
         Detection Rate : 0.6800          
   Detection Prevalence : 0.7500          
      Balanced Accuracy : 0.7894          
                                          
       'Positive' Class : 1               
                                          </code></pre>
</div>
</div>
</section></section>
<section>
<section id="métodos-no-supervisados" class="title-slide slide level1 center">
<h1>Métodos no supervisados</h1>
<p>Como hemos mencionado antes, en los métodos no supervisados <strong>sólo</strong> tenemos las variables <span class="math inline">\(X_1,X_2,\dots,X_p\)</span> medidas en <span class="math inline">\(n\)</span> observaciones. No estaremos interesados en predecir, porque no tenemos una variable respuesta <span class="math inline">\(Y\)</span> que esté asociada a nuestros datos. En cambio, nuestro objetivo será <em>descubrir</em> características interesantes de las variables medidas.</p>
<ul>
<li class="fragment"><p>¿Hay alguna forma que nos entregue información de visualizar los datos?</p></li>
<li class="fragment"><p>¿Podemos encontrar subgrupos entre las variables u observaciones?</p></li>
</ul>
<div class="fragment">
<p>En lo que sigue nos concentraremos en 2 técnicas particulares: <em>clustering</em> y análisis de componentes principales.</p>
</div>
</section>
<section id="introducción" class="slide level2">
<h2>Introducción</h2>
<p>En general, realizar técnicas no supervisadas tiende a ser más difícil que realizar un método supervisado, pues no se tiene un objetivo claro para el análisis (como lo es predecir en el caso supervisado).</p>
<p>Usualmente, las metodologías no supervisadas se realizar como parte del análisis exploratorio de dato. Además, no existe un consenso en la mejor forma de evaluar las técnicas implementadas en datos de prueba. En el caso supervisados, podemos probar nuestro modelo creado con un conjunto de prueba, pero en el caso no supervisado no es posible debido a que no sabemos el respuesta <strong>verdadera</strong>.</p>
<p>Ejemplos de aplicación de estas metodologías son vastas:</p>
<ul>
<li class="fragment"><p>Identificación de cáncer</p></li>
<li class="fragment"><p>Historiales de compras</p></li>
<li class="fragment"><p>Etc.</p></li>
</ul>
</section>
<section id="análisis-de-componentes-principales" class="slide level2">
<h2>Análisis de componentes principales</h2>
<p>El análisis de componentes principales nos permite resumir, ante un conjunto grande de variables correlacionadas, un subconjunto con menor número de variables representativas que colectivamente explican la mayoría de la variabilidad del conjunto original.</p>
<p>Además de producir variables que pueden ser usadas para métodos supervisados, <strong>PCA</strong> (por sus siglas en inglés, <em>principal component analysis</em>) sirve como herramienta para visualizar datos.</p>
</section>
<section id="qué-son-las-componentes-principales" class="slide level2">
<h2>¿Qué son las componentes principales?</h2>
<p>Supongamos que deseamos visualizar <span class="math inline">\(n\)</span> observaciones con mediciones en un conjunto de <span class="math inline">\(a\)</span> variables/características/<em>features</em>, <span class="math inline">\(X_1,X_2,\dots,X_p\)</span>, como parte de un análisis exploratorio de datos. Podemos examinar los gráficos bidimensionales de dispersión de los datos, que cada una contiene <span class="math inline">\(n\)</span> mediciones de observaciones de 2 variables. Sin embargo, hay <span class="math inline">\(\begin{pmatrix} p \\ 2 \end{pmatrix}=p(p-1)/2\)</span> de tales gráficos.</p>
<p>Por ejemplo, para <span class="math inline">\(p=10\)</span> habrán 45 gráficos. Por lo que si, <span class="math inline">\(p\)</span> es grande no nos será posible mostrarlos todos, y además ninguno de ellos será informativo debido a que sólo tienen un pequeña fracción del total de información disponible en los datos.</p>
<p>Así, necesitamos una mejor metodología para poder visualizar las <span class="math inline">\(n\)</span> observaciones cuando <span class="math inline">\(p\)</span> es grande.</p>
</section>
<section id="qué-son-las-componentes-principales-continuación" class="slide level2">
<h2>¿Qué son las componentes principales?: continuación</h2>
<p>En particular, quisieramos encontrar una presentación de baja dimensionalidad de los datos que capture la mayor cantidad de información posible. Por ejemplo, si podemos obtener un diagrama bidimensional de los datos que capture la mayoría de la información, entonces podemos graficar las observaciones en aquel espacio.</p>
<p>PCA nos entrega una herramienta para hacer justamento esto. Encuentra una representación de baja dimensionalidad del conjunto de datos que contiene la mayor variabilidad posible. La idea es que cada una de las <span class="math inline">\(n\)</span> observaciones vive en un espacio <span class="math inline">\(p-\)</span>dimensional, pero no todas estas dimensiones son igualmente interesantes.</p>
</section>
<section id="qué-son-las-componentes-principales-continuación-1" class="slide level2">
<h2>¿Qué son las componentes principales?: continuación</h2>
<p>PCA busca un pequeño número de dimensiones que sean lo más interesantes posibles, donde el concepto de <em>interesante</em> es medido por la cantidad que varían las observaciones en cada uno de las dimensiones.</p>
<p>Cada una de las dimensiones encontradas por PCA es una combinación lineal de <span class="math inline">\(p\)</span> variables.</p>
<p>Ahora nos enfocamos en la manera en que PCA encuentra estas dimensiones o componentes principales.</p>
</section>
<section id="qué-son-las-componentes-principales-continuación-2" class="slide level2">
<h2>¿Qué son las componentes principales?: continuación</h2>
<p>El <strong>primer componente principal</strong> de un conjunto de variables <span class="math inline">\(X_1,X_2,\dots,X_p\)</span> es la combinación lineal normalizada de las variables</p>
<p><span class="math display">\[
Z_1=\phi_{11}X_1+\phi_{21}X_2+\dots+\phi_{p1}X_p
\]</span></p>
<p>que tenga la <strong>mayor varianza</strong>. Por <em>normalizada</em>, se refiere a que</p>
<p><span class="math display">\[
\sum_{j=1}^{p} \phi_{j1}^{2}=1.
\]</span></p>
<p>Llamamos a los elementos <span class="math inline">\(\phi_{11},\phi_{2 1},\dots,\phi_{p1}\)</span>reciben en el nombre de <em>loadings</em> y son los que definen a la componente. Así, estos elementos conforman el vector de <em>loadings</em> de los componentes principales <span class="math inline">\(\phi_1=(\phi_{11}\,\phi_{21}\dots\phi_{p1})'\)</span></p>
</section>
<section id="qué-son-las-componentes-principales-continuación-3" class="slide level2">
<h2>¿Qué son las componentes principales?: continuación</h2>
<p>Dado un conjunto de datos <span class="math inline">\(\mathbf{X}\)</span> de tamaño <span class="math inline">\(n\times p\)</span>. ¿Cómo calculamos la primera componente principal?</p>
<p>Debido a que sólo estamos interesado en la varianza, asumiremos que cada variable de <span class="math inline">\(\mathbf{X}\)</span> ha sido centrada en cero ( esto es, que las medias de las columnas sean cero). Luego, buscamos la combinación lineal de las variables medidas con forma:</p>
<p><span class="math display">\[
z_{i1}=\phi_{11}x_{i1}+\phi_{21}x_{i2}+\dots+\phi_{p1}x_{ip}
\]</span></p>
<p>que tenga la mayor varianza muestral, sujeto a la restricción que <span class="math inline">\(\sum_{j=1}^{p} \phi_{j1}^{2}=1\)</span>. En otras palabras, el vector de <em>loadings</em> de la primera componente principal resuelve el siguiente problema de optimización:</p>
<p><span class="math display">\[
\max_{\phi_{11},\dots,\phi_{p1}}\Bigg\{ \dfrac{1}{n}\sum_{i=1}^{n} \left( \sum_{j=1}^{p} \phi_{j1}x_{ij}\right)\Bigg\} \text{ sujeto a }\sum_{j=1}^{p}\phi_{j1}^2=1
\]</span></p>
</section>
<section id="reproducibilidad-de-las-componentes" class="slide level2">
<h2>Reproducibilidad de las componentes</h2>
<p>El proceso de PCA genera siempre las mismas componentes principales independientemente del software utilizado, es decir, el valor de los <em>loadings</em> resultantes es el mismo.</p>
<p>La única discrepancia que podría suceder es que los signos estén invertidos, pues los <em>loadings</em> determinan la dirección de la componente.</p>
<h3 id="influencia-de-outliers">Influencia de outliers</h3>
<p>Al trabajar con varianzas, el método PCA es altamente sensible a <em>outliers</em>, por lo que es altamente recomendable estudiar si los hay. La detección de valores atípicos con respecto a una determinada dimensión es algo relativamente sencillo de hacer mediante comprobaciones gráficas.</p>
<p>Las técnicas diagnóstico de datos anómalos escapa de los objetivos del curso, pero son estudiados en análisis multivariado o modelos lineales (dentro del contexto de regresión)</p>
</section>
<section id="proporción-de-varianza-explicada" class="slide level2">
<h2>Proporción de varianza explicada</h2>
<p>Una de las preguntas más frecuentes que surge tras realizar un PCA es: ¿Cuánta información presente en el set de datos original se pierde al proyectar las observaciones en un espacio de menor dimensión? o lo que es lo mismo ¿Cuanta información es capaz de capturar cada una de las componentes principales obtenidas? Para contestar a estas preguntas se recurre a la proporción de varianza explicada por cada componente principal.</p>
<p>Asumiendo que las variables se han estandarizado para tener media cero, la varianza total presente en el set de datos se define como:</p>
<p><span class="math display">\[
\sum_{j=1}^p Var(X_j) = \sum_{j=1}^p \dfrac{1}{n} \sum_{i=1}^n x^{2}_{ij}
\]</span></p>
</section>
<section id="proporción-de-varianza-explicada-continuación" class="slide level2">
<h2>Proporción de varianza explicada: continuación</h2>
<p>y la varianza explicada por la componente <span class="math inline">\(m\)</span> es:</p>
<p><span class="math display">\[
\dfrac{1}{n} \sum_{i=1}^n z^{2}_{im} = \dfrac{1}{n} \sum_{i=1}^n  \left( \sum_{j=1}^p \phi_{jm}x_{ij} \right)^2
\]</span></p>
<p>Así, la proporción de varianza explicada por la componente <span class="math inline">\(m\)</span> viene dada por</p>
<p><span class="math display">\[
\dfrac{\sum_{i=1}^n  \left( \sum_{j=1}^p \phi_{jm}x_{ij} \right)^2} {\sum_{j=1}^p \sum_{i=1}^n x^{2}_{ij}}
\]</span></p>
<p>Esta proporción y su forma acumulada (a lo largo de las componentes) nos entrega información crucial a la hora de elegir cuantas componentes principales utilizar en nuestro análisis.</p>
</section>
<section id="número-óptimo-de-componentes-principales" class="slide level2">
<h2>Número óptimo de componentes principales</h2>
<p>Por lo general, dada una matriz de datos de dimensiones <span class="math inline">\(n \times p\)</span>, el número de componentes principales que se pueden calcular es como máximo de <span class="math inline">\(\min\{n-1,p\}\)</span>. Sin embargo, siendo el objetivo del PCA reducir la <strong>dimensionalidad</strong>, suelen ser de interés utilizar el número mínimo de componentes que resultan suficientes para explicar los datos.</p>
<p>No existe una respuesta o método único que permita identificar cual es el número óptimo de componentes principales a utilizar. Una forma de proceder muy extendida consiste en evaluar la proporción de varianza explicada acumulada y seleccionar el número de componentes mínimo a partir del cual el incremento deja de ser sustancial.</p>
</section>
<section id="número-óptimo-de-componentes-principales-continuación" class="slide level2">
<h2>Número óptimo de componentes principales: continuación</h2>

<img data-src="images/week9/optimal_pca.png" class="r-stretch quarto-figure-center"></section>
<section id="ejemplo-1" class="slide level2">
<h2>Ejemplo</h2>
<p>Datos del porcentaje de asaltos, asesinatos y secuestros por cada 100 mil habitantes para cada uno de los estos de US, en el año 1973. Adicionalmente, se incluye el porcentaje de la población de cada estado que vive en zonas rurales.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1"></a><span class="fu">data</span>(<span class="st">"USArrests"</span>)</span>
<span id="cb14-2"><a href="#cb14-2"></a><span class="fu">head</span>(USArrests)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>           Murder Assault UrbanPop Rape
Alabama      13.2     236       58 21.2
Alaska       10.0     263       48 44.5
Arizona       8.1     294       80 31.0
Arkansas      8.8     190       50 19.5
California    9.0     276       91 40.6
Colorado      7.9     204       78 38.7</code></pre>
</div>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1"></a><span class="fu">apply</span>(<span class="at">X =</span> USArrests, <span class="at">MARGIN =</span> <span class="dv">2</span>, <span class="at">FUN =</span> mean)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  Murder  Assault UrbanPop     Rape 
   7.788  170.760   65.540   21.232 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1"></a><span class="fu">apply</span>(<span class="at">X =</span> USArrests, <span class="at">MARGIN =</span> <span class="dv">2</span>, <span class="at">FUN =</span> var)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    Murder    Assault   UrbanPop       Rape 
  18.97047 6945.16571  209.51878   87.72916 </code></pre>
</div>
</div>
<p>Si no estandarizamos, la variable <em>Assault</em> será la que dominará nuestro PCA.</p>
</section>
<section id="ejemplo-continuación-4" class="slide level2">
<h2>Ejemplo: continuación</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1"></a>pca <span class="ot">&lt;-</span> <span class="fu">prcomp</span>(USArrests, <span class="at">scale =</span> <span class="cn">TRUE</span>)</span>
<span id="cb20-2"><a href="#cb20-2"></a><span class="fu">names</span>(pca)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "sdev"     "rotation" "center"   "scale"    "x"       </code></pre>
</div>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1"></a>pca<span class="sc">$</span>center</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  Murder  Assault UrbanPop     Rape 
   7.788  170.760   65.540   21.232 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1"></a>pca<span class="sc">$</span>scale</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   Murder   Assault  UrbanPop      Rape 
 4.355510 83.337661 14.474763  9.366385 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1"></a>pca<span class="sc">$</span>rotation</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                PC1        PC2        PC3         PC4
Murder   -0.5358995  0.4181809 -0.3412327  0.64922780
Assault  -0.5831836  0.1879856 -0.2681484 -0.74340748
UrbanPop -0.2781909 -0.8728062 -0.3780158  0.13387773
Rape     -0.5434321 -0.1673186  0.8177779  0.08902432</code></pre>
</div>
</div>
<ul>
<li class="fragment"><p>Elementos <em>center</em> y <em>scale</em> contienen la media y desviación en escala original.</p></li>
<li class="fragment"><p>Elemento <em>rotation</em> contiene el valor de los loadings para cada componente</p></li>
</ul>
</section>
<section id="ejemplo-continuación-5" class="slide level2">
<h2>Ejemplo: continuación</h2>
<p>Valor de las componentes principales para cada observación (<em>principal component scores</em>)</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1"></a><span class="fu">head</span>(pca<span class="sc">$</span>x)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                  PC1        PC2         PC3          PC4
Alabama    -0.9756604  1.1220012 -0.43980366  0.154696581
Alaska     -1.9305379  1.0624269  2.01950027 -0.434175454
Arizona    -1.7454429 -0.7384595  0.05423025 -0.826264240
Arkansas    0.1399989  1.1085423  0.11342217 -0.180973554
California -2.4986128 -1.5274267  0.59254100 -0.338559240
Colorado   -1.4993407 -0.9776297  1.08400162  0.001450164</code></pre>
</div>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1"></a><span class="fu">dim</span>(pca<span class="sc">$</span>x)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 50  4</code></pre>
</div>
</div>
</section>
<section id="ejemplo-continuación-6" class="slide level2">
<h2>Ejemplo: continuación</h2>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1"></a><span class="fu">biplot</span>(<span class="at">x =</span> pca, <span class="at">scale =</span> <span class="dv">0</span>, <span class="at">cex =</span> <span class="fl">0.6</span>, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"blue4"</span>, <span class="st">"brown3"</span>))</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>

</div>
<img data-src="lec_week9_files/figure-revealjs/unnamed-chunk-24-1.png" width="960" class="r-stretch quarto-figure-center"></section>
<section id="ejemplo-continuación-7" class="slide level2">
<h2>Ejemplo: continuación</h2>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1"></a>pca<span class="sc">$</span>rotation <span class="ot">&lt;-</span> <span class="sc">-</span>pca<span class="sc">$</span>rotation</span>
<span id="cb33-2"><a href="#cb33-2"></a>pca<span class="sc">$</span>x        <span class="ot">&lt;-</span> <span class="sc">-</span>pca<span class="sc">$</span>x</span>
<span id="cb33-3"><a href="#cb33-3"></a><span class="fu">biplot</span>(<span class="at">x =</span> pca, <span class="at">scale =</span> <span class="dv">0</span>, <span class="at">cex =</span> <span class="fl">0.6</span>, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"blue4"</span>, <span class="st">"brown3"</span>))</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>

</div>
<img data-src="lec_week9_files/figure-revealjs/unnamed-chunk-26-1.png" width="960" class="r-stretch quarto-figure-center"></section>
<section id="ejemplo-continuación-8" class="slide level2">
<h2>Ejemplo: continuación</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1"></a>pca<span class="sc">$</span>sdev<span class="sc">^</span><span class="dv">2</span></span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 2.4802416 0.9897652 0.3565632 0.1734301</code></pre>
</div>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1"></a>prop_varianza <span class="ot">&lt;-</span> pca<span class="sc">$</span>sdev<span class="sc">^</span><span class="dv">2</span> <span class="sc">/</span> <span class="fu">sum</span>(pca<span class="sc">$</span>sdev<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb36-2"><a href="#cb36-2"></a>prop_varianza</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.62006039 0.24744129 0.08914080 0.04335752</code></pre>
</div>
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1"></a>ej_pca<span class="ot">&lt;-</span><span class="fu">ggplot</span>(<span class="at">data =</span> <span class="fu">data.frame</span>(prop_varianza, <span class="at">pc =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>),</span>
<span id="cb38-2"><a href="#cb38-2"></a>       <span class="fu">aes</span>(<span class="at">x =</span> pc, <span class="at">y =</span> prop_varianza)) <span class="sc">+</span></span>
<span id="cb38-3"><a href="#cb38-3"></a>  <span class="fu">geom_col</span>(<span class="at">width =</span> <span class="fl">0.3</span>) <span class="sc">+</span></span>
<span id="cb38-4"><a href="#cb38-4"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb38-5"><a href="#cb38-5"></a>  <span class="fu">theme_bw</span>() <span class="sc">+</span></span>
<span id="cb38-6"><a href="#cb38-6"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Componente principal"</span>,</span>
<span id="cb38-7"><a href="#cb38-7"></a>       <span class="at">y =</span> <span class="st">"Prop. de varianza explicada"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="ejemplo-continuación-9" class="slide level2">
<h2>Ejemplo: continuación</h2>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1"></a>ej_pca</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>

</div>
<img data-src="lec_week9_files/figure-revealjs/unnamed-chunk-30-1.png" width="960" class="r-stretch quarto-figure-center"></section>
<section id="ejemplo-continuación-10" class="slide level2">
<h2>Ejemplo: continuación</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1"></a>prop_varianza_acum <span class="ot">&lt;-</span> <span class="fu">cumsum</span>(prop_varianza)</span>
<span id="cb40-2"><a href="#cb40-2"></a>prop_varianza_acum</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.6200604 0.8675017 0.9566425 1.0000000</code></pre>
</div>
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1"></a>ej_pca2<span class="ot">&lt;-</span><span class="fu">ggplot</span>(<span class="at">data =</span> <span class="fu">data.frame</span>(prop_varianza_acum, <span class="at">pc =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>),</span>
<span id="cb42-2"><a href="#cb42-2"></a>       <span class="fu">aes</span>(<span class="at">x =</span> pc, <span class="at">y =</span> prop_varianza_acum, <span class="at">group =</span> <span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb42-3"><a href="#cb42-3"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb42-4"><a href="#cb42-4"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb42-5"><a href="#cb42-5"></a>  <span class="fu">theme_bw</span>() <span class="sc">+</span></span>
<span id="cb42-6"><a href="#cb42-6"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Componente principal"</span>,</span>
<span id="cb42-7"><a href="#cb42-7"></a>       <span class="at">y =</span> <span class="st">"Prop. varianza explicada acumulada"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="ejemplo-continuación-11" class="slide level2">
<h2>Ejemplo: continuación</h2>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1"></a>ej_pca2</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>

</div>
<img data-src="lec_week9_files/figure-revealjs/unnamed-chunk-34-1.png" width="960" class="r-stretch quarto-figure-center"></section>
<section id="métodos-de-agrupamiento" class="slide level2">
<h2>Métodos de agrupamiento</h2>
<p>Los métodos de agrupamiento o <em>clustering</em> hacen referencia a un conjunto de técnicas que tienen por finalidad encontrar subgrupos o <em>clusters</em> en un conjuntos de datos dado. Al separar en grupos las observaciones que componen un conjunto de datos, interpretamos que los elementos en un mismo grupo son más <em>similares</em> entre sí que con los de otros grupos.</p>
<p>Lo anterior, naturalmente provoca la interrogante ¿Qué se considera que dos datos sean <em>similares</em> o <em>distintos</em>?</p>
<p>Generalmente, responder a esta pregunta frecuentemente requiere tener consideraciones sobre el dominio o naturaleza de los datos en estudio.</p>
</section>
<section id="métodos-de-agrupamiento-continuación" class="slide level2">
<h2>Métodos de agrupamiento: continuación</h2>
<p>Por ejemplo, supongamos que tenemos un conjunto de <span class="math inline">\(n\)</span> observaciones, cada una con <span class="math inline">\(p\)</span> <em>features</em>. Las <span class="math inline">\(n\)</span> observaciones podrían corresponder a muestras de tejido de pacientes con cáncer de mamas, y las <span class="math inline">\(p\)</span> características podrían corresponder a las mediciones recogidas desde cada tejido; mediciones clínicas, mediciones sobre la expresión de genes, etc.</p>
<p>Podríamos tener alguna razón para creer que existe alguna tipo de heterogeneidad entre las <span class="math inline">\(n\)</span> muestras de tejidos; por ejemplo, quizás existen algunos sub-tipos diferentes de cáncer de mama. En este caso, las técnicas de agrupamiento podrían ser utilizadas para encontrar estos subgrupos.</p>
</section>
<section id="métodos-de-agrupamiento-continuación-1" class="slide level2">
<h2>Métodos de agrupamiento: continuación</h2>
<p>Este es un problema no-supervisado debido a que estamos tratando de descubrir la estructure en base a un conjunto de datos. Las técnicas de PCA y <em>clustering</em> buscan simplificar los datos a través de un pequeño número de abstracciones, pero sus mecanismos son diferentes:</p>
<ul>
<li class="fragment"><p>PCA busca encontrar una representación de dimensión baja de las observaciones que expliquen una gran parte de la varianza.</p></li>
<li class="fragment"><p>Los métodos de agrupamiento buscan encontrar subgrupos homogéneos entre las observaciones.</p></li>
</ul>
<div class="fragment">
<p>Debido a que la popularidad y uso de agrupamientos, existen un gran número metodologías de <em>clustering</em>. Nos concentraremos un particularmente dos, <strong>K-medias</strong> y <strong>agrupamiento jerárquico</strong></p>
</div>
</section>
<section id="medidas-de-distancia" class="slide level2">
<h2>Medidas de distancia</h2>
<p>Todos los métodos de <em>clustering</em> tienen una cosa en común, para poder llevar a cabo las agrupaciones necesitan definir y cuantificar la similitud entre las observaciones. Dentro de este contexto, cuantificaremos aquella similitud usando distintos tipos de distancia entre las observaciones, debido a que en principio podemos escoger cualquier tipo de distancia, hace a esta metodología bastante flexible.</p>
</section>
<section id="distancia-euclidiana" class="slide level2">
<h2>Distancia euclidiana</h2>
<p>Para un espacio euclidiano <span class="math inline">\(n-\)</span>dimensional definimos la distancia euclidiana entre dos puntos <span class="math inline">\(p=(p_1,p_2,\dots,p_n)\)</span> y <span class="math inline">\(q=(q_1,q_2,\dots,q_n)\)</span> como la cantidad:</p>
<p><span class="math display">\[d_{euc}(p,q)\sqrt{(p_1-q_1)^2+(p_2-q_2)^2+\dots(p_n-q_n)^2}=\sqrt{\sum_{i=1}^n (p_i-q_i)^2}\]</span></p>
<p>Elevar esta distancia al cuadrado permite dar más peso a aquellas observaciones que están más alejadas.</p>
</section>
<section id="distancia-de-manhattan" class="slide level2">
<h2>Distancia de Manhattan</h2>
<p>La distancia de Manhattan, también conocida como <em>taxicab metric</em> o distancia <span class="math inline">\(L^1\)</span>, define la distancia entre dos puntos <span class="math inline">\(p\)</span> y <span class="math inline">\(q\)</span> como la sumatoria de las diferencias absolutas entre cada dimensión. Esta medida es menos sensible a datos anómalos que la distancia euclidiana.</p>
<p>En este tipo de distancia existen múltiples caminos para unir dos puntos con el mismo valor de distancia de Manhattan, ya que su valor es igual al desplazamiento total en cada una de las dimensiones. Esta distancia está definida como:</p>
<p><span class="math display">\[d_{man}(p,q)=\sum_{i=1}^{n}|(p_i-q_i)|\]</span></p>
</section>
<section id="distancia-de-manhattan-continuación" class="slide level2">
<h2>Distancia de Manhattan: continuación</h2>

<img data-src="lec_week9_files/figure-revealjs/unnamed-chunk-36-1.png" width="960" class="r-stretch"></section>
<section id="correlación" class="slide level2">
<h2>Correlación</h2>
<p>La correlación es una medida de distancia muy útil cuando la definición de simulitud se hace en términos de patrón o forma y no de desplazamiento o magnitud ¿Qué quiere decir esto?</p>

<img data-src="lec_week9_files/figure-revealjs/unnamed-chunk-38-1.png" width="960" class="r-stretch quarto-figure-center"></section>
<section id="correlación-continuación" class="slide level2">
<h2>Correlación: continuación</h2>
<p>La correlación la definimos como:</p>
<p><span class="math display">\[d_{cor}(p,q)=1-\text{correlación}(p,q)\]</span></p>
<p>Según esta definición, tendremos que a una correlación de magnitud 1, la similitud o distancia será cero, y por tanto las consideraremos iguales.</p>
<p>Existen más distancias que podemos ocupar, entre ellas <em>Jackknife correlation</em>,<em>Simple matching coefficient</em> (para datos binarios), índice Jacard, distancia coseno, entre otras.</p>
</section>
<section id="k-medias" class="slide level2">
<h2>K-medias</h2>
<p>El método de K-medias o <span class="math inline">\(K-means\)</span> agrupa las observaciones en <span class="math inline">\(K\)</span> grupos distintos donde <span class="math inline">\(K\)</span> se determina antes de realizar el análisis. Una vez definido el valor de <span class="math inline">\(k\)</span>, esta metodología encuentra los <span class="math inline">\(k\)</span> mejores clusters, entendiendo como mejor <em>cluster</em> aquel cuya varianza interna (varianza intra-cluster) sea lo más pequeña posible.</p>
<p>Así, este método es un problema de optimización, en el que se reparten las observaciones en <span class="math inline">\(K\)</span> <em>clusters</em> de forma que la suma de las varianzas internas de todos ellos sea la menor posible. Esto requiere definir un modo de cuantificar la varianza interna.</p>
</section>
<section id="k-medias-continuación" class="slide level2 small">
<h2>K-medias: continuación</h2>
<p>Consideremos <span class="math inline">\(C_1,\dots,C_k\)</span> como los conjuntos formados por los índices de las observaciones de cada uno de los <em>clusters</em>. Esto es, <span class="math inline">\(C_1\)</span> contiene los índices de las observaciones agrupadas en el <em>cluster</em> 1. Para indicar la pertenencia de una observación a un <em>cluster</em> particular es <span class="math inline">\(i\in C_k\)</span>. Todos estos conjuntos satisfacen las siguientes dos propiedades:</p>
<ol type="1">
<li class="fragment"><span class="math inline">\(C_1 \cup C_2 \dots \cup C_k=\{1,\dots,n\}\)</span></li>
<li class="fragment"><span class="math inline">\(C_k \cap C_{k'}=\emptyset\)</span></li>
</ol>
<div class="fragment">
<p>Dos medidas frecuentemente utilizadas para definir la varianza interna de cada <em>cluster</em> <span class="math inline">\(W(C_k)\)</span> son:</p>
<ol type="1">
<li class="fragment"><span class="math inline">\(W(C_k)=\sum_{x_i\in C_k} (x_i-\mu_k)^2\)</span></li>
<li class="fragment"><span class="math inline">\(W(C_k)=\dfrac{1}{|C_k|}\sum_{i,i'\in C_k}\sum_{j=1}^{p}(x_{ij}-x_{i'j})^2\)</span></li>
</ol>
</div>
<div class="fragment">
<p>Minimizar la suma total de la varianza interna <span class="math inline">\(\sum_{k=1}^{K}W(C_k)\)</span> de forma exacta (encontrar un mínimo global) es un proceso complejo debido a la inmensa cantidad de formas en las que <span class="math inline">\(n\)</span> observaciones se pueden dividir en <span class="math inline">\(K\)</span> grupos. Sin embargo, es posible obtener una solución que si bien no es la óptima, nos entrega un óptimo local.</p>
</div>
</section>
<section id="k-medias-continuación-1" class="slide level2">
<h2>K-medias: continuación</h2>
<p>El algoritmo empleado en este caso es:</p>
<ol type="1">
<li class="fragment">Asignar aleatoriamente un número entre 1 y <span class="math inline">\(K\)</span> a cada observación. Esto sirve como asignación inicial aleatoria de las observaciones a los <em>clusters</em>.</li>
<li class="fragment">Iterar los siguientes pasos hasta que la asignación de las observaciones de los <em>clusters</em> no cambie o se alcance un número máximo de iteraciones preestablecido.
<ul>
<li class="fragment">Para cada uno de los <em>clusters</em> calcular su centroide (centro de gravedad)</li>
<li class="fragment">Asignar cada observación al <em>cluster</em> cuyo centroide está más próximo.</li>
</ul></li>
</ol>
<div class="fragment">
<p>Este algoritmo garantiza que, en cada paso, se reduzca la intra-varianza total de los <em>clusters</em> hasta alcanzar un óptimo local.</p>
</div>
</section>
<section id="figura-k-medias" class="slide level2">
<h2>Figura: K-medias</h2>

<img data-src="images/week9/k_means.png" class="r-stretch quarto-figure-center"></section>
<section id="k-medias-continuación-2" class="slide level2">
<h2>K-medias: continuación</h2>
<p>Otra forma de implementar el algoritmo de K-means clustering es la siguiente:</p>
<ol type="1">
<li class="fragment">Especificar el número K de <em>clusters</em> que se quieren crear.</li>
<li class="fragment">Seleccionar de forma aleatoria k observaciones del set de datos como centroides iniciales.</li>
<li class="fragment">Asignar cada una de las observaciones al centroide más cercano.</li>
<li class="fragment">Para cada uno de los K <em>clusters</em> recalcular su centroide.</li>
<li class="fragment">Repetir los pasos 3 y 4 hasta que las asignaciones no cambien o se alcance el número máximo de iteraciones establecido.</li>
</ol>
<div class="fragment">
<p>Debido a que el algoritmo de <em>K-means</em> no evalúa todas las posibles distribuciones de las observaciones sino solo parte de ellas, los resultados obtenidos dependen de la asignación aleatoria inicial (paso 1). Por esta razón, es importante ejecutar el algoritmo varias veces (25-50), cada una con una asignación aleatoria inicial distinta, y seleccionar aquella que haya conseguido un menor valor de varianza total.</p>
</div>
</section>
<section id="k-means-ventajas-y-desventajas" class="slide level2">
<h2>K-means: Ventajas y desventajas</h2>
<p><em>K-means</em> es uno de los métodos de clustering más utilizados. Destaca por la sencillez y velocidad de su algoritmo, sin embargo, presenta una serie de limitaciones que se deben tener en cuenta.</p>
<ol type="1">
<li class="fragment"><p>Requiere que se indique de antemano el número de <em>clusters</em> que se van a crear. Esto puede ser complicado si no se dispone de información adicional sobre los datos con los que se trabaja. Se han desarrollado varias estrategias para ayudar a identificar potenciales valores óptimos de K, aunque todas ellas son orientativas.</p></li>
<li class="fragment"><p>Las agrupaciones resultantes pueden variar dependiendo de la asignación aleatoria inicial de los centroides. Para minimizar este problema se recomienda repetir el proceso de clustering entre 25-50 veces y seleccionar como resultado definitivo el que tenga menor suma total de varianza interna. Aun así, solo se puede garantizar la reproducibilidad de los resultados si se emplean semillas.</p></li>
<li class="fragment"><p>Presenta problemas de robustez frente a outliers. La única solución es excluirlos o recurrir a otros métodos de clustering más robustos como <em>K-medoids (partitioning around medoids; PAM)</em>.</p></li>
</ol>
</section>
<section id="ejemplo-2" class="slide level2">
<h2>Ejemplo</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb44-2"><a href="#cb44-2"></a><span class="fu">library</span>(ggpubr)</span>
<span id="cb44-3"><a href="#cb44-3"></a><span class="fu">set.seed</span>(<span class="dv">411</span>)</span>
<span id="cb44-4"><a href="#cb44-4"></a>datos <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(<span class="at">n =</span> <span class="dv">100</span><span class="sc">*</span><span class="dv">2</span>), <span class="at">nrow =</span> <span class="dv">100</span>, <span class="at">ncol =</span> <span class="dv">2</span>,</span>
<span id="cb44-5"><a href="#cb44-5"></a>                <span class="at">dimnames =</span> <span class="fu">list</span>(<span class="cn">NULL</span>,<span class="fu">c</span>(<span class="st">"x"</span>, <span class="st">"y"</span>)))</span>
<span id="cb44-6"><a href="#cb44-6"></a>datos <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(datos)</span>
<span id="cb44-7"><a href="#cb44-7"></a>media_grupos <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(<span class="at">n =</span> <span class="dv">8</span>, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">4</span>), <span class="at">nrow =</span> <span class="dv">4</span>, <span class="at">ncol =</span> <span class="dv">2</span>,</span>
<span id="cb44-8"><a href="#cb44-8"></a>                       <span class="at">dimnames =</span> <span class="fu">list</span>(<span class="cn">NULL</span>, <span class="fu">c</span>(<span class="st">"media_x"</span>, <span class="st">"media_y"</span>)))</span>
<span id="cb44-9"><a href="#cb44-9"></a>media_grupos <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(media_grupos)</span>
<span id="cb44-10"><a href="#cb44-10"></a>media_grupos <span class="ot">&lt;-</span> media_grupos <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">grupo =</span> <span class="fu">c</span>(<span class="st">"a"</span>,<span class="st">"b"</span>,<span class="st">"c"</span>,<span class="st">"d"</span>))</span>
<span id="cb44-11"><a href="#cb44-11"></a>datos <span class="ot">&lt;-</span> datos <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">grupo =</span> <span class="fu">sample</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="st">"a"</span>,<span class="st">"b"</span>,<span class="st">"c"</span>,<span class="st">"d"</span>),</span>
<span id="cb44-12"><a href="#cb44-12"></a>                                         <span class="at">size =</span> <span class="dv">100</span>,</span>
<span id="cb44-13"><a href="#cb44-13"></a>                                         <span class="at">replace =</span> <span class="cn">TRUE</span>))</span>
<span id="cb44-14"><a href="#cb44-14"></a>datos <span class="ot">&lt;-</span> <span class="fu">left_join</span>(datos, media_grupos, <span class="at">by =</span> <span class="st">"grupo"</span>)</span>
<span id="cb44-15"><a href="#cb44-15"></a>datos <span class="ot">&lt;-</span> datos <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">x =</span> x <span class="sc">+</span> media_x,</span>
<span id="cb44-16"><a href="#cb44-16"></a>                          <span class="at">y =</span> y <span class="sc">+</span> media_y)</span>
<span id="cb44-17"><a href="#cb44-17"></a></span>
<span id="cb44-18"><a href="#cb44-18"></a>km1<span class="ot">&lt;-</span><span class="fu">ggplot</span>(<span class="at">data =</span> datos, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">color =</span> grupo)) <span class="sc">+</span></span>
<span id="cb44-19"><a href="#cb44-19"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="fl">2.5</span>) <span class="sc">+</span></span>
<span id="cb44-20"><a href="#cb44-20"></a>  <span class="fu">theme_bw</span>()</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="ejemplo-continuación-12" class="slide level2">
<h2>Ejemplo: continuación</h2>

<img data-src="lec_week9_files/figure-revealjs/unnamed-chunk-42-1.png" width="960" class="r-stretch quarto-figure-center"></section>
<section id="ejemplo-continuación-13" class="slide level2">
<h2>Ejemplo: continuación</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1"></a><span class="fu">set.seed</span>(<span class="dv">411</span>)</span>
<span id="cb45-2"><a href="#cb45-2"></a>km_clusters <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(<span class="at">x =</span> datos[, <span class="fu">c</span>(<span class="st">"x"</span>, <span class="st">"y"</span>)], <span class="at">centers =</span> <span class="dv">4</span>, <span class="at">nstart =</span> <span class="dv">50</span>)</span>
<span id="cb45-3"><a href="#cb45-3"></a>km_clusters</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>K-means clustering with 4 clusters of sizes 24, 25, 33, 18

Cluster means:
          x          y
1 -4.840884 -5.5399183
2  2.970914  0.3436634
3  3.119718 -3.9643128
4 -5.188403 -0.9255762

Clustering vector:
  [1] 4 1 3 2 4 3 1 2 2 3 2 3 3 3 3 4 3 2 3 4 1 3 2 3 2 1 1 2 1 3 3 2 1 1 1 3 3
 [38] 3 2 3 1 3 1 2 2 4 4 2 2 1 1 3 2 2 4 1 1 2 4 1 4 4 3 2 2 4 1 3 1 4 4 2 3 4
 [75] 3 3 3 3 2 4 1 4 2 3 3 3 2 1 4 4 1 3 1 3 2 1 3 3 1 2

Within cluster sum of squares by cluster:
[1] 39.08613 55.16049 61.76208 42.71183
 (between_SS / total_SS =  91.4 %)

Available components:

[1] "cluster"      "centers"      "totss"        "withinss"     "tot.withinss"
[6] "betweenss"    "size"         "iter"         "ifault"      </code></pre>
</div>
</div>
</section>
<section id="ejemplo-continuación-14" class="slide level2">
<h2>Ejemplo: continuación</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1"></a>datos <span class="ot">&lt;-</span> datos <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">cluster =</span> km_clusters<span class="sc">$</span>cluster)</span>
<span id="cb47-2"><a href="#cb47-2"></a>datos <span class="ot">&lt;-</span> datos <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">cluster =</span> <span class="fu">as.factor</span>(cluster),<span class="at">grupo=</span><span class="fu">as.factor</span>(grupo))</span>
<span id="cb47-3"><a href="#cb47-3"></a></span>
<span id="cb47-4"><a href="#cb47-4"></a>km2<span class="ot">&lt;-</span><span class="fu">ggplot</span>(<span class="at">data =</span> datos, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">color =</span> grupo)) <span class="sc">+</span>  <span class="fu">geom_text</span>(<span class="fu">aes</span>(<span class="at">label =</span> cluster), <span class="at">size =</span> <span class="dv">5</span>) <span class="sc">+</span> <span class="fu">theme_bw</span>() <span class="sc">+</span> <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"none"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="ejemplo-continuación-15" class="slide level2">
<h2>Ejemplo: continuación</h2>

<img data-src="lec_week9_files/figure-revealjs/unnamed-chunk-48-1.png" width="960" class="r-stretch quarto-figure-center"></section>
<section id="ejemplo-continuación-16" class="slide level2">
<h2>Ejemplo: continuación</h2>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>       grupo real
cluster  a  b  c  d
      1  0  0 24  0
      2  0 25  0  0
      3 33  0  0  0
      4  0  0  1 17</code></pre>
</div>
</div>
</section>
<section id="ejemplo-continuación-17" class="slide level2">
<h2>Ejemplo: continuación</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1"></a>datos <span class="ot">&lt;-</span> datos <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">select</span>(x,y)</span>
<span id="cb49-2"><a href="#cb49-2"></a><span class="fu">set.seed</span>(<span class="dv">411</span>)</span>
<span id="cb49-3"><a href="#cb49-3"></a>km_clusters_2 <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(<span class="at">x =</span> datos, <span class="at">centers =</span> <span class="dv">2</span>, <span class="at">nstart =</span> <span class="dv">50</span>)</span>
<span id="cb49-4"><a href="#cb49-4"></a>datos <span class="ot">&lt;-</span> datos <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">cluster =</span> km_clusters_2<span class="sc">$</span>cluster)</span>
<span id="cb49-5"><a href="#cb49-5"></a>p1 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(<span class="at">data =</span> datos, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">color =</span> <span class="fu">as.factor</span>(cluster))) <span class="sc">+</span></span>
<span id="cb49-6"><a href="#cb49-6"></a>      <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb49-7"><a href="#cb49-7"></a>      <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Kmeans con k=2"</span>) <span class="sc">+</span></span>
<span id="cb49-8"><a href="#cb49-8"></a>      <span class="fu">theme_bw</span>() <span class="sc">+</span></span>
<span id="cb49-9"><a href="#cb49-9"></a>      <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"none"</span>)</span>
<span id="cb49-10"><a href="#cb49-10"></a></span>
<span id="cb49-11"><a href="#cb49-11"></a>datos <span class="ot">&lt;-</span> datos <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">select</span>(x, y)</span>
<span id="cb49-12"><a href="#cb49-12"></a></span>
<span id="cb49-13"><a href="#cb49-13"></a>km_clusters_6 <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(<span class="at">x =</span> datos, <span class="at">centers =</span> <span class="dv">6</span>, <span class="at">nstart =</span> <span class="dv">50</span>)</span>
<span id="cb49-14"><a href="#cb49-14"></a>datos <span class="ot">&lt;-</span> datos <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">cluster =</span> km_clusters_6<span class="sc">$</span>cluster)</span>
<span id="cb49-15"><a href="#cb49-15"></a>p2 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(<span class="at">data =</span> datos, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">color =</span> <span class="fu">as.factor</span>(cluster))) <span class="sc">+</span></span>
<span id="cb49-16"><a href="#cb49-16"></a>      <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb49-17"><a href="#cb49-17"></a>      <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Kmeans con k=6"</span>) <span class="sc">+</span></span>
<span id="cb49-18"><a href="#cb49-18"></a>      <span class="fu">theme_bw</span>() <span class="sc">+</span></span>
<span id="cb49-19"><a href="#cb49-19"></a>      <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"none"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="ejemplo-continuación-18" class="slide level2">
<h2>Ejemplo: continuación</h2>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1"></a><span class="fu">library</span>(gridExtra)</span>
<span id="cb50-2"><a href="#cb50-2"></a><span class="fu">ggarrange</span>(p1, p2)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>

</div>
<img data-src="lec_week9_files/figure-revealjs/unnamed-chunk-54-1.png" width="960" class="r-stretch quarto-figure-center"></section>
<section id="agrupamiento-jerárquico" class="slide level2">
<h2>Agrupamiento jerárquico</h2>
<p>Agrupamiento jerárquico o <em>Hierarchical clustering</em> es una alternativa no requiere que se pre-especifique el número de <em>clusters</em>. Los métodos que engloba el <em>hierarchical clustering</em> se subdividen en dos tipos dependiendo de la estrategia seguida para crear los grupos:</p>
<ul>
<li class="fragment"><p><em>Agglomerative clustering (bottom-up)</em>: el agrupamiento se inicia en la base del árbol, donde cada observación forma un <em>cluster</em> individual. Los <em>clusters</em> se van combinado a medida que la estructura crece hasta converger en una única “rama” central.</p></li>
<li class="fragment"><p><em>Divisive clustering (top-down)</em>: es la estrategia opuesta al <em>agglomerative clustering</em>, se inicia con todas las observaciones contenidas en un mismo <em>cluster</em> y se suceden divisiones hasta que cada observación forma un <em>cluster</em> individual.</p></li>
</ul>
<div class="fragment">
<p>En ambos casos, los resultados pueden representarse de forma muy intuitiva en una estructura de árbol llamada dendrograma.</p>
</div>
</section>
<section id="agglomerative" class="slide level2">
<h2>Agglomerative</h2>
<p>La estructura resultante de un <em>agglomerative hierarchical clustering</em> se obtiene mediante un algoritmo sencillo.</p>
<ol type="1">
<li class="fragment"><p>El proceso se inicia considerando cada una de las observaciones como un cluster individual, formando así la base del dendrograma (hojas).</p></li>
<li class="fragment"><p>Se inicia un proceso iterativo hasta que todas las observaciones pertenecen a un único cluster:</p>
<ul>
<li class="fragment"><p>Se calcula la distancia entre cada posible par de los <span class="math inline">\(n\)</span> clusters. El investigador debe determinar el tipo de medida emplea para cuantificar la similitud entre observaciones o grupos (distancia y linkage).</p></li>
<li class="fragment"><p>Los dos clusters más similares se fusionan, de forma que quedan <span class="math inline">\(n-1\)</span> clusters.</p></li>
</ul></li>
<li class="fragment"><p>Determinar dónde cortar la estructura de árbol generada (dendrograma).</p></li>
</ol>
</section>
<section id="figura-h.c.-agglomerative" class="slide level2">
<h2>Figura H.C. agglomerative</h2>

<img data-src="images/week9/agglomerative.png" class="r-stretch quarto-figure-center"></section>
<section id="agglomerative-continuación" class="slide level2 small">
<h2>Agglomerative: continuación</h2>
<p>Para que el proceso de agrupamiento pueda llevarse a cabo tal como indica el algoritmo anterior, es necesario definir cómo se cuantifica la similitud entre dos <em>clusters</em>. Es decir, se tiene que extender el concepto de distancia entre pares de observaciones para que sea aplicable a pares de grupos, cada uno formado por varias observaciones. A este proceso se le conoce como <strong>linkage</strong>. A continuación, se describen los tipos de linkage más empleados y sus definiciones.</p>
<ol type="1">
<li class="fragment"><p><strong>Complete or Maximum</strong>: Se calcula la distancia entre todos los posibles pares formados por una observación del <em>cluster</em> A y una del <em>cluster</em> B. La mayor de todas ellas se selecciona como la distancia entre los dos <em>clusters</em>. Se trata de la medida más conservadora (<em>maximal intercluster dissimilarity</em>).</p></li>
<li class="fragment"><p><strong>Single or Minimum</strong>: Se calcula la distancia entre todos los posibles pares formados por una observación del <em>cluster</em> A y una del <em>cluster</em> B. La menor de todas ellas se selecciona como la distancia entre los dos clusters. Se trata de la medida menos conservadora (<em>minimal intercluster dissimilarity</em>).</p></li>
<li class="fragment"><p><strong>Average</strong>: Se calcula la distancia entre todos los posibles pares formados por una observación del <em>cluster</em> A y una del <em>cluster</em> B. El valor promedio de todas ellas se selecciona como la distancia entre los dos clusters (<em>mean intercluster dissimilarity</em>).</p></li>
</ol>
</section>
<section id="agglomerative-continuación-1" class="slide level2">
<h2>Agglomerative: continuación</h2>
<ol start="4" type="1">
<li class="fragment"><p><strong>Centroid</strong>: Se calcula el centroide de cada uno de los <em>clusters</em> y se selecciona la distancia entre ellos como la distancia entre los dos clusters.</p></li>
<li class="fragment"><p><strong>Ward</strong>: Se trata de un método general. La selección del par de <em>clusters</em> que se combinan en cada paso del <em>agglomerative hierarchical clustering</em> se basa en el valor óptimo de una función objetivo, pudiendo ser esta última cualquier función definida por el analista. El conocido método <em>Ward’s minimum variance</em> es un caso particular en el que el objetivo es minimizar la suma total de varianza intra-cluster.</p></li>
</ol>
<div class="fragment">
<p>Los métodos de <em>linkage complete, average</em> y <em>Ward’s minimum variance</em> suelen ser los preferidos por los analistas debido a que generan dendrogramas más compensados. Sin embargo, no se puede determinar que uno sea mejor que otro, ya que depende del caso de estudio en cuestión.</p>
</div>
</section>
<section id="divisive" class="slide level2">
<h2>Divisive</h2>
<p>El algoritmo más conocido de <strong>divisive hierarchical clustering</strong> es DIANA (<em>DIvisive ANAlysis Clustering</em>). Este algoritmo se inicia con un único <em>cluster</em> que contiene todas las observaciones, a continuación, se van sucediendo divisiones hasta que cada observación forma un <em>cluster</em> independiente. En cada iteración, se selecciona el <em>cluster</em> con mayor diámetro, entendiendo por diámetro de un <em>cluster</em> la mayor de las diferencias entre dos de sus observaciones.</p>
<p>Una vez seleccionado el <em>cluster</em>, se identifica la observación más dispar, que es aquella con mayor distancia promedio respecto al resto de observaciones que forman el <em>cluster</em>, esta observación inicia el nuevo <em>cluster</em>. Se reasignan las observaciones en función de si están más próximas al nuevo <em>cluster</em> o al resto de la partición, dividiendo así el <em>cluster</em> seleccionado en dos nuevos <em>clusters</em>.</p>
</section>
<section id="divisive-continuación" class="slide level2">
<h2>Divisive: continuación</h2>
<ol type="1">
<li class="fragment"><p>Todas las n observaciones forman un único cluster.</p></li>
<li class="fragment"><p>Repetir hasta que hayan n clusters:</p>
<ul>
<li class="fragment">Calcular para cada <em>cluster</em> la mayor de las distancias entre pares de observaciones (diámetro del <em>cluster</em>).</li>
<li class="fragment">Seleccionar el <em>cluster</em> con mayor diámetro.</li>
<li class="fragment">Calcular la distancia media de cada observación respecto a las demás.</li>
<li class="fragment">La observación más distante inicia un nuevo <em>cluster</em>.</li>
<li class="fragment">Se reasignan las observaciones restantes al nuevo <em>cluster</em> o al viejo dependiendo de cual está más próximo.</li>
</ul></li>
</ol>
<div class="fragment">
<p>A diferencia del <em>clustering</em> aglomerativo, en el que hay que elegir un tipo de distancia y un método de <em>linkage</em>, en el <em>clustering</em> divisivo sólo hay que elegir la distancia.</p>
</div>
</section>
<section id="dendograma" class="slide level2">
<h2>Dendograma</h2>
<p>Para ilustrar cómo se interpreta un dendograma, se simula un set de datos y se somete a un proceso de hierarchical clustering.</p>
<p>Supongamos que se dispone de 45 observaciones en un espacio de dos dimensiones, que pertenecen a 3 grupos. Aunque se ha coloreado de forma distinta cada uno de los grupos, se va a suponer que se desconoce esta información y que se desea aplicar el método de <em>hierarchical clustering</em> para intentar reconocer los grupos.</p>
</section>
<section id="dendograma-continuación" class="slide level2">
<h2>Dendograma: continuación</h2>

<img data-src="images/week9/hierarchical.png" class="r-stretch quarto-figure-center"></section>
<section id="dendograma-continuación-1" class="slide level2">
<h2>Dendograma: continuación</h2>
<p>Al aplicar <em>hierarchical clustering</em>, empleando como medida de similitud la distancia euclídea y <em>linkage complete</em>, se obtiene el siguiente dendrograma. Como los datos se han simulado en aproximamdamente la misma escala, no es necesario estandarizarlos, de no ser así, sí se tendrían que estandarizar.</p>

<img data-src="images/week9/hierarchical2.png" class="r-stretch quarto-figure-center"></section>
<section id="verificación-del-árbol-resultante" class="slide level2">
<h2>Verificación del árbol resultante</h2>
<p>Una vez creado el dendrograma, hay que evaluar hasta qué punto su estructura refleja las distancias originales entre observaciones. Una forma de hacerlo es empleando el coeficiente de correlación entre las distancias <em>cophenetic</em> del dendrograma (altura de los nodos) y la matriz de distancias original.</p>
<p>Cuanto más cercano es el valor a 1, mejor refleja el dendrograma la verdadera similitud entre las observaciones. Valores superiores a 0.75 suelen considerarse como buenos.</p>
</section>
<section id="ejemplo-3" class="slide level2">
<h2>Ejemplo</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1"></a><span class="fu">data</span>(USArrests)</span>
<span id="cb51-2"><a href="#cb51-2"></a>datos <span class="ot">&lt;-</span> <span class="fu">scale</span>(USArrests)</span>
<span id="cb51-3"><a href="#cb51-3"></a>mat_dist <span class="ot">&lt;-</span> <span class="fu">dist</span>(<span class="at">x =</span> datos, <span class="at">method =</span> <span class="st">"euclidean"</span>)</span>
<span id="cb51-4"><a href="#cb51-4"></a>hc_euclidiana_complete <span class="ot">&lt;-</span> <span class="fu">hclust</span>(<span class="at">d =</span> mat_dist, <span class="at">method =</span> <span class="st">"complete"</span>)</span>
<span id="cb51-5"><a href="#cb51-5"></a>hc_euclidiana_average  <span class="ot">&lt;-</span> <span class="fu">hclust</span>(<span class="at">d =</span> mat_dist, <span class="at">method =</span> <span class="st">"average"</span>)</span>
<span id="cb51-6"><a href="#cb51-6"></a><span class="fu">cor</span>(<span class="at">x =</span> mat_dist, <span class="fu">cophenetic</span>(hc_euclidiana_complete))</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.6979437</code></pre>
</div>
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1"></a><span class="fu">cor</span>(<span class="at">x =</span> mat_dist, <span class="fu">cophenetic</span>(hc_euclidiana_average))</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.7180382</code></pre>
</div>
</div>
<p>Para estos datos, el método de <em>linkage average</em> consigue representar un poco mejor la similitud entre observaciones.</p>
</section>
<section id="corte-del-árbol" class="slide level2">
<h2>Corte del árbol</h2>
<p>Además de representar en un dendrograma la similitud entre observaciones, se tiene que poder identificar el número de <em>clusters</em> creados y qué observaciones forman parte de cada uno. Si se realiza un corte horizontal a una determinada altura del dendrograma, el <strong>número de ramas que sobrepasan</strong> (en sentido ascendente) dicho corte se corresponde con el número de clusters.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1"></a><span class="fu">library</span>(factoextra)</span>
<span id="cb55-2"><a href="#cb55-2"></a>datos <span class="ot">&lt;-</span> USArrests</span>
<span id="cb55-3"><a href="#cb55-3"></a>datos <span class="ot">&lt;-</span> <span class="fu">scale</span>(datos)</span>
<span id="cb55-4"><a href="#cb55-4"></a><span class="fu">set.seed</span>(<span class="dv">411</span>)</span>
<span id="cb55-5"><a href="#cb55-5"></a></span>
<span id="cb55-6"><a href="#cb55-6"></a>hc_euclidiana_completo <span class="ot">&lt;-</span> <span class="fu">hclust</span>(<span class="at">d =</span> <span class="fu">dist</span>(<span class="at">x =</span> datos, <span class="at">method =</span> <span class="st">"euclidean"</span>),</span>
<span id="cb55-7"><a href="#cb55-7"></a>                               <span class="at">method =</span> <span class="st">"complete"</span>)</span>
<span id="cb55-8"><a href="#cb55-8"></a></span>
<span id="cb55-9"><a href="#cb55-9"></a>dendograma1<span class="ot">&lt;-</span><span class="fu">fviz_dend</span>(<span class="at">x =</span> hc_euclidiana_completo, <span class="at">k =</span> <span class="dv">2</span>, <span class="at">cex =</span> <span class="fl">0.6</span>) <span class="sc">+</span></span>
<span id="cb55-10"><a href="#cb55-10"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="fl">5.5</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>) <span class="sc">+</span></span>
<span id="cb55-11"><a href="#cb55-11"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Herarchical clustering"</span>,</span>
<span id="cb55-12"><a href="#cb55-12"></a>       <span class="at">subtitle =</span> <span class="st">"Distancia euclídea, Linkage complete, K=2"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="corte-del-árbol-continuación" class="slide level2">
<h2>Corte del árbol: continuación</h2>

<img data-src="lec_week9_files/figure-revealjs/unnamed-chunk-60-1.png" width="960" class="r-stretch quarto-figure-center"></section>
<section id="corte-del-árbol-continuación-1" class="slide level2">
<h2>Corte del árbol: continuación</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb56-1"><a href="#cb56-1"></a>dendograma2<span class="ot">&lt;-</span><span class="fu">fviz_dend</span>(<span class="at">x =</span> hc_euclidiana_completo, <span class="at">k =</span> <span class="dv">4</span>, <span class="at">cex =</span> <span class="fl">0.6</span>) <span class="sc">+</span></span>
<span id="cb56-2"><a href="#cb56-2"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="fl">3.5</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>) <span class="sc">+</span></span>
<span id="cb56-3"><a href="#cb56-3"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Herarchical clustering"</span>,</span>
<span id="cb56-4"><a href="#cb56-4"></a>       <span class="at">subtitle =</span> <span class="st">"Distancia euclídea, Linkage complete, K=4"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="corte-del-árbol-continuación-2" class="slide level2">
<h2>Corte del árbol: continuación</h2>

<img data-src="lec_week9_files/figure-revealjs/unnamed-chunk-64-1.png" width="960" class="r-stretch quarto-figure-center"></section>
<section id="corte-del-árbol-continuación-3" class="slide level2">
<h2>Corte del árbol: continuación</h2>
<p>Dos propiedades adicionales se derivan de la forma en que se generan los <em>clusters</em> en el método de <em>hierarchical clustering</em>:</p>
<ol type="1">
<li class="fragment"><p>Dada la longitud variable de las ramas, siempre existe un intervalo de altura para el que cualquier corte da lugar al mismo número de <em>clusters</em>.</p></li>
<li class="fragment"><p>Con un solo dendrograma se dispone de la flexibilidad para generar cualquier número de clusters desde 1 a <span class="math inline">\(n\)</span>. La selección del número óptimo puede valorarse de forma visual, tratando de identificar las ramas principales en base a la altura a la que ocurren las uniones.</p></li>
</ol>
</section></section>
<section id="qué-veremos-la-próxima-semana" class="title-slide slide level1 center">
<h1>¿Qué veremos la próxima semana?</h1>
<ul>
<li class="fragment">Certamen #2</li>
</ul>
</section>

<section id="que-deben-preparar-para-la-próxima-semana" class="title-slide slide level1 center">
<h1>¿Que deben preparar para la próxima semana?</h1>
<ul>
<li class="fragment">Estudiar para el certamen</li>
</ul>


<img src="images/logo_usm.png" class="slide-logo r-stretch"><div class="footer footer-default">
<p>IND 163 - Semana 9</p>
</div>
</section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'smaller': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'fade',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1600,

        height: 900,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    
    <script>
      // htmlwidgets need to know to resize themselves when slides are shown/hidden.
      // Fire the "slideenter" event (handled by htmlwidgets.js) when the current
      // slide changes (different for each slide format).
      (function () {
        function fireSlideChanged(previousSlide, currentSlide) {

          // dispatch for htmlwidgets
          const event = window.document.createEvent("Event");
          event.initEvent("slideenter", true, true);
          window.document.dispatchEvent(event);

          // dispatch for shiny
          if (window.jQuery) {
            if (previousSlide) {
              window.jQuery(previousSlide).trigger("hidden");
            }
            if (currentSlide) {
              window.jQuery(currentSlide).trigger("shown");
            }
          }
        }

        // hookup for reveal
        if (window.Reveal) {
          window.Reveal.addEventListener("slidechanged", function(event) {
            fireSlideChanged(event.previousSlide, event.currentSlide);
          });
        }

        // hookup for slidy
        if (window.w3c_slidy) {
          window.w3c_slidy.add_observer(function (slide_num) {
            // slide_num starts at position 1
            fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);
          });
        }

      })();
    </script>

    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        target: function(trigger) {
          return trigger.previousElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copiada");
        setTimeout(function() {
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn) {
        const config = {
          allowHTML: true,
          content: contentFn,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'quarto-reveal',
          placement: 'bottom-start'
        };
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          let href = ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const cites = ref.parentNode.getAttribute('data-cites').split(' ');
        tippyHover(ref, function() {
          var popup = window.document.createElement('div');
          cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    });
    </script>
    

</body></html>